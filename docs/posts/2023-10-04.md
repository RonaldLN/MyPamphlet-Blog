---
draft: false
date: 2023-10-04
authors:
  - ronald_luo
categories:
  - Records of Trivia
comments: true
---

# å¤ç°ä»£ç è¿‡ç¨‹è®°å½•

## OpenP5 (ç¬¬ä¸€æ¬¡å°è¯•ä½†ä¸­æ–­)

è®ºæ–‡ç½‘å€ï¼š

[OpenP5: Benchmarking Foundation Models for Recommendation | Papers With Code](https://paperswithcode.com/paper/openp5-benchmarking-foundation-models-for)

ä»£ç åœ°å€ï¼š

[agiresearch/OpenP5: OpenP5: An Open-source Platform for Developing, Fine-tuning, and Evaluating LLM-based Recommenders (github.com) - https://github.com/agiresearch/OpenP5](https://github.com/agiresearch/OpenP5)

<!-- more -->

ç¯å¢ƒ `environment.txt` :

```txt
python==3.9.7
transformers==4.26.0
torch==1.8.1+cu111
sklearn==1.1.2
torchvision==0.9.1+cu111
tqdm==4.64.1
time
collections
argparse
os
sys
numpy==1.23.1
```

ç”±äº `time` `colloctions` `os` `sys` å¥½åƒéƒ½æ˜¯è‡ªå¸¦çš„ä¸èƒ½è£…ï¼Œè€Œ `torch` `torchvision` éœ€è¦è£…çš„æ˜¯ cuda ç‰ˆçš„ï¼Œæ‰€ä»¥æˆ‘å°†å…¶ä¿®æ”¹ä¸º

```txt
transformers==4.26.0
scikit-learn==1.1.2
tqdm==4.64.1
argparse
numpy==1.23.1
```

å…¶ä¸­ `sklearn` åœ¨ä¹‹å‰å®‰è£…æ—¶å‘ç°è¯´æ˜¯æ›´æ”¹æˆäº† `scikit-learn` æ‰€ä»¥æˆ‘ä¹Ÿè¿›è¡Œäº†ä¿®æ”¹

è¿™æ ·å°±å¯ä»¥ç›´æ¥è¿è¡Œå‘½ä»¤æ¥å®‰è£…å¯¹åº”çš„åŒ…

```bash
pip install -r environment.txt
```

---

å®‰è£…å¥½è¿™äº›åï¼Œæ‰“å¼€ clone çš„ä»“åº“ï¼Œå‘ç° `train.py` ä¸­è¿˜æ˜¯æœ‰ä¸€äº›æŠ¥é”™

![openp5_new_version](../images/openp5_new_version.png)

å¹¶ä¸” `README.md` ä¸­å†™ç€

>   ## Usage
>
>   Download the data from [Google Drive link](https://drive.google.com/drive/folders/1W5i5ryetj_gkcOpG1aZfL5Y8Yk6RxwYE?usp=sharing), and put them into `./data` folder.
>
>   The training command can be found in `./command` folder. Run the command such as
>
>   ```bash
>   cd command
>   sh ML1M_random.sh
>   ```

ç„¶è€Œå¹¶æ²¡æœ‰ `./command` æ–‡ä»¶å¤¹ï¼Œç„¶åæˆ‘å‘ç°ï¼Œä»mainåˆ†æ”¯é‡Œä¸‹è½½çš„æ–‡ä»¶é‡Œé¢å¹¶æ²¡æœ‰commandæ–‡ä»¶å¤¹ï¼Œ

è€Œæœ‰ä¸€ä¸ª old_versionåˆ†æ”¯é‡Œé¢æœ‰ï¼Œå¹¶ä¸” `./command/ML1M_random.sh` æ–‡ä»¶ä¸­çš„å‘½ä»¤æ˜¯è¦è¿è¡Œ `./src/main.py` æ–‡ä»¶ï¼Œè€Œè¿™ä¸ªæ–‡ä»¶åªåœ¨ old_version åˆ†æ”¯ä¸­æœ‰ï¼Œå¹¶ä¸”å‡ ä¹æ‰€è£…çš„åŒ…éƒ½åœ¨ `main.py` ä¸­è¢«å¯¼å…¥ï¼Œæ‰€ä»¥æˆ‘å†³å®šä½¿ç”¨ old_version

---

ç”±äºéœ€è¦æ‰§è¡Œ `.sh` æ–‡ä»¶ï¼Œè¿™ä¸ªåœ¨windowsçš„cmdä¸­å¥½åƒä¸èƒ½ä½¿ç”¨ï¼Œåªèƒ½åœ¨git bashç»ˆç«¯ä¸­ä½¿ç”¨ï¼Œè€Œæˆ‘åˆéœ€è¦ä½¿ç”¨ conda ç¯å¢ƒï¼Œ

æ‰€ä»¥å‘ç°åœ¨ bash ä¸­ä¸èƒ½åƒcmdä¸­ä¸€æ ·è¿è¡Œ `conda activate openp5` 

æœç´¢åå‘ç°äº†è¿™ä¸ªæ–¹æ³•æœ‰æ•ˆ

[Conda environment fails to activate with Git Bash Â· Issue #19534 Â· microsoft/vscode-python (github.com)](https://github.com/microsoft/vscode-python/issues/19534#issuecomment-1194774160)

>   [æ‰‹å†Œ - Anaconda](https://ronaldln.github.io/MyPamphlet/ç³»ç»Ÿ%26ç¯å¢ƒ/anaconda/#5)

`source` anaconda3 å®‰è£…è·¯å¾„ä¸‹çš„ `/Scripts/activate`

```bash
source /e/Programs/Anaconda3/Scripts/activate
```

ç„¶åå°±ä¼šå¯åŠ¨ anaconda çš„ base ç¯å¢ƒï¼Œè¿™æ—¶ `conda activate openp5` å°±æœ‰å¯ä»¥ä½¿ç”¨ openp5 çš„è™šæ‹Ÿç¯å¢ƒäº†

---

### è§£å†³ `.pyc` æ–‡ä»¶å¯¼å…¥é—®é¢˜

old_versionçš„ `main.py` ä¸­æœ‰ä¸€å¤„æŠ¥é”™ï¼Œ

![openp5_old_version](../images/openp5_old_version.png){ loading=lazy }

æ˜¯ä½¿ç”¨äº†è‡ªå·±çš„åŒ…ï¼Œè€Œå¥½åƒ pycharm ä¸­ `./src/model` æ–‡ä»¶å¤¹ä¸­å¹¶æ²¡æœ‰è¿™ä¸ª `P5` çš„ä¸œè¥¿

ç»è¿‡ä¸€é¡¿æŠ˜è…¾ä¹‹åï¼Œå¶ç„¶å‘ç°äº†ï¼ŒåŸæœ¬çš„ä»“åº“é‡Œé¢ï¼Œ `model` æ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€ä¸ª `__pycache__` æ–‡ä»¶å¤¹ï¼Œé‡Œé¢æœ‰ä¸€ä¸ª `P5.cpython-39.pyc` æ–‡ä»¶ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ªåº”è¯¥å°±æ˜¯ `main.py` è¦å¯¼çš„åŒ…ï¼Œ

æ‰€ä»¥å¼€å§‹æŸ¥è¯¢å¦‚ä½•åœ¨ pycharm ä¸­æ‰èƒ½å¯¼å…¥è¿™ä¸ªåŒ…(å› ä¸ºæœ€è¿‘ä¹Ÿæœ‰ä¸€ä¸ªç±»ä¼¼çš„æƒ…å†µ(ç¼–è¯‘Orbbec SDK) - åœ¨ç»ˆç«¯ä¸­èƒ½ä½¿ç”¨/å¯¼å…¥pycæ–‡ä»¶ï¼Œè€Œåœ¨pycharmä¸­ä¼šæŠ¥é”™)ï¼Œç„¶åå‘ç°äº†

[python - Cannot see pyc files in PyCharm - Stack Overflow](https://stackoverflow.com/questions/64209855/cannot-see-pyc-files-in-pycharm)

è¿™ä¸ªçš„[å›ç­”](https://stackoverflow.com/a/64214290)

éœ€è¦åœ¨ pycharm çš„ **==è®¾ç½® - ç¼–è¾‘å™¨ - æ–‡ä»¶ç±»å‹ - å¿½ç•¥çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹==** é‡Œï¼ŒæŠŠ `*.pyc` å’Œ `__pycache__` å»æ‰

---

ä½†ç»è¿‡æµ‹è¯•ä¹‹åå‘ç°ï¼Œå³ä½¿èƒ½çœ‹åˆ°pycæ–‡ä»¶ä¹Ÿè¿˜æ˜¯ä¼šæŠ¥é”™

æƒ³èµ·æ¥åœ¨ [Orbbec SDK for Python ä½¿ç”¨æ‰‹å†Œ æµ‹è¯• Sample](https://vcp.developer.orbbec.com.cn:9001/project-2/doc-70/#æµ‹è¯•-sample) é‡Œï¼Œç»ˆç«¯ä¸Šä½¿ç”¨æ—¶ï¼Œéœ€è¦è®¾ç½®ä¸€ä¸ª `PYTHONPATH` çš„ç¯å¢ƒå˜é‡ï¼Œ

```bash
# set PYTHONPATH environment variable to include the lib directory in the install directory
export PYTHONPATH=$PYTHONPATH:$(pwd)/install/lib/
```

æ‰€ä»¥å¼€å§‹æŸ¥è¯¢å¦‚ä½•åœ¨ pycharm ä¸­è®¾ç½® `PYTHONPATH` ç¯å¢ƒå˜é‡ï¼Œç„¶åä»

[python - PyCharm and PYTHONPATH - Stack Overflow](https://stackoverflow.com/questions/28326362/pycharm-and-pythonpath)

[Manage interpreter paths | PyCharm Documentation (jetbrains.com)](https://www.jetbrains.com/help/pycharm/installing-uninstalling-and-reloading-interpreter-paths.html)

æ‰¾åˆ°äº†æ–¹æ³•ï¼š

**==è®¾ç½® - é¡¹ç›® - Pythonè§£é‡Šå™¨ - åœ¨ `Python è§£é‡Šå™¨:` å³ä¾§çš„æ¡†çš„å³ä¾§ç‚¹å‡»å‘ä¸‹çš„ä¸‰è§’/ç®­å¤´ - å…¨éƒ¨æ˜¾ç¤º - åœ¨å·¦ä¸Šè§’å›¾æ ‡ä¸­æ‰¾åˆ° `æ˜¾ç¤ºè§£é‡Šå™¨è·¯å¾„` - æ·»åŠ ç›¸åº”çš„è·¯å¾„==**

![show_paths_of_interpreter](../images/show_paths_of_interpreter.png){ loading=lazy }

![add_paths_of_interpreter](../images/add_paths_of_interpreter.png){ loading=lazy }

ç”±äº `main.py` æ–‡ä»¶ä¸­ï¼Œæ˜¯

```python
from model.P5 import P5
```

æ‰€ä»¥åº”è¯¥æ˜¯æŠŠ `model` çš„çˆ¶ç›®å½•(å³é¡¹ç›®çš„æ ¹ç›®å½•)æ·»åŠ ä¸Š

```txt
E:\Github\code_reproduction\OpenP5-old_version
```

æ·»åŠ å®Œäº†ä¹‹åï¼Œå³ä½¿è¿˜æ˜¯ä¼šæœ‰çº¢è‰²çš„è­¦å‘Šï¼Œä½†èƒ½è¿è¡Œäº†

![run_openp5](../images/run_openp5.png){ loading=lazy }

---

## æŠ¥é”™

??? quote "error info"

    ```bash
    $ sh ML1M_random.sh
    {'seed': 2023, 'model_dir': '../model', 'checkpoint_dir': '../checkpoint', 'model_name': 'model.pt', 'log_dir': '../log', 'distributed': 1, 'gpu': '0,1', 'master_addr': 'localhost', 'master_port': '1991', 'logging_level': 20, 'data_path': '../data', 'item_indexing': 'random', 'tasks': 'sequential,straightforward', 'datasets': 'ML1M', 'prompt_file': '../prompt.txt', 'sequential_order': 'original', 'collaborative_token_size': 200, 'collaborative_cluster': 20, 'collaborative_last_token': 'sequential', 'collaborative_float32': 0, 'max_his': 20, 'his_prefix': 1, 'his_sep': ' , ', 'skip_empty_his': 1, 'valid_prompt': 'seen:0', 'valid_prompt_sample': 1, 'valid_sample_num': '3,3', 'test_prompt': 'seen:0', 'sample_prompt': 1, 'sample_num': '3,3', 'batch_size': 128, 'eval_batch_size': 20, 'dist_sampler': 0, 'optim': 'AdamW', 'epochs': 10, 'lr': 0.001, 'clip': 1, 'logging_step': 100, 'warmup_prop': 0.05, 'gradient_accumulation_steps': 1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'dropout': 0.1, 'alpha': 2, 'train': 1, 'backbone': 't5-small', 'metrics': 'hit@5,hit@10,ndcg@5,ndcg@10', 'load': 0, 'random_initialize': 1, 'test_epoch': 0, 'valid_select': 0, 'test_before_train': 0, 'test_filtered': 0, 'test_filtered_batch': 1, 'log_name': '1_1_1_1_20_1991_ML1M_sequential,straightforward_t5-small_random_0.001_10_128_3,3_prompt', 'model_path': '../model\\ML1M\\1_1_1_1_20_1991_ML1M_sequential,straightforward_t5-small_random_0.001_10_128_3,3_prompt.pt', 'rank': 0}
    '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000018265B83BB0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 703c57e1-700e-4497-8fdf-3cf500baea63)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json
    '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000018265B83BB0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 703c57e1-700e-4497-8fdf-3cf500baea63)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json
    '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001821CE4F7C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: fece3cd4-12a5-4ab5-b3a2-7fea10c749bf)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json
    '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001821CE4F7C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: fece3cd4-12a5-4ab5-b3a2-7fea10c749bf)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json
    Traceback (most recent call last):
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connection.py", line 203, in _new_conn
        sock = connection.create_connection(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
        raise err
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
        sock.connect(sa)
    socket.timeout: timed out
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connectionpool.py", line 790, in urlopen
        response = self._make_request(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connectionpool.py", line 491, in _make_request
        raise new_e
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connectionpool.py", line 467, in _make_request
        self._validate_conn(conn)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connectionpool.py", line 1092, in _validate_conn
        conn.connect()
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connection.py", line 611, in connect
        self.sock = sock = self._new_conn()
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connection.py", line 212, in _new_conn
        raise ConnectTimeoutError(
    urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x000001821CE4F7C0>, 'Connection to huggingface.co timed out. (connect timeout=10)')
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\requests\adapters.py", line 486, in send
        resp = conn.urlopen(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\connectionpool.py", line 844, in urlopen
        retries = retries.increment(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\urllib3\util\retry.py", line 515, in increment
        raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001821CE4F7C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\file_download.py", line 1232, in hf_hub_download
        metadata = get_hf_file_metadata(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\utils\_validators.py", line 118, in _inner_fn
        return fn(*args, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\file_download.py", line 1599, in get_hf_file_metadata
        r = _request_wrapper(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\file_download.py", line 417, in _request_wrapper
        response = _request_wrapper(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\file_download.py", line 452, in _request_wrapper
        return http_backoff(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\utils\_http.py", line 274, in http_backoff
        raise err
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\utils\_http.py", line 258, in http_backoff
        response = session.request(method=method, url=url, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\requests\sessions.py", line 589, in request
        resp = self.send(prep, **send_kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\requests\sessions.py", line 703, in send
        r = adapter.send(request, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\utils\_http.py", line 63, in send
        return super().send(request, *args, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\requests\adapters.py", line 507, in send
        raise ConnectTimeout(e, request=request)
    requests.exceptions.ConnectTimeout: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001821CE4F7C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: fece3cd4-12a5-4ab5-b3a2-7fea10c749bf)')
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\transformers\utils\hub.py", line 409, in cached_file
        resolved_file = hf_hub_download(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\utils\_validators.py", line 118, in _inner_fn
        return fn(*args, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\huggingface_hub\file_download.py", line 1349, in hf_hub_download
        raise LocalEntryNotFoundError(
    huggingface_hub.utils._errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File "E:\Github\code_reproduction\OpenP5-old_version\src\main.py", line 233, in <module>
        single_main()
      File "E:\Github\code_reproduction\OpenP5-old_version\src\main.py", line 92, in single_main
        tokenizer = AutoTokenizer.from_pretrained(args.backbone)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 613, in from_pretrained
        config = AutoConfig.from_pretrained(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\transformers\models\auto\configuration_auto.py", line 852, in from_pretrained
        config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\transformers\configuration_utils.py", line 565, in get_config_dict
        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\transformers\configuration_utils.py", line 620, in _get_config_dict
        resolved_config_file = cached_file(
      File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\transformers\utils\hub.py", line 443, in cached_file
        raise EnvironmentError(
    OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like t5-small is not the path to a directory containing a file named config.json.
    Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
    ```

## GenRec (ç¬¬ä¸€æ¬¡å°è¯•ä½†ä¸­æ–­)

ç”±äºä¸Šä¸€ä¸ªä»£ç æš‚æ—¶è§£å†³ä¸äº† `OSError: We couldn't connect to 'https://huggingface.co' to load this file ...` çš„æŠ¥é”™ï¼Œæ‰€ä»¥å¼€å§‹å°è¯•è¿™ç¯‡è®ºæ–‡çš„ä»£ç 

è®ºæ–‡ç½‘å€ï¼š

[GenRec: Large Language Model for Generative Recommendation | Papers With Code](https://paperswithcode.com/paper/text-based-large-language-model-for)

ä»£ç åœ°å€ï¼š

[rutgerswiselab/GenRec: Large Language Model for Generative Recommendation (github.com)](https://github.com/rutgerswiselab/genrec)

å®‰è£…å¥½ç¯å¢ƒä¹‹åï¼Œåˆæ¬¡è¿è¡Œå‘ç°

```bash
ModuleNotFoundError: No module named 'scipy'
```

äºæ˜¯å®‰è£… `scipy` 

ç„¶åå†è¿è¡Œ

```bash
ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`
```

å°è¯• `pip install accelerate` å’Œ `pip install -i https://test.pypi.org/simple/ bitsandbytes` åï¼Œæ˜¾ç¤ºéƒ½å®‰è£…äº†ï¼Œæ‰€ä»¥ä¸Šç½‘æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ï¼Œç„¶åå‘ç°ä¸¤ä¸ª **==ä¸ä¹…å‰çš„å›ç­”==**

-   [python - Accelerate and bitsandbytes is needed to install but I did - Stack Overflow](https://stackoverflow.com/questions/76924239/accelerate-and-bitsandbytes-is-needed-to-install-but-i-did)

    ä¸­ [jasonçš„å›ç­”](https://stackoverflow.com/a/76976563)

-   [anon8231489123/vicuna-13b-GPTQ-4bit-128g Â· I keep getting this: ImportError: Using `load_in_8bit=True` requires Accelerate (huggingface.co)](https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/11)

    ä¸­ [anudeepadiçš„å›ç­”](https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/11#650c4ea403e1ec1fc2ff0714)

éƒ½è¯´äº†å°† `transformers` çš„ç‰ˆæœ¬*é™çº§ downgrade* åˆ° `4.30` å°±å¥½äº†(æˆ‘æœ¬æ¥è‡ªåŠ¨å®‰è£…çš„ç‰ˆæœ¬æ˜¯ `4.35.0.dev0` )(è¿™ä¸ª `transformers` æ€ä¹ˆè¿™ä¹ˆéº»çƒ¦ï¼Œä¸Šä¸€ç¯‡è®ºæ–‡ä¹Ÿæ˜¯å¼„è¿™ç©æ„å¼„äº†å¥½ä¹…ğŸ˜¡ğŸ¤¬)

ä¿®æ”¹ä»¥åå‘ç°å¯ä»¥è¿è¡Œäº†

![run_genrec](../images/run_genrec.png){ loading=lazy }

æœ€åå‡ºç°è¿™æ ·çš„æŠ¥é”™

```bash
Traceback (most recent call last):
  File "E:\Github\code_reproduction\GenRec\rec.py", line 289, in <module>
    fire.Fire(train)
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\fire\core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\fire\core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\fire\core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "E:\Github\code_reproduction\GenRec\rec.py", line 104, in train
    model = LlamaForCausalLM.from_pretrained(
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\transformers\modeling_utils.py", line 2819, in from_pretrained
    raise ValueError(
ValueError:
                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
                        `device_map` to `from_pretrained`. Check
                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
                        for more details.
```

---

>   (10.6)

æˆ‘æ€€ç–‘å¯èƒ½æ˜¯å› ä¸ºè¿™ä¸ªç¯å¢ƒæ²¡æœ‰è£… cuda çš„åŸå› ï¼Œæ‰€ä»¥æ‰“ç®—æ¢ç”¨ä¹‹å‰ OpenP5 çš„ç¯å¢ƒï¼Œäºæ˜¯å¼€å§‹å®‰è£…å¯¹ç›¸åº”çš„åŒ…

ç”±äºå‡ºç°äº†ç›¸ä¼¼çš„ç½‘ç»œé—®é¢˜ï¼Œ

>   ```bash
>   pip install -r requirements.txt
>   ```
>
>   `requirements.txt` ä¸­æœ‰å‡ ä¸ªéœ€è¦ä» github ä¸Šå®‰è£…åŒ…
>
>   ```txt
>   git+https://github.com/huggingface/peft.git
>   git+https://github.com/huggingface/transformers.git
>   ```

æˆ‘æ‰“ç®—ç›´æ¥ `pip install peft` ( `transformers` ä¹‹å‰ `openp5` ç¯å¢ƒä¸Šå·²ç»å®‰è£…è¿‡äº†)

å®‰è£…æ—¶ï¼ŒæŠŠä¹‹å‰ç¯å¢ƒä¸­çš„ `torch-1.8.1+cu111` å¸è½½äº†

```bash
Installing collected packages: mpmath, sympy, psutil, networkx, MarkupSafe, jinja2, torch
  Attempting uninstall: torch
    Found existing installation: torch 1.8.1+cu111
    Uninstalling torch-1.8.1+cu111:
      Successfully uninstalled torch-1.8.1+cu111
```

æ‰€ä»¥åœ¨ç»“æŸååˆé‡æ–°å®‰è£…äº†(è¿˜å¥½ä¹‹å‰çš„ `.whl` æ–‡ä»¶è¿˜ä¿ç•™ç€)

```bash
pip install "torch-1.8.1+cu111-cp39-cp39-win_amd64.whl"
```

ä½†æ˜¯å‘ç° `peft` å’Œ `accelerate` è¦æ±‚æ›´é«˜ç‰ˆæœ¬çš„torchï¼Œ

```bash
peft 0.6.0.dev0 requires torch>=1.13.0, but you have torch 1.8.1+cu111 which is incompatible.
accelerate 0.23.0 requires torch>=1.10.0, but you have torch 1.8.1+cu111 which is incompatible.
```

æ‰€ä»¥æˆ‘æ‰“ç®—å®‰è£…ç‰ˆæœ¬ä½ä¸€äº› `peft` å’Œ `accelerate` 

---

å‘ç° peft 0.4.x çš„ç‰ˆæœ¬ä¹Ÿè¦æ±‚torchæœ€ä½ç‰ˆæœ¬1.13ï¼Œæ‰€ä»¥æˆ‘æ‰“ç®—ç›´æ¥ç»™åŸæ¥çš„ `genrec` ç¯å¢ƒå®‰è£…æ–°çš„ torch ï¼Œ

ä½†æ˜¯å‘ç°ï¼Œtorch 1.13 æœ€ä½åªæ”¯æŒ 11.6 çš„ cuda ï¼Œå› æ­¤åˆå®‰è£…äº† 11.6 çš„ cuda ï¼Œ ç„¶åå®‰è£…äº† torch-1.13.0+cu116

ç„¶åè¿è¡Œå°±å’Œä¹‹å‰æ˜¾ç¤ºçš„ä¿¡æ¯ä¸ä¸€æ ·äº†

```bash
RuntimeError:
        CUDA Setup failed despite GPU being available. Please run the following command to get more information:

        python -m bitsandbytes

        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them
        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes
        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues
```

ä¸Šç½‘æœç´¢åˆ°

[CUDA Setup failed despite GPU being available. Inspect the CUDA SETUP outputs above to fix your environment! Â· Issue #175 Â· TimDettmers/bitsandbytes (github.com)](https://github.com/TimDettmers/bitsandbytes/issues/175)

æ³¨æ„åˆ° [Keith-Honçš„å›ç­”](https://github.com/TimDettmers/bitsandbytes/issues/175#issuecomment-1488003048)å¯èƒ½å¯è¡Œï¼Œå¹¶ä¸” [maximus-sallamä¹Ÿè¯´åˆ°ä»–çš„æ–¹æ³•å¯è¡Œ](https://github.com/TimDettmers/bitsandbytes/issues/175#issuecomment-1502651860)

!!! quote

    I have fixed it by including the .dll and fixed the file path. It now works on windows 10.
    
    [https://github.com/Keith-Hon/bitsandbytes-windows.git](https://github.com/Keith-Hon/bitsandbytes-windows.git)
    
    Install the bitsandbytes library by
    
    pip install bitsandbytes-windows.
    
    Be noted that it may not work directly with transformers library as it references the bitsandbytes package by using 'bitsandbytes' name. <= to avoid this issue, you could directly install from the git repo
    
    pip install git+https://github.com/Keith-Hon/bitsandbytes-windows.git

>   bitsandbytes-windows çš„ github ä»“åº“åœ°å€
>
>   [https://github.com/Keith-Hon/bitsandbytes-windows](https://github.com/Keith-Hon/bitsandbytes-windows)

æ‰€ä»¥æˆ‘å°è¯•

```bash
pip install bitsandbytes-windows
```

ç„¶åå‘ç°å¯è¡Œï¼Œä¹‹å‰çš„æŠ¥é”™æ¶ˆå¤±äº†ï¼Œ

ä½†éšä¹‹è€Œæ¥çš„æ˜¯ï¼Œç¢°åˆ°äº†å’Œä¸Šä¸€ç¯‡è®ºæ–‡ä¸€æ ·çš„é—®é¢˜/æŠ¥é”™(è¿æ¥ä¸äº† `huggingface.co` )

```bash
requests.exceptions.ProxyError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /decapoda-research/llama-7b-hf/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1129)'))))"), '(Request ID: 3eadaf0f-dc33-4690-98b6-d2e5dc10c3ba)')
```

---

### è§£å†³ huggingface.co è¿æ¥ä¸ä¸Šé—®é¢˜ï¼ŒæˆåŠŸä½¿ç”¨ç¦»çº¿æ¨¡å¼

æœç´¢ç›¸å…³ä¿¡æ¯æ—¶ï¼Œå‘ç°äº†ä¸€ç¯‡æ–‡ç« 

[python æŠ¥é”™ requests.exceptions.ConnectionError: HTTPSConnectionPool(host=â€˜huggingface.coâ€˜,port=443):M_requests.exceptions.sslerror: httpsconnectionpool(-CSDNåšå®¢](https://blog.csdn.net/weixin_41862755/article/details/120686319)

æåˆ°äº†å¯ä»¥å°†æ¨¡å‹ä¸‹è½½ä¸‹æ¥ï¼Œå¹¶æåˆ°ä¸‹è½½æ¨¡å‹å¯ä»¥å‚é˜… [å¦‚ä½•ä»huggingfaceå®˜ç½‘ä¸‹è½½æ¨¡å‹-CSDNåšå®¢](https://blog.csdn.net/weixin_41862755/article/details/120686480)

ç„¶åæˆ‘çªç„¶æƒ³åˆ°äº†ï¼Œä¸Šä¸€ç¯‡è®ºæ–‡ä»£ç æœ€åçš„æŠ¥é”™ä¿¡æ¯ä¸­ï¼Œæœ‰æåˆ°

>   ```bash
>   Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
>   ```

æ‰€ä»¥æˆ‘é‡æ–°å¼€å§‹æŸ¥çœ‹å®ƒçš„æ–‡æ¡£å¹¶ç†è§£åº”è¯¥æ€ä¹ˆä½¿ç”¨ç¦»çº¿æ¨¡å¼(ä¹‹å‰ä¹Ÿçœ‹äº†ä½†æ˜¯æ²¡ç†è§£åº”è¯¥æ€ä¹ˆä½¿ç”¨ç¦»çº¿æ¨¡å¼)

[Offline mode - Installation (huggingface.co)  https://huggingface.co/docs/transformers/installation#offline-mode](https://huggingface.co/docs/transformers/installation#offline-mode)

==ç”±äº== csdn é‚£ä¸¤ç¯‡æ–‡ç« éƒ½ ==æ²¡æœ‰æ˜è¯´ä¸‹è½½çš„æ–‡ä»¶åº”è¯¥æ”¾åœ¨å“ªä¸ªæ–‡ä»¶å¤¹==ï¼Œ==æ‰€ä»¥== æœ€åæˆ‘ ==åˆ¤æ–­åº”è¯¥æ˜¯å¯ä»¥è‡ªå®šä¹‰å­˜æ”¾çš„è·¯å¾„== (æˆ‘ä¸€å¼€å§‹ä»¥ä¸ºéœ€è¦æ”¾åœ¨æŒ‡å®šçš„è·¯å¾„ä¸‹)ï¼Œ**äºæ˜¯å¼€å§‹æŸ¥çœ‹åº”è¯¥å¦‚ä½•åŠ è½½è‡ªå®šä¹‰çš„è·¯å¾„**

ç„¶åæˆ‘å…ˆæ˜¯æ³¨æ„åˆ°äº†å®˜æ–¹æ–‡æ¡£ä¸­çš„

```python
from transformers import T5Model

model = T5Model.from_pretrained("./path/to/local/directory", local_files_only=True)
```

è¿™é‡Œæˆ‘è®¤ä¸ºæ˜¯å¼€å§‹åŠ è½½äº†æ¨¡å‹ï¼Œäºæ˜¯æˆ‘åœ¨ `rec.py` ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä»£ç ï¼Œå‘ç° 104 è¡Œæœ‰åŠ è½½æ¨¡å‹çš„ä»£ç 

```python
    model = LlamaForCausalLM.from_pretrained(
        base_model,
        load_in_8bit=True,
        torch_dtype=torch.float16,
        device_map=device_map,
    )
```

ç„¶å ++ctrl++ + ç‚¹å‡» `base_model` æŸ¥çœ‹å…·ä½“å®ƒæ˜¯ä»€ä¹ˆï¼Œç„¶åè·³è½¬åˆ°ç¬¬ 28 è¡Œ

```python
def train(
    # model/data params
    base_model: str = "",  # the only required argument
    data_path: str = "yahma/alpaca-cleaned",
    output_dir: str = "/common/users/jj635/llama/mycheckpoint/",
    # training hyperparams
    batch_size: int = 128,#used to be 128
    micro_batch_size: int = 4,
    num_epochs: int = 3,
    ...
```

ç„¶åæƒ³åˆ°äº†å‘½ä»¤è¡Œè¾“å…¥çš„å‘½ä»¤(ä¹Ÿæ˜¯æ„Ÿè§‰åˆ°è·Ÿä¹‹å‰ä½¿ç”¨ yolov7 æ—¶ï¼Œ*å‘½ä»¤è¡Œå‘½ä»¤å’Œä»£ç ä¸­çš„å¯¹åº”å…³ç³»* æœ‰ç‚¹åƒ)

```bash
python rec.py \
    --base_model 'decapoda-research/llama-7b-hf' \
    --data_path './moives' \
    --output_dir './checkpoint'
```

ç„¶åå†å¯¹æ¯”å®˜æ–¹æ–‡æ¡£ä¸­çš„ï¼Œç¦»çº¿æ¨¡å¼çš„å‘½ä»¤è¡Œå‘½ä»¤

```bash
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

==æ‰€ä»¥å¾—åˆ°ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ³•==ï¼Œå³å…ˆå°†

äºæ˜¯æŒ‰ç…§ [å¦‚ä½•ä»huggingfaceå®˜ç½‘ä¸‹è½½æ¨¡å‹-CSDNåšå®¢](https://blog.csdn.net/weixin_41862755/article/details/120686480) ä¸­å†™çš„ï¼Œåœ¨ [Models - Hugging Face https://huggingface.co/models](https://huggingface.co/models) ä¸­æ‰¾åˆ°/æŸ¥è¯¢åˆ°å¯¹åº”çš„æ¨¡å‹åº“

-   æ ¹æ®ä¹‹å‰çš„æŠ¥é”™ä¿¡æ¯

    ```bash
    ... Max retries exceeded with url: /decapoda-research/llama-7b-hf/resolve/main/config.json ...
    ```

-   æˆ–è€…æ ¹æ®ä½¿ç”¨çš„å‘½ä»¤ä¸­çš„

    ```bash
        --base_model 'decapoda-research/llama-7b-hf' \
    ```

å¯ä»¥æ‰¾åˆ°ç›¸åº”çš„ç½‘å€

[decapoda-research/llama-7b-hf Â· Hugging Face](https://huggingface.co/decapoda-research/llama-7b-hf)

ç„¶åç‚¹å‡» `File and versions` ï¼Œå°±å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½

[decapoda-research/llama-7b-hf at main (huggingface.co)](https://huggingface.co/decapoda-research/llama-7b-hf/tree/main)

æœ€åå¯¹ `rec.py` ä¸­è¿›è¡Œç›¸åº”çš„ä¿®æ”¹ï¼Œä¹‹å‰çš„æŠ¥é”™æ¶ˆå¤±

>   ä½†æ˜¯å‡ºç°äº†å’Œä¹‹å‰(æœ€å¼€å§‹æ—¶)ä¸€æ ·çš„æŠ¥é”™ä¿¡æ¯ğŸ™ƒğŸ™„(æ€ä¹ˆå…œå…œè½¬è½¬å›åˆ°åŸåœ°)
>
>   ```bash
>   ValueError:u
>                           Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit
>                           the quantized model. If you want to dispatch the model on the CPU or the disk while keeping
>                           these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom
>                           `device_map` to `from_pretrained`. Check
>                           https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
>                           for more details.
>   ```

---

ç²—ç•¥å°è¯•äº†

[Quantize ğŸ¤— Transformers models (huggingface.co) https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu)

ä¸Šçš„æ–¹æ³•ï¼Œæˆ‘å°†ä»£ç ä¿®æ”¹æˆ

```python
model = LlamaForCausalLM.from_pretrained(
        # base_model,
        "../llama-7b-hf",
        load_in_8bit=True,
        torch_dtype=torch.float16,
        # device_map=device_map,

        local_files_only=True,
        device_map={
            "transformer.word_embeddings": 0,
            "transformer.word_embeddings_layernorm": 0,
            "lm_head": "cpu",
            "transformer.h": 0,
            "transformer.ln_f": 0,
        },
        quantization_config=BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True),
    )
```

ä½†æœ€åæ˜¾ç¤º

```bash
===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
binary_path: E:\Programs\Anaconda3\envs\genrec\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll
CUDA SETUP: Loading binary E:\Programs\Anaconda3\envs\genrec\lib\site-packages\bitsandbytes\cuda_setup\libbitsandbytes_cuda116.dll...
Training Alpaca-LoRA model with params:
base_model: decapoda-research/llama-7b-hf
data_path: ./moives
output_dir: ./checkpoint
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 0
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: True
group_by_length: False
wandb_project:
wandb_run_name:
wandb_watch:
wandb_log_model:
resume_from_checkpoint: None

Loading checkpoint shards:   0%|                                                                | 0/33 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "E:\Github\code_reproduction\GenRec\rec.py", line 302, in <module>
    fire.Fire(train)
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\fire\core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\fire\core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\fire\core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "E:\Github\code_reproduction\GenRec\rec.py", line 106, in train
    model = LlamaForCausalLM.from_pretrained(
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\transformers\modeling_utils.py", line 2881, in from_pretrained
    ) = cls._load_pretrained_model(
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\transformers\modeling_utils.py", line 3228, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\transformers\modeling_utils.py", line 710, in _load_state_dict_into_meta_model
    raise ValueError(f"{param_name} doesn't have any device set.")
ValueError: model.layers.0.self_attn.q_proj.weight doesn't have any device set.
```

---

çœ‹åˆ°äº†è¿™ä¸ªé—®é¢˜

[google/flan-ul2 Â· ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM (huggingface.co)](https://huggingface.co/google/flan-ul2/discussions/8)

æ ¹æ®å…¶ä¸­[ybelkaba çš„å›ç­”](https://huggingface.co/google/flan-ul2/discussions/8#6436f03af8962b4332ba2644)æ‰€æåˆ°çš„ï¼Œæˆ‘çŒœè¿™ä¸ªæŠ¥é”™åº”è¯¥ç”±äºcpuå’Œgpuå†…å­˜ä¸å¤Ÿå¤§

---

## OpenP5 (ç¬¬äºŒæ¬¡å°è¯•)

ç”±äº GenRec çš„ä»£ç é‡åˆ°çš„é—®é¢˜æš‚æ—¶ä¸çŸ¥é“å¦‚ä½•è§£å†³ï¼Œå¹¶ä¸”ç”±äºè§£å†³äº† huggingface.co çš„ç¦»çº¿ä½¿ç”¨é—®é¢˜ï¼Œæ‰€ä»¥æ‰“ç®—å†æ¬¡å°è¯•è¿è¡Œ OpenP5

é¦–å…ˆæ˜¯ä¸‹è½½æ¨¡å‹ï¼Œç„¶åä¿®æ”¹ç›¸åº”çš„ä»£ç 

æˆ‘åœ¨ `./src/main.py` ç¬¬ 88 è¡Œå¤„æ–°å¢ä¸€è¡Œ `args.backbone = "../../" + args.backbone` ï¼š

```python
    ...
    args.rank = 0
    
    device = torch.device("cuda", int(args.gpu.split(',')[0]))
    # offline set
    args.backbone = "../../" + args.backbone
    ...
```

å¹¶ä¸”æŠŠæ¯ä¸ª `.from_pretrained()` ä¸­éƒ½åŠ ä¸Šäº† `local_files_only=True` ï¼Œå¹¶ä¸”è¿è¡Œ

```bash
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
sh ML1M_random.sh
```

è¿è¡Œæ—¶ï¼Œä¹‹å‰çš„æŠ¥é”™æ¶ˆå¤±äº†ï¼Œå˜æˆäº†

```bash
Traceback (most recent call last):
  File "E:\Github\code_reproduction\OpenP5-old_version\src\main.py", line 241, in <module>
    single_main()
  File "E:\Github\code_reproduction\OpenP5-old_version\src\main.py", line 98, in single_main
    TrainSet, ValidSet = get_dataset(args)
  File "E:\Github\code_reproduction\OpenP5-old_version\src\main.py", line 33, in get_dataset
    TrainDataset = MultiTaskDataset(args, data, 'train')
  File "E:\Github\code_reproduction\OpenP5-old_version\src\data\MultiTaskDataset.py", line 110, in __init__
    dist.barrier()
  File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\torch\distributed\distributed_c10d.py", line 2419, in barrier
    default_pg = _get_default_group()
  File "E:\Programs\Anaconda3\envs\openp5\lib\site-packages\torch\distributed\distributed_c10d.py", line 347, in _get_default_group
    raise RuntimeError("Default process group has not been initialized, "
RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
```

