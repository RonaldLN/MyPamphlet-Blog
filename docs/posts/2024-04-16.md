---
draft: false
date: 2024-04-16
authors:
  - ronald_luo
categories:
  - Configure & Debug
comments: true
---

# 2024 RoboCup@Home *What is that* é¡¹ç›®å‡†å¤‡è®°å½•

ç”±äºè¿™ä¸ªé¡¹ç›®éœ€è¦è¯†åˆ«å‡ºæ‰‹æŒ‡å‘ç‰©ä½“ğŸ‘‰çš„æ‰‹åŠ¿ï¼Œæ‰€ä»¥å°±åœ¨ç½‘ä¸ŠæŸ¥æ‰¾å®ç°äº†ç›¸å…³åŠŸèƒ½çš„ç°æˆçš„åº“ï¼Œ

>   16æ—¥

## æŸ¥é˜…MediaPipeåº“è¯´æ˜

äºæ˜¯æŸ¥åˆ°è°·æ­Œçš„MediaPipeåº“ä¸­æœ‰æ‰‹åŠ¿è¯†åˆ«çš„åŠŸèƒ½ï¼Œ

<!-- more -->

>   -   [Gesture recognition task guide  | MediaPipe  | Google for Developers](https://developers.google.cn/mediapipe/solutions/vision/gesture_recognizer)
>   -   [Hand landmarks detection guide  | MediaPipe  | Google for Developers](https://developers.google.cn/mediapipe/solutions/vision/hand_landmarker)
>
>   >   æ‰‹åŠ¿è¯†åˆ«åŠŸèƒ½çš„**pythonç¤ºä¾‹ä»£ç (åªè¯†åˆ«å›¾ç‰‡)**å’Œ**webç¤ºä¾‹(è°ƒç”¨æ‘„åƒå¤´å®æ—¶è§†é¢‘è¯†åˆ«)**
>   >
>   >   -   [gesture_recognizer.ipynb - Colab (google.com)](https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#scrollTo=O6PN9FvIx614)
>   >   -   [MediaPipe HandGestureRecognizer Task for web (codepen.io)](https://codepen.io/mediapipe-preview/pen/zYamdVd)

ç„¶åå°è¯•è¿è¡Œpythonç¤ºä¾‹ä»£ç å¹¶å®‰è£…ç›¸åº”çš„ç¯å¢ƒï¼Œ

ä¸€ç•ªæ“ä½œåèƒ½æˆåŠŸè¿è¡Œä»£ç å¹¶åœ¨å›¾åƒä¸Šæ˜¾ç¤ºæ‰‹çš„å…³é”®ç‚¹ã€‚

---

## githubå¼€æºæ‰‹åŠ¿è¯†åˆ«ä»“åº“

[åœ¨githubä¸Šæœç´¢æ‰‹åŠ¿è¯†åˆ«ç›¸å…³çš„å¼€æºä»“åº“](https://github.com/search?q=gesture+recognition+&type=repositories)ï¼Œç„¶åå‘ç°ä¸€ä¸ªè¯´æ˜ç”¨æ—¥æ–‡å†™çš„ä½¿ç”¨mediapipeæ¥è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«çš„ä»“åº“

!!! info

    [Kazuhito00/hand-gesture-recognition-using-mediapipe: MediaPipe(Pythonç‰ˆ)ã‚’ç”¨ã„ã¦æ‰‹ã®å§¿å‹¢æ¨å®šã‚’è¡Œã„ã€æ¤œå‡ºã—ãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ç”¨ã„ã¦ã€ç°¡æ˜“ãªMLPã§ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³ã¨ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’èªè­˜ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã™ã€‚ï¼ˆEstimate hand pose using MediaPipe(Python version). This is a sample program that recognizes hand signs and finger gestures with a simple MLP using the detected key points.ï¼‰ (github.com)](https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe)

å…‹éš†å¹¶å°è¯•è¿è¡Œ `app.py` ï¼Œå‘ç°æ²¡æœ‰tensorflowï¼Œç„¶åå®‰è£…tensorflowåå°±èƒ½æ­£å¸¸è¿è¡Œäº†ï¼Œ

>   ç¥å¥‡çš„æ˜¯ï¼Œæˆ‘å·²å®‰è£…çš„pythonåŒ…çš„ç‰ˆæœ¬åŸºæœ¬ä¸Šéƒ½é«˜äº `requirements.txt` ä¸­çš„åŒ…ç‰ˆæœ¬ï¼Œä½†è¿˜æ˜¯å¯ä»¥è¿è¡ŒğŸ˜„

å‘ç°è¿™ä¸ªä»“åº“å¯ä»¥è¯†åˆ«åˆ°æŒ‡ğŸ‘‰çš„æ‰‹åŠ¿(å„ä¸ªæ–¹å‘éƒ½å¯ä»¥ï¼Œç›´æ¥ä½¿ç”¨mediapipeä¼¼ä¹åªèƒ½è¯†åˆ«æŒ‡å‘ä¸Šçš„æ‰‹åŠ¿)ï¼Œ**å¹¶ç‰¹åˆ«æ˜¾ç¤ºæŒ‡å°–çš„å…³é”®ç‚¹**ï¼Œæ‰€ä»¥æ„Ÿè§‰å¾ˆç¬¦åˆWhat is thaté¡¹ç›®çš„è¦æ±‚ï¼Œ

äºæ˜¯å¼€å§‹ç†è§£ `app.py` ä¸­çš„ä»£ç ï¼Œ

å€ŸåŠ©copilotæ¥è§£é‡Š `app.py` ä¸­çš„ `main()` å‡½æ•°

???+ example "å¯¹è¯è®°å½•"

    RonaldLN:
    
    `@workspace` `/explain` 
    
    &nbsp;
    
    GitHub Copilot:
    
    ??? quote "å·²ä½¿ç”¨ 1 å¼•ç”¨"
    
        -   app.py: 41-181
    
    è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªæ‰‹åŠ¿è¯†åˆ«ç¨‹åºçš„ä¸»è¦éƒ¨åˆ†ã€‚å®ƒä½¿ç”¨äº†OpenCVå’ŒMediaPipeåº“æ¥æ•è·å’Œå¤„ç†è§†é¢‘å¸§ï¼Œä»¥è¯†åˆ«å’Œè·Ÿè¸ªæ‰‹çš„ä½ç½®å’Œæ‰‹åŠ¿ã€‚
    
    é¦–å…ˆï¼Œä»£ç é€šè¿‡`get_args()`å‡½æ•°è·å–å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿™äº›å‚æ•°åŒ…æ‹¬è®¾å¤‡IDã€å›¾åƒå®½åº¦å’Œé«˜åº¦ã€æ˜¯å¦ä½¿ç”¨é™æ€å›¾åƒæ¨¡å¼ä»¥åŠæœ€å°æ£€æµ‹å’Œè·Ÿè¸ªç½®ä¿¡åº¦ç­‰ã€‚ç„¶åï¼Œä½¿ç”¨è¿™äº›å‚æ•°åˆå§‹åŒ–ä¸€ä¸ªOpenCVçš„`VideoCapture`å¯¹è±¡ï¼Œç”¨äºä»æŒ‡å®šè®¾å¤‡æ•è·è§†é¢‘ã€‚
    
    æ¥ä¸‹æ¥ï¼Œä»£ç åŠ è½½äº†MediaPipeçš„æ‰‹éƒ¨è§£å†³æ–¹æ¡ˆæ¨¡å‹ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ª`Hands`å¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡ä¼šç”¨äºåç»­çš„æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹ã€‚åŒæ—¶ï¼Œè¿˜åˆå§‹åŒ–äº†ä¸¤ä¸ªåˆ†ç±»å™¨ï¼Œä¸€ä¸ªç”¨äºå…³é”®ç‚¹åˆ†ç±»ï¼Œå¦ä¸€ä¸ªç”¨äºç‚¹å†å²åˆ†ç±»ã€‚
    
    ç„¶åï¼Œä»£ç ä»CSVæ–‡ä»¶ä¸­è¯»å–äº†å…³é”®ç‚¹åˆ†ç±»å™¨å’Œç‚¹å†å²åˆ†ç±»å™¨çš„æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾å°†ç”¨äºåç»­çš„æ‰‹åŠ¿è¯†åˆ«ã€‚
    
    åœ¨ä¸»å¾ªç¯ä¸­ï¼Œä»£ç é¦–å…ˆè·å–å½“å‰çš„FPSå€¼ï¼Œç„¶åæ£€æµ‹æ˜¯å¦æŒ‰ä¸‹äº†ESCé”®ã€‚å¦‚æœæŒ‰ä¸‹äº†ESCé”®ï¼Œç¨‹åºå°†é€€å‡ºã€‚ç„¶åï¼Œä»£ç ä»`VideoCapture`å¯¹è±¡ä¸­è¯»å–ä¸€å¸§å›¾åƒï¼Œå¹¶å°†å…¶ç¿»è½¬ï¼Œä»¥å®ç°é•œåƒæ˜¾ç¤ºã€‚
    
    æ¥ç€ï¼Œä»£ç å°†å›¾åƒä»BGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™`Hands`å¯¹è±¡è¿›è¡Œå¤„ç†ã€‚å¦‚æœåœ¨å›¾åƒä¸­æ£€æµ‹åˆ°äº†æ‰‹ï¼Œ`Hands`å¯¹è±¡å°†è¿”å›ä¸€ä¸ªåŒ…å«æ‰‹éƒ¨å…³é”®ç‚¹çš„ç»“æœã€‚
    
    å¯¹äºæ£€æµ‹åˆ°çš„æ¯ä¸€åªæ‰‹ï¼Œä»£ç é¦–å…ˆè®¡ç®—å…¶å¤–æ¥çŸ©å½¢å’Œå…³é”®ç‚¹åˆ—è¡¨ï¼Œç„¶åå¯¹å…³é”®ç‚¹åˆ—è¡¨å’Œç‚¹å†å²è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä¿å­˜å­¦ä¹ æ•°æ®ã€‚ç„¶åï¼Œä»£ç ä½¿ç”¨å…³é”®ç‚¹åˆ†ç±»å™¨å¯¹é¢„å¤„ç†åçš„å…³é”®ç‚¹åˆ—è¡¨è¿›è¡Œåˆ†ç±»ï¼Œå¦‚æœæ£€æµ‹åˆ°äº†æŒ‡å‘æ‰‹åŠ¿ï¼Œå°±å°†äººæŒ‡å…³é”®ç‚¹æ·»åŠ åˆ°ç‚¹å†å²ä¸­ï¼Œå¦åˆ™æ·»åŠ [0,0]ã€‚
    
    ç„¶åï¼Œä»£ç æ£€æŸ¥ç‚¹å†å²çš„é•¿åº¦ï¼Œå¦‚æœé•¿åº¦ç­‰äºå†å²é•¿åº¦çš„ä¸¤å€ï¼Œå°±ä½¿ç”¨ç‚¹å†å²åˆ†ç±»å™¨å¯¹é¢„å¤„ç†åçš„ç‚¹å†å²è¿›è¡Œåˆ†ç±»ã€‚ç„¶åï¼Œä»£ç å°†åˆ†ç±»ç»“æœæ·»åŠ åˆ°æ‰‹åŠ¿å†å²ä¸­ï¼Œå¹¶è®¡ç®—æœ€å¸¸è§çš„æ‰‹åŠ¿IDã€‚
    
    æœ€åï¼Œä»£ç å°†å¤–æ¥çŸ©å½¢ã€å…³é”®ç‚¹ã€ä¿¡æ¯æ–‡æœ¬ç­‰ç»˜åˆ¶åˆ°å›¾åƒä¸Šï¼Œå¹¶æ˜¾ç¤ºå›¾åƒã€‚å½“é€€å‡ºä¸»å¾ªç¯åï¼Œä»£ç é‡Šæ”¾`VideoCapture`å¯¹è±¡ï¼Œå¹¶é”€æ¯æ‰€æœ‰OpenCVçª—å£ã€‚

å‘ç° `main` ä¸­çš„ *ä¸»å¾ªç¯* éƒ¨åˆ†ä»£ç çš„ç»“æ„å¤§è‡´æ˜¯è¿™æ ·

```python title="app.py"
while True:
    
    ### è·å–å›¾åƒå¹¶è¿›è¡Œå¤„ç†å’Œè¯†åˆ«ï¼Œ
    ### å°†è¯†åˆ«ç»“æœä¿å­˜åœ¨ result ä¸­ï¼Œ
    ### å¦‚æœæ£€æµ‹åˆ°æŒ‰ESCé”®åˆ™é€€å‡ºç¨‹åº

    #  ####################################################################
    if results.multi_hand_landmarks is not None:
        for hand_landmarks, handedness in zip(results.multi_hand_landmarks,
                                              results.multi_handedness):
            ### å¦‚æœè¯†åˆ«åˆ°æ‰‹ï¼Œè¿›è¡Œä¸€äº›å¤„ç†

            ### å¦‚æœè¯†åˆ«åˆ°æ˜¯æŒ‡çš„æ‰‹åŠ¿(hand_sign_id å€¼ä¸º 2)ï¼Œ
            ### å°±å°†æŒ‡å°–çš„åæ ‡æ·»åŠ åˆ°å†å²å…³é”®ç‚¹åˆ—è¡¨ä¸­
            
            # ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³åˆ†é¡
            hand_sign_id = keypoint_classifier(pre_processed_landmark_list)
            if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
                point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™
            else:
                point_history.append([0, 0])

            ### å‰©ä½™çš„ä¸€äº›å…¶ä»–å¤„ç†
            
    else:
        point_history.append([0, 0])

    debug_image = draw_point_history(debug_image, point_history)
    debug_image = draw_info(debug_image, fps, mode, number)

    # ç”»é¢åæ˜  #############################################################
    cv.imshow('Hand Gesture Recognition', debug_image)
```

---

## githubä¸Š23å¹´RoboCup@Home *What is that* é¡¹ç›®å®ç°ä»£ç ä»“åº“

åœ¨[bingä¸Šæœç´¢](https://www.bing.com/search?pglt=131&q=robocup+å®¶æ”¿+what's+that&cvid=f0e6138aeb74424e867b348d21a7aa02&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCTE1NzE3ajBqMagCCLACAQ&FORM=ANNTA1&adppc=EdgeStart&PC=NMTS&mkt=zh-CN)æ—¶ï¼Œæ— æ„ä¸­å‘ç°githubä¸Šæœ‰ä¸€ä¸ªä»“åº“å­˜äº†ä¸œå—å¤§å­¦ä¸€ä¸ªé˜Ÿä¼23å¹´å®Œæˆwhat is thaté¢˜ç›®(ä»¥åŠä¸€äº›å…¶ä»–é¢˜ç›®çš„)ï¼Œ

[JeexiongRen/RoboCup-home-What-is-that (github.com)](https://github.com/JeexiongRen/RoboCup-home-What-is-that)

---

>   17æ—¥

äº†è§£åˆ°ä¼¼ä¹ä¸œå—å¤§å­¦é‚£ä¸ªä»“åº“ä¸å¤ªé€‚åˆï¼Œæ‰€ä»¥æ‰“ç®—å€ŸåŠ©æ—¥æœ¬é‚£ä¸ªä»“åº“æ¥å®ç°ã€‚

## å‚è€ƒ `mrsupw_vison_publisher.py` ç¼–å†™&æµ‹è¯•ç®€å•çš„æ‘„åƒå¤´æ•°æ®çš„è¯»å–

ç”±äºæ‘„åƒå¤´çš„ä½¿ç”¨éœ€è¦è°ƒç”¨ `PyK4A` è¿™ä¸ªåŒ…ï¼Œæ‰€ä»¥å‚è€ƒå·²æœ‰çš„pythonæ–‡ä»¶ï¼Œæ‘˜å–å¿…è¦çš„ä»£ç è¿›è¡Œè¿è¡Œæµ‹è¯•

```python linenums="1"
#!/usr/bin/python3
# -*- coding: utf-8 -*-
import argparse
import sys
import time

import cv2
import rospy
from geometry_msgs.msg import *
from pyk4a import PyK4A, ColorResolution, Config
from std_msgs.msg import *
from xm_msgs.msg import *


def main():
    config = Config(color_resolution=ColorResolution.RES_720P)
    camera = PyK4A(config)
    camera.start()
    capture = camera.get_capture()
    
    cv2.imshow("Image", capture.color)
    print(type(capture.color))
    cv2.waitkey()

if __name__ == '__main__':
    main()
```

è¿è¡Œå‘ç°èƒ½æ˜¾ç¤ºæ‹ç…§çš„å›¾åƒï¼Œå¹¶ä¸” `capture.color` çš„ç±»å‹æ˜¯ `numpy.ndarray` æ‰€ä»¥å¯ä»¥ç›´æ¥ä½¿ç”¨ `cv2.imshow` æ˜¾ç¤ºå‡ºæ¥ã€‚

???+ tip "ä¸ªäººç†è§£"

    ```python
    config = Config(color_resolution=ColorResolution.RES_720P)
    camera = PyK4A(config)
    camera.start()
    ```

    æ˜¯è®¾ç½®æ‘„åƒå¤´å‚æ•°å¹¶å¯åŠ¨æ‘„åƒå¤´

    ```python
    capture = camera.get_capture()
    ```

    è¯»å–å®æ—¶çš„å›¾åƒï¼Œ

    è€Œåœ¨ `capture` ä¸­æœ‰

    -   `color` : å½©è‰²å¸§
    -   `depth` : æ·±åº¦å¸§

    ---

    å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœ `cv2.waitkey(n)` ä¸­è®¾ç½®çš„æ•°å€¼å¤ªå°ï¼Œæˆ–æ²¡æœ‰è®¾ç½®ï¼Œé‚£ä¹ˆä¼šæ˜¾ç¤ºä¸å‡ºå›¾åƒï¼Œ

    ???+ example "å¯¹è¯è®°å½•"

        User:

        ```python
        key = cv.waitKey(10)
        ```

        æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

        &nbsp;

        Wenxin:

        åœ¨OpenCVä¸­ï¼Œ`cv2.waitKey(delay)`å‡½æ•°ç”¨äºç­‰å¾…ä¸€ä¸ªé”®ç›˜äº‹ä»¶ã€‚å‚æ•°`delay`æ˜¯ä»¥æ¯«ç§’ä¸ºå•ä½çš„ç­‰å¾…æ—¶é—´ã€‚è¯¥å‡½æ•°è¿”å›è¢«æŒ‰ä¸‹çš„é”®çš„ASCIIç ï¼Œæˆ–è€…å¦‚æœåœ¨è¿™æ®µæ—¶é—´å†…æ²¡æœ‰æŒ‰é”®è¢«æŒ‰ä¸‹ï¼Œåˆ™è¿”å›-1ã€‚

        åœ¨ä½ æä¾›çš„ä»£ç ç‰‡æ®µä¸­ï¼š

        ```python
        key = cv2.waitKey(10)
        ```

        `cv2.waitKey(10)`ä¼šç­‰å¾…10æ¯«ç§’ï¼Œç„¶åè¿”å›è¢«æŒ‰ä¸‹çš„é”®çš„ASCIIç ï¼Œæˆ–è€…å¦‚æœæ²¡æœ‰æŒ‰é”®è¢«æŒ‰ä¸‹åˆ™è¿”å›-1ã€‚è¿™é€šå¸¸ç”¨åœ¨å¾ªç¯ä¸­ï¼Œä»¥ä¾¿æŒç»­æ˜¾ç¤ºå›¾åƒå¹¶æ£€æµ‹ç”¨æˆ·çš„æŒ‰é”®äº‹ä»¶ã€‚

        ä¾‹å¦‚ï¼Œåœ¨æ˜¾ç¤ºè§†é¢‘æˆ–å®æ—¶å›¾åƒæµæ—¶ï¼Œä½ å¯èƒ½ä¼šè¿™æ ·ä½¿ç”¨`cv2.waitKey`ï¼š

        ```python
        import cv2  

        # å‡è®¾capæ˜¯ä¸€ä¸ªVideoCaptureå¯¹è±¡ï¼Œå®ƒæ‰“å¼€äº†è§†é¢‘æ–‡ä»¶æˆ–ç›¸æœºæµ  
        cap = cv2.VideoCapture(0)  

        while True:  
            # ä»VideoCaptureå¯¹è±¡ä¸­è¯»å–ä¸€å¸§å›¾åƒ  
            ret, frame = cap.read()  

            # å¦‚æœæˆåŠŸè¯»å–å¸§ï¼Œretä¸ºTrue  
            if ret:  
                # æ˜¾ç¤ºå›¾åƒ  
                cv2.imshow('Video', frame)  

                # ç­‰å¾…1æ¯«ç§’ï¼Œæ£€æµ‹æŒ‰é”®  
                key = cv2.waitKey(1)  

                # å¦‚æœæŒ‰ä¸‹'q'é”®ï¼Œé€€å‡ºå¾ªç¯  
                if key == ord('q'):  
                    break  
            else:  
                # å¦‚æœæ²¡æœ‰æˆåŠŸè¯»å–å¸§ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘æ’­æ”¾ç»“æŸï¼‰ï¼Œä¹Ÿé€€å‡ºå¾ªç¯  
                break  

        # é‡Šæ”¾VideoCaptureå¯¹è±¡å¹¶é”€æ¯æ‰€æœ‰çª—å£  
        cap.release()  
        cv2.destroyAllWindows()
        ```

        åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`cv2.waitKey(1)`ä½¿ç¨‹åºæ¯1æ¯«ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æŒ‰é”®è¢«æŒ‰ä¸‹ã€‚å¦‚æœç”¨æˆ·æŒ‰ä¸‹äº†'q'é”®ï¼ˆå…¶ASCIIç ä¸º113ï¼‰ï¼Œåˆ™ç¨‹åºé€šè¿‡`break`è¯­å¥é€€å‡ºå¾ªç¯ã€‚æ³¨æ„ï¼Œ`ord('q')`å‡½æ•°ç”¨äºè·å–å­—ç¬¦'q'çš„ASCIIç ã€‚

        `cv2.waitKey`å‡½æ•°åœ¨å®æ—¶è§†é¢‘å¤„ç†å’Œç”¨æˆ·äº¤äº’ä¸­éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ç¨‹åºåœ¨æ˜¾ç¤ºå›¾åƒçš„åŒæ—¶å“åº”ç”¨æˆ·çš„è¾“å…¥ã€‚

ä»¿ç…§ `hand-gesture-recognition-using-mediapipe` ä¸­çš„ `app.py`

```python
def main():
    config = Config(color_resolution=ColorResolution.RES_720P)
    camera = PyK4A(config)
    camera.start()
    
    while True:
        capture = camera.get_capture()
    
        cv2.imshow("Image", capture.color)
        cv2.imshow("Image2", capture.depth)

        key = cv2.waitkey(10)
        if key == 27:
            break
```

æµ‹è¯•èƒ½æ­£å¸¸æ˜¾ç¤ºå½©è‰²å¸§å’Œæ·±åº¦å¸§çš„è§†é¢‘æµã€‚

---

## ä¿®æ”¹ `hand-gesture-recognition-using-mediapipe`

å…ˆæ˜¯è¯¢é—®äº†copilotä¸€ä¸‹è¯»å–å›¾åƒä¹‹åè¿›è¡Œå¤„ç†çš„éƒ¨åˆ†

???+ example "å¯¹è¯è®°å½•"

    RonaldLN:
    
    `@workspace` `/explain` Write an explanation for the active selection as paragraphs of text.
    
    ```python
    # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
    ret, image = cap.read()
    if not ret:
        break
    image = cv.flip(image, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
    debug_image = copy.deepcopy(image)
    ```
    
    &nbsp;
    
    GitHub Copilot:
    
    ??? quote "å·²ä½¿ç”¨ 6 å¼•ç”¨"
    
        -   app.py: 110-116
        -   .../copy.py: 128
        -   .../cv2/__init__.pyi: 3312
        -   .../cv2/__init__.pyi: 3314
        -   .../cv2/__init__.pyi: 5614
        -   .../cv2/__init__.pyi: 5616
    
    è¿™æ®µä»£ç æ˜¯ä»æ‘„åƒå¤´æ•è·å›¾åƒå¹¶è¿›è¡Œå¤„ç†çš„éƒ¨åˆ†ã€‚
    
    é¦–å…ˆï¼Œä½¿ç”¨ `cap.read()` æ–¹æ³•ä»æ‘„åƒå¤´æ•è·ä¸€å¸§å›¾åƒã€‚è¿™ä¸ªæ–¹æ³•è¿”å›ä¸¤ä¸ªå€¼ï¼šä¸€ä¸ªå¸ƒå°”å€¼ `ret` å’Œä¸€ä¸ªå›¾åƒ `image`ã€‚å¦‚æœ `ret` æ˜¯ `False`ï¼Œé‚£ä¹ˆè¯´æ˜æ²¡æœ‰æˆåŠŸè¯»å–åˆ°å›¾åƒï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‘„åƒå¤´æ²¡æœ‰æ­£ç¡®æ‰“å¼€æˆ–è€…å·²ç»æ²¡æœ‰æ›´å¤šçš„å¸§å¯ä»¥è¯»å–ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»£ç ä¼šæ‰§è¡Œ `break` è¯­å¥ï¼Œè·³å‡ºå½“å‰çš„å¾ªç¯ã€‚
    
    å¦‚æœæˆåŠŸè¯»å–åˆ°å›¾åƒï¼Œé‚£ä¹ˆä»£ç ä¼šä½¿ç”¨ OpenCV çš„ `cv.flip` å‡½æ•°å°†å›¾åƒè¿›è¡Œç¿»è½¬ã€‚è¿™é‡Œçš„å‚æ•° `1` è¡¨ç¤ºæ²¿ç€å‚ç›´è½´ç¿»è½¬å›¾åƒï¼Œä¹Ÿå°±æ˜¯å®ç°äº†é•œåƒæ•ˆæœã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯è®©å›¾åƒçš„æ˜¾ç¤ºæ–¹å¼æ›´ç¬¦åˆäººçš„ç›´è§‰ã€‚
    
    æœ€åï¼Œä»£ç ä½¿ç”¨ `copy.deepcopy` å‡½æ•°åˆ›å»ºäº†å›¾åƒçš„ä¸€ä¸ªæ·±æ‹·è´ï¼Œä¿å­˜åœ¨ `debug_image` å˜é‡ä¸­ã€‚æ·±æ‹·è´æ„å‘³ç€ `debug_image` å’Œ `image` æ˜¯å®Œå…¨ç‹¬ç«‹çš„ä¸¤ä¸ªå¯¹è±¡ï¼Œä¿®æ”¹å…¶ä¸­ä¸€ä¸ªä¸ä¼šå½±å“å¦ä¸€ä¸ªã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†åç»­çš„è°ƒè¯•æˆ–è€…å¤„ç†ï¼Œé¿å…ç›´æ¥ä¿®æ”¹åŸå§‹å›¾åƒã€‚

ç„¶åå°±ä¿®æ”¹äº†éƒ¨åˆ†ä»£ç 

```python
# ã‚«ãƒ¡ãƒ©æº–å‚™ ###############################################################
# cap = cv.VideoCapture(cap_device)
# cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)
# cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)
config = Config(color_resolution=ColorResolution.RES_720P)
camera = PyK4A(config)
camera.start()
```

```python
    while True:
        ...

        # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
        # ret, image = cap.read()
        # if not ret:
        #     break
        image = capture.color
        image = cv.flip(image, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
        debug_image = copy.deepcopy(image)
```

å°è¯•è¿”å›æŒ‡å°–çš„æ·±åº¦å¸§æ•°æ®ï¼Œ

```python
# ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³åˆ†é¡
hand_sign_id = keypoint_classifier(pre_processed_landmark_list)
if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
    point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™
    fingerltip_y, fingerltip_x = andmark_list[8]
    print(capture.depth[fingerltip_x, fingerltip_y])
else:
    point_history.append([0, 0])
```

è¿è¡Œæµ‹è¯•ï¼Œèƒ½å‘æŒ¥ä¹‹é—´çš„æ·±åº¦æ•°æ®ï¼Œä½†æ˜¯å‘ç°å½©è‰²å¸§å’Œæ·±åº¦å¸§çš„å¤§å°å’ŒèŒƒå›´ä¸ä¸€æ ·(å› æ­¤æ˜¯é”™è¯¯çš„æ·±åº¦æ•°æ®)ï¼Œéœ€è¦è¿›è¡Œå¯¹é½

---

>   18æ—¥

## å¯¹é½å½©è‰²å¸§å’Œæ·±åº¦å¸§ï¼Œå®ç°è·å–æ­£ç¡®æ·±åº¦æ•°æ®

å‘ç° `capture` ä¸­æœ‰ `transformed_color` å’Œ `transformed_depth` ï¼Œ

åˆ†åˆ«æ˜¯è°ƒæ•´æˆå’Œæ·±åº¦å¸§ä¸€æ ·å¤§å°å¹¶**æŠŠæ— æ·±åº¦æ•°æ®ä½ç½®çš„åƒç´ å»é™¤**çš„å½©è‰²å¸§ï¼Œå’Œè°ƒæ•´æˆå½©è‰²å¸§å¤§å°çš„æ·±åº¦å¸§ï¼Œ

äºæ˜¯æ‰“ç®—ä½¿ç”¨ `transformed_depth` ï¼Œå¹¶åšäº†ä¸€ä¸‹ä¿®æ”¹

```python hl_lines="6 8"
# ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
# ret, image = cap.read()
# if not ret:
#     break
image = capture.color
depth_image = capture.transformed_depth
image = cv.flip(image, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
depth_image = cv.flip(depth_image, 1)
debug_image = copy.deepcopy(image)
```

```python hl_lines="6"
# ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³åˆ†é¡
hand_sign_id = keypoint_classifier(pre_processed_landmark_list)
if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
    point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™
    fingerltip_y, fingerltip_x = andmark_list[8]
    print(depth_image[fingerltip_x, fingerltip_y])
else:
    point_history.append([0, 0])
```

---

>   29æ—¥

## è§£å†³æ–°ç”µè„‘ `K4AException` çš„é”™è¯¯

ç”±äºæ›´æ¢ä½¿ç”¨æ–°çš„ä¸»æœºç”µè„‘ï¼Œåœ¨æ–°ç”µè„‘ä¸Šå°è¯•ä½¿ç”¨ `PyK4A` æ¥è°ƒç”¨æ‘„åƒå¤´æ—¶ï¼Œå´æ˜¾ç¤ºäº† `K4AException` çš„é”™è¯¯ï¼Œ

è€Œä½¿ç”¨opencvæ¥è°ƒç”¨æ‘„åƒå¤´( `cv2.VideoCapture(0)` )å´èƒ½æ˜¾ç¤ºå‡ºæ‹æ‘„çš„ç”»é¢ï¼Œ

äºæ˜¯å°è¯•è¯¢é—®copilot

???+ example "å¯¹è¯è®°å½•"

    RonaldLN: 
    
    æˆ‘åœ¨å¯åŠ¨æ‘„åƒå¤´æ—¶ï¼Œå¼•èµ·äº† K4AException çš„é”™è¯¯ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ
    
    &nbsp;
    
    GitHub Copilot: 
    
    `K4AException` æ˜¯åœ¨ `_verify_error` å‡½æ•°ä¸­å¼•å‘çš„ï¼Œå½“ `res` å‚æ•°ç­‰äº `Result.Failed` æ—¶ã€‚è¿™é€šå¸¸æ„å‘³ç€ Kinect for Azureï¼ˆK4Aï¼‰æ¨¡å—åœ¨å°è¯•æ‰§è¡ŒæŸé¡¹æ“ä½œæ—¶é‡åˆ°äº†é”™è¯¯ã€‚
    
    è¿™å¯èƒ½æ˜¯ç”±äºå¤šç§åŸå› ï¼Œä¾‹å¦‚ï¼š
    
    -   æ‘„åƒå¤´æœªæ­£ç¡®è¿æ¥æˆ–é…ç½®
    -   é©±åŠ¨ç¨‹åºæˆ–åº“çš„é—®é¢˜
    -   ç¡¬ä»¶æ•…éšœ
    
    ä¸ºäº†æ›´å‡†ç¡®åœ°ç¡®å®šé—®é¢˜ï¼Œä½ å¯èƒ½éœ€è¦æŸ¥çœ‹å¼•å‘ `K4AException` çš„å…·ä½“ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…æŸ¥çœ‹ä»»ä½•ç›¸å…³çš„é”™è¯¯æ—¥å¿—æˆ–æ¶ˆæ¯ã€‚
    
    &nbsp;
    
    RonaldLN: 
    
    æˆ‘ä½¿ç”¨opencv videocaptureèƒ½æ­£å¸¸è°ƒç”¨æ‘„åƒå¤´è¯»å–è§†é¢‘ç”»é¢ï¼Œä½†æ˜¯ä½¿ç”¨pyk4aå´ä¸èƒ½è¯»å–å‡ºå›¾åƒç”»é¢
    
    &nbsp;
    
    GitHub Copilot: 
    
    è¿™å¯èƒ½æ˜¯ç”±äºä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š
    
    1.   Kinect for Azureï¼ˆK4Aï¼‰çš„è®¾å¤‡ç´¢å¼•ä¸ OpenCV çš„è®¾å¤‡ç´¢å¼•ä¸åŒã€‚åœ¨ OpenCV ä¸­ï¼Œä½ å¯èƒ½ä½¿ç”¨çš„æ˜¯è®¾å¤‡ç´¢å¼• 0ï¼Œä½†åœ¨ K4A ä¸­ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„è®¾å¤‡ç´¢å¼•ã€‚
    2.   K4A éœ€è¦ç‰¹å®šçš„é©±åŠ¨ç¨‹åºå’Œåº“ï¼Œè€Œè¿™äº›å¯èƒ½æ²¡æœ‰æ­£ç¡®å®‰è£…æˆ–é…ç½®ã€‚
    3.   K4A å¯èƒ½éœ€è¦æ›´é«˜çš„æƒé™æ‰èƒ½è®¿é—®æ‘„åƒå¤´ã€‚
    
    ä½ å¯ä»¥å°è¯•ä»¥ä¸‹æ­¥éª¤æ¥è§£å†³é—®é¢˜ï¼š
    
    1.   ç¡®ä¿ä½ å·²ç»æ­£ç¡®å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„ K4A é©±åŠ¨ç¨‹åºå’Œåº“ã€‚
    2.   å°è¯•ä½¿ç”¨ä¸åŒçš„è®¾å¤‡ç´¢å¼•æ¥åˆå§‹åŒ– K4Aã€‚
    3.   ç¡®ä¿ä½ çš„ç¨‹åºæœ‰è¶³å¤Ÿçš„æƒé™æ¥è®¿é—®æ‘„åƒå¤´ã€‚
    4.   æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸ K4A ç›¸å…³çš„é”™è¯¯æ—¥å¿—æˆ–æ¶ˆæ¯ï¼Œè¿™å¯èƒ½ä¼šæä¾›æ›´å¤šå…³äºé—®é¢˜çš„ä¿¡æ¯ã€‚
    
    å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œä½ å¯èƒ½éœ€è¦æŸ¥çœ‹æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæˆ–è€…è”ç³» K4A çš„æ”¯æŒå›¢é˜Ÿä»¥è·å–å¸®åŠ©ã€‚

äºæ˜¯æˆ‘æ€€ç–‘è°ƒç”¨è¿™ä¸ªæ‘„åƒå¤´è¿˜æœ‰å…¶ä»–çš„é©±åŠ¨ä¹‹ç±»çš„éœ€è¦å®‰è£…ï¼Œç„¶ååˆåœ¨ `PyK4A` ä»“åº“çš„ `REAME.md` ä¸­çœ‹åˆ°äº†

>   <h2>Prerequisites</h2>
>
>   The [Azure-Kinect-Sensor-SDK](https://github.com/microsoft/Azure-Kinect-Sensor-SDK) is required to build this library. To use the SDK, refer to the installation instructions [here](https://github.com/microsoft/Azure-Kinect-Sensor-SDK/blob/develop/docs/usage.md).

æ‰€ä»¥å‰å»æŸ¥çœ‹è¿™ä¸ªSDKã€‚

>   <h2>Installation</h2>
>
>   To use the SDK, please refer to the installation instructions in [usage](https://github.com/microsoft/Azure-Kinect-Sensor-SDK/blob/develop/docs/usage.md)

å‰å¾€æŸ¥çœ‹å®‰è£…è¯´æ˜

>   -   **AMD64** users, please follow [these instructions](https://docs.microsoft.com/en-us/windows-server/administration/linux-package-repository-for-microsoft-software) to configure Microsoft's Package Repository on your machine.

çœ‹åˆ°é…ç½®çš„æ­¥éª¤(ä½†ä¸ç¡®å®šæ˜¯ä¸æ˜¯å¿…é¡»)

>   <h3>Debian-based Linux distributions</h3>
>
>   -   Download the repo config package: `curl -sSL -O https://packages.microsoft.com/config/<distribution>/<version>/packages-microsoft-prod.deb`
>   -   Install the repo config package: `sudo dpkg -i packages-microsoft-prod.deb`
>   -   Delete the repo config package after installing: `rm packages-microsoft-prod.deb`
>   -   Update package index files: `sudo apt-get update`
>   -   To install the Microsoft product package you're after using this Linux repository (packages.microsoft.com): `sudo apt-get install <package-name>`

ç„¶åå®‰è£…3ä¸ªåŒ…

-   libk4a1.4
-   libk4a1.4-dev
-   k4a-tools

ä¹‹åè¿˜è¦æŒ‰ç…§è¿™é‡Œæ·»åŠ ä¸ªæ–‡ä»¶

[Azure-Kinect-Sensor-SDK/docs/usage.md#device-setup](https://github.com/microsoft/Azure-Kinect-Sensor-SDK/blob/develop/docs/usage.md#device-setup)

>   è¿‡ç¨‹ä¸­å¶ç„¶æœç´¢åˆ°ä¸€ä¸ªæ–‡æ¡£ï¼Œè¯´æ˜äº†ubuntu 18.04ä¸Šå®‰è£…azure kinect sdkçš„å¤§è‡´è¿‡ç¨‹
>
>   [Notes on Setting up the Microsoft Azure Kinect on Ubuntu 18.04 (github.com)](https://gist.github.com/madelinegannon/c212dbf24fc42c1f36776342754d81bc)
>
>   å¹¶é€šè¿‡è¿™ä¸ªæ–‡æ¡£å‘ç°ï¼Œå®‰è£…å¥½ä¹‹åå¯ä»¥é€šè¿‡
>
>   ```bash
>   k4aviewer
>   ```
>
>   æ¥æ‰“å¼€ä¸€ä¸ªå…³äºæ‘„åƒå¤´çš„ç•Œé¢

æœ€åå°±å¯ä»¥æ­£å¸¸ä½¿ç”¨ `PyK4A` è¿™ä¸ªåŒ…æ¥è°ƒç”¨æ‘„åƒå¤´äº†

---

>   (5æœˆ)5æ—¥

ä¸ºäº†èŠ‚çœèµ„æºçš„æ¶ˆè€—ï¼Œå‡å°‘è¯†åˆ«æ‰€éœ€çš„åŒºåŸŸï¼Œæ‰“ç®—æ²¿ç€æ‰‹æŒ‡æ–¹å‘å»¶é•¿ä¸€å®šé•¿åº¦å–ä¸€ä¸ªç‚¹ï¼Œåœ¨ä»¥è¿™ä¸ªç‚¹ä¸ºä¸­å¿ƒå–ä¸€ä¸ªçŸ©å½¢ï¼Œ

å€ŸåŠ©copilotå¾—åˆ°ä»£ç 

```python hl_lines="4-31"
if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
    point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™

    point1, point2, point3 = landmark_list[8], landmark_list[6], landmark_list[5]
    # ç»™å®šä¸¤ç‚¹ï¼Œç”»çº¿
    cv.line(debug_image, (int(point1[0]), int(point1[1])),
            (int(point3[0]), int(point3[1])), (0, 0, 255), 5)

    # å‘point1æ–¹å‘å»¶é•¿æŒ‡å®šè·ç¦»ï¼Œç”»ä¸€ä¸ªç‚¹
    point1 = np.array(point1)
    point2 = np.array(point2)
    point3 = np.array(point3)
    # ä¸¤ç‚¹é—´çš„å‘é‡
    vector = point1 - point3
    # å‘é‡é•¿åº¦
    length = np.sqrt(np.sum(np.square(vector)))
    # å½’ä¸€åŒ–å‘é‡
    unit_vector = vector / length
    # å»¶é•¿é•¿åº¦
    point4 = point1 + unit_vector * 100

    cv.circle(debug_image, (int(point4[0]), int(point4[1])), 5, (0, 0, 255), -1)

    # ä»¥point4ä¸ºä¸­å¿ƒï¼Œç”»ä¸€ä¸ªçŸ©å½¢ï¼Œå¤§å°ä¸ºå›¾åƒçš„20%
    x, y = int(point4[0]), int(point4[1])
    w = int(cap_width * 0.2)
    h = int(cap_height * 0.2)
    cv.rectangle(debug_image, (x - w // 2, y - h // 2), (x + w // 2, y + h // 2), (0, 0, 255), 2)

    # å°†çŸ©å½¢åŒºåŸŸçš„å›¾åƒä¿å­˜ä¸‹æ¥
    cv.imwrite('test.jpg', debug_image[y - h // 2: y + h // 2, x - w // 2: x + w // 2])
```

>   -   `landmark_list[8]` : é£ŸæŒ‡æŒ‡å°–å…³é”®ç‚¹
>   -   `landmark_list[6]` : é£ŸæŒ‡è¿‘ä¾§æŒ‡é—´å…³èŠ‚å…³é”®ç‚¹
>   -   `landmark_list[5]` : é£ŸæŒ‡æŒ‡æ ¹å…³é”®ç‚¹

æ•ˆæœå¦‚ä¸‹ï¼š

![what_is_that_point](../images/what_is_that_point.png){ loading=lazy }

ç”±äºä¹‹åå‘ç°ï¼Œæ¡†æœ‰äº›å°ï¼Œäºæ˜¯å†³å®šåœ¨çºµå‘ä¸Šå¢å¤§ä¸€å€(æ¨ªå‘ä¸Šçš„èŒƒå›´åˆšåˆšå¥½)ï¼Œå¹¶ä¸”æµ‹è¯•æ—¶å‘ç° `imwrite` æœ‰å‡ºç°è¿‡é”™è¯¯ï¼Œäºæ˜¯æ”¾åˆ°äº† `try` é‡Œé¢

```python hl_lines="4 8-11"
# ä»¥point4ä¸ºä¸­å¿ƒï¼Œç”»ä¸€ä¸ªçŸ©å½¢ï¼Œå¤§å°ä¸ºå›¾åƒçš„20%
x, y = int(point4[0]), int(point4[1])
w = int(cap_width * 0.2)
h = int(cap_height * 0.4)
cv.rectangle(debug_image, (x - w // 2, y - h // 2), (x + w // 2, y + h // 2), (0, 0, 255), 2)

# å°†çŸ©å½¢åŒºåŸŸçš„å›¾åƒä¿å­˜ä¸‹æ¥
try:
    cv.imwrite('test.jpg', debug_image[y - h // 2: y + h // 2, x - w // 2: x + w // 2])
except:
    pass
```

