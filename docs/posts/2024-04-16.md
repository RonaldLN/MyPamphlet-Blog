---
draft: true
date: 2024-04-16
authors:
  - ronald_luo
categories:
  - Records of Trivia
comments: true
---

# 2024 RoboCup@Home *What is that* é¡¹ç›®å‡†å¤‡è®°å½•

ç”±äºŽè¿™ä¸ªé¡¹ç›®éœ€è¦è¯†åˆ«å‡ºæ‰‹æŒ‡å‘ç‰©ä½“ðŸ‘‰çš„æ‰‹åŠ¿ï¼Œæ‰€ä»¥å°±åœ¨ç½‘ä¸ŠæŸ¥æ‰¾å®žçŽ°äº†ç›¸å…³åŠŸèƒ½çš„çŽ°æˆçš„åº“ï¼Œ

>   16æ—¥

## æŸ¥é˜…MediaPipeåº“è¯´æ˜Ž

äºŽæ˜¯æŸ¥åˆ°è°·æ­Œçš„MediaPipeåº“ä¸­æœ‰æ‰‹åŠ¿è¯†åˆ«çš„åŠŸèƒ½ï¼Œ

<!-- more -->

>   -   [Gesture recognition task guide  | MediaPipe  | Google for Developers](https://developers.google.cn/mediapipe/solutions/vision/gesture_recognizer)
>   -   [Hand landmarks detection guide  | MediaPipe  | Google for Developers](https://developers.google.cn/mediapipe/solutions/vision/hand_landmarker)
>
>   >   æ‰‹åŠ¿è¯†åˆ«åŠŸèƒ½çš„**pythonç¤ºä¾‹ä»£ç (åªè¯†åˆ«å›¾ç‰‡)**å’Œ**webç¤ºä¾‹(è°ƒç”¨æ‘„åƒå¤´å®žæ—¶è§†é¢‘è¯†åˆ«)**
>   >
>   >   -   [gesture_recognizer.ipynb - Colab (google.com)](https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/gesture_recognizer/python/gesture_recognizer.ipynb#scrollTo=O6PN9FvIx614)
>   >   -   [MediaPipe HandGestureRecognizer Task for web (codepen.io)](https://codepen.io/mediapipe-preview/pen/zYamdVd)

ç„¶åŽå°è¯•è¿è¡Œpythonç¤ºä¾‹ä»£ç å¹¶å®‰è£…ç›¸åº”çš„çŽ¯å¢ƒï¼Œ

ä¸€ç•ªæ“ä½œåŽèƒ½æˆåŠŸè¿è¡Œä»£ç å¹¶åœ¨å›¾åƒä¸Šæ˜¾ç¤ºæ‰‹çš„å…³é”®ç‚¹ã€‚

---

## githubå¼€æºæ‰‹åŠ¿è¯†åˆ«ä»“åº“

[åœ¨githubä¸Šæœç´¢æ‰‹åŠ¿è¯†åˆ«ç›¸å…³çš„å¼€æºä»“åº“](https://github.com/search?q=gesture+recognition+&type=repositories)ï¼Œç„¶åŽå‘çŽ°ä¸€ä¸ªè¯´æ˜Žç”¨æ—¥æ–‡å†™çš„ä½¿ç”¨mediapipeæ¥è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«çš„ä»“åº“

!!! info

    [Kazuhito00/hand-gesture-recognition-using-mediapipe: MediaPipe(Pythonç‰ˆ)ã‚’ç”¨ã„ã¦æ‰‹ã®å§¿å‹¢æŽ¨å®šã‚’è¡Œã„ã€æ¤œå‡ºã—ãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ç”¨ã„ã¦ã€ç°¡æ˜“ãªMLPã§ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³ã¨ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’èªè­˜ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã™ã€‚ï¼ˆEstimate hand pose using MediaPipe(Python version). This is a sample program that recognizes hand signs and finger gestures with a simple MLP using the detected key points.ï¼‰ (github.com)](https://github.com/Kazuhito00/hand-gesture-recognition-using-mediapipe)

å…‹éš†å¹¶å°è¯•è¿è¡Œ `app.py` ï¼Œå‘çŽ°æ²¡æœ‰tensorflowï¼Œç„¶åŽå®‰è£…tensorflowåŽå°±èƒ½æ­£å¸¸è¿è¡Œäº†ï¼Œ

>   ç¥žå¥‡çš„æ˜¯ï¼Œæˆ‘å·²å®‰è£…çš„pythonåŒ…çš„ç‰ˆæœ¬åŸºæœ¬ä¸Šéƒ½é«˜äºŽ `requirements.txt` ä¸­çš„åŒ…ç‰ˆæœ¬ï¼Œä½†è¿˜æ˜¯å¯ä»¥è¿è¡ŒðŸ˜„

å‘çŽ°è¿™ä¸ªä»“åº“å¯ä»¥è¯†åˆ«åˆ°æŒ‡ðŸ‘‰çš„æ‰‹åŠ¿(å„ä¸ªæ–¹å‘éƒ½å¯ä»¥ï¼Œç›´æŽ¥ä½¿ç”¨mediapipeä¼¼ä¹Žåªèƒ½è¯†åˆ«æŒ‡å‘ä¸Šçš„æ‰‹åŠ¿)ï¼Œ**å¹¶ç‰¹åˆ«æ˜¾ç¤ºæŒ‡å°–çš„å…³é”®ç‚¹**ï¼Œæ‰€ä»¥æ„Ÿè§‰å¾ˆç¬¦åˆWhat is thaté¡¹ç›®çš„è¦æ±‚ï¼Œ

äºŽæ˜¯å¼€å§‹ç†è§£ `app.py` ä¸­çš„ä»£ç ï¼Œ

å€ŸåŠ©copilotæ¥è§£é‡Š `app.py` ä¸­çš„ `main()` å‡½æ•°

???+ example "å¯¹è¯è®°å½•"

    SeazSu:
    
    `@workspace` `/explain` 
    
    &nbsp;
    
    GitHub Copilot:
    
    ??? quote "å·²ä½¿ç”¨ 1 å¼•ç”¨"
    
        -   app.py: 41-181
    
    è¿™æ®µä»£ç æ˜¯ä¸€ä¸ªæ‰‹åŠ¿è¯†åˆ«ç¨‹åºçš„ä¸»è¦éƒ¨åˆ†ã€‚å®ƒä½¿ç”¨äº†OpenCVå’ŒMediaPipeåº“æ¥æ•èŽ·å’Œå¤„ç†è§†é¢‘å¸§ï¼Œä»¥è¯†åˆ«å’Œè·Ÿè¸ªæ‰‹çš„ä½ç½®å’Œæ‰‹åŠ¿ã€‚
    
    é¦–å…ˆï¼Œä»£ç é€šè¿‡`get_args()`å‡½æ•°èŽ·å–å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿™äº›å‚æ•°åŒ…æ‹¬è®¾å¤‡IDã€å›¾åƒå®½åº¦å’Œé«˜åº¦ã€æ˜¯å¦ä½¿ç”¨é™æ€å›¾åƒæ¨¡å¼ä»¥åŠæœ€å°æ£€æµ‹å’Œè·Ÿè¸ªç½®ä¿¡åº¦ç­‰ã€‚ç„¶åŽï¼Œä½¿ç”¨è¿™äº›å‚æ•°åˆå§‹åŒ–ä¸€ä¸ªOpenCVçš„`VideoCapture`å¯¹è±¡ï¼Œç”¨äºŽä»ŽæŒ‡å®šè®¾å¤‡æ•èŽ·è§†é¢‘ã€‚
    
    æŽ¥ä¸‹æ¥ï¼Œä»£ç åŠ è½½äº†MediaPipeçš„æ‰‹éƒ¨è§£å†³æ–¹æ¡ˆæ¨¡åž‹ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ª`Hands`å¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡ä¼šç”¨äºŽåŽç»­çš„æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹ã€‚åŒæ—¶ï¼Œè¿˜åˆå§‹åŒ–äº†ä¸¤ä¸ªåˆ†ç±»å™¨ï¼Œä¸€ä¸ªç”¨äºŽå…³é”®ç‚¹åˆ†ç±»ï¼Œå¦ä¸€ä¸ªç”¨äºŽç‚¹åŽ†å²åˆ†ç±»ã€‚
    
    ç„¶åŽï¼Œä»£ç ä»ŽCSVæ–‡ä»¶ä¸­è¯»å–äº†å…³é”®ç‚¹åˆ†ç±»å™¨å’Œç‚¹åŽ†å²åˆ†ç±»å™¨çš„æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾å°†ç”¨äºŽåŽç»­çš„æ‰‹åŠ¿è¯†åˆ«ã€‚
    
    åœ¨ä¸»å¾ªçŽ¯ä¸­ï¼Œä»£ç é¦–å…ˆèŽ·å–å½“å‰çš„FPSå€¼ï¼Œç„¶åŽæ£€æµ‹æ˜¯å¦æŒ‰ä¸‹äº†ESCé”®ã€‚å¦‚æžœæŒ‰ä¸‹äº†ESCé”®ï¼Œç¨‹åºå°†é€€å‡ºã€‚ç„¶åŽï¼Œä»£ç ä»Ž`VideoCapture`å¯¹è±¡ä¸­è¯»å–ä¸€å¸§å›¾åƒï¼Œå¹¶å°†å…¶ç¿»è½¬ï¼Œä»¥å®žçŽ°é•œåƒæ˜¾ç¤ºã€‚
    
    æŽ¥ç€ï¼Œä»£ç å°†å›¾åƒä»ŽBGRæ ¼å¼è½¬æ¢ä¸ºRGBæ ¼å¼ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™`Hands`å¯¹è±¡è¿›è¡Œå¤„ç†ã€‚å¦‚æžœåœ¨å›¾åƒä¸­æ£€æµ‹åˆ°äº†æ‰‹ï¼Œ`Hands`å¯¹è±¡å°†è¿”å›žä¸€ä¸ªåŒ…å«æ‰‹éƒ¨å…³é”®ç‚¹çš„ç»“æžœã€‚
    
    å¯¹äºŽæ£€æµ‹åˆ°çš„æ¯ä¸€åªæ‰‹ï¼Œä»£ç é¦–å…ˆè®¡ç®—å…¶å¤–æŽ¥çŸ©å½¢å’Œå…³é”®ç‚¹åˆ—è¡¨ï¼Œç„¶åŽå¯¹å…³é”®ç‚¹åˆ—è¡¨å’Œç‚¹åŽ†å²è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä¿å­˜å­¦ä¹ æ•°æ®ã€‚ç„¶åŽï¼Œä»£ç ä½¿ç”¨å…³é”®ç‚¹åˆ†ç±»å™¨å¯¹é¢„å¤„ç†åŽçš„å…³é”®ç‚¹åˆ—è¡¨è¿›è¡Œåˆ†ç±»ï¼Œå¦‚æžœæ£€æµ‹åˆ°äº†æŒ‡å‘æ‰‹åŠ¿ï¼Œå°±å°†äººæŒ‡å…³é”®ç‚¹æ·»åŠ åˆ°ç‚¹åŽ†å²ä¸­ï¼Œå¦åˆ™æ·»åŠ [0,0]ã€‚
    
    ç„¶åŽï¼Œä»£ç æ£€æŸ¥ç‚¹åŽ†å²çš„é•¿åº¦ï¼Œå¦‚æžœé•¿åº¦ç­‰äºŽåŽ†å²é•¿åº¦çš„ä¸¤å€ï¼Œå°±ä½¿ç”¨ç‚¹åŽ†å²åˆ†ç±»å™¨å¯¹é¢„å¤„ç†åŽçš„ç‚¹åŽ†å²è¿›è¡Œåˆ†ç±»ã€‚ç„¶åŽï¼Œä»£ç å°†åˆ†ç±»ç»“æžœæ·»åŠ åˆ°æ‰‹åŠ¿åŽ†å²ä¸­ï¼Œå¹¶è®¡ç®—æœ€å¸¸è§çš„æ‰‹åŠ¿IDã€‚
    
    æœ€åŽï¼Œä»£ç å°†å¤–æŽ¥çŸ©å½¢ã€å…³é”®ç‚¹ã€ä¿¡æ¯æ–‡æœ¬ç­‰ç»˜åˆ¶åˆ°å›¾åƒä¸Šï¼Œå¹¶æ˜¾ç¤ºå›¾åƒã€‚å½“é€€å‡ºä¸»å¾ªçŽ¯åŽï¼Œä»£ç é‡Šæ”¾`VideoCapture`å¯¹è±¡ï¼Œå¹¶é”€æ¯æ‰€æœ‰OpenCVçª—å£ã€‚

å‘çŽ° `main` ä¸­çš„ *ä¸»å¾ªçŽ¯* éƒ¨åˆ†ä»£ç çš„ç»“æž„å¤§è‡´æ˜¯è¿™æ ·

```python title="app.py"
while True:
    
    ### èŽ·å–å›¾åƒå¹¶è¿›è¡Œå¤„ç†å’Œè¯†åˆ«ï¼Œ
    ### å°†è¯†åˆ«ç»“æžœä¿å­˜åœ¨ result ä¸­ï¼Œ
    ### å¦‚æžœæ£€æµ‹åˆ°æŒ‰ESCé”®åˆ™é€€å‡ºç¨‹åº

    #  ####################################################################
    if results.multi_hand_landmarks is not None:
        for hand_landmarks, handedness in zip(results.multi_hand_landmarks,
                                              results.multi_handedness):
            ### å¦‚æžœè¯†åˆ«åˆ°æ‰‹ï¼Œè¿›è¡Œä¸€äº›å¤„ç†

            ### å¦‚æžœè¯†åˆ«åˆ°æ˜¯æŒ‡çš„æ‰‹åŠ¿(hand_sign_id å€¼ä¸º 2)ï¼Œ
            ### å°±å°†æŒ‡å°–çš„åæ ‡æ·»åŠ åˆ°åŽ†å²å…³é”®ç‚¹åˆ—è¡¨ä¸­
            
            # ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³åˆ†é¡ž
            hand_sign_id = keypoint_classifier(pre_processed_landmark_list)
            if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
                point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™
            else:
                point_history.append([0, 0])

            ### å‰©ä½™çš„ä¸€äº›å…¶ä»–å¤„ç†
            
    else:
        point_history.append([0, 0])

    debug_image = draw_point_history(debug_image, point_history)
    debug_image = draw_info(debug_image, fps, mode, number)

    # ç”»é¢åæ˜  #############################################################
    cv.imshow('Hand Gesture Recognition', debug_image)
```

---

## githubä¸Š23å¹´RoboCup@Home *What is that* é¡¹ç›®å®žçŽ°ä»£ç ä»“åº“

åœ¨[bingä¸Šæœç´¢](https://www.bing.com/search?pglt=131&q=robocup+å®¶æ”¿+what's+that&cvid=f0e6138aeb74424e867b348d21a7aa02&gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCTE1NzE3ajBqMagCCLACAQ&FORM=ANNTA1&adppc=EdgeStart&PC=NMTS&mkt=zh-CN)æ—¶ï¼Œæ— æ„ä¸­å‘çŽ°githubä¸Šæœ‰ä¸€ä¸ªä»“åº“å­˜äº†ä¸œå—å¤§å­¦ä¸€ä¸ªé˜Ÿä¼23å¹´å®Œæˆwhat is thaté¢˜ç›®(ä»¥åŠä¸€äº›å…¶ä»–é¢˜ç›®çš„)ï¼Œ

[JeexiongRen/RoboCup-home-What-is-that (github.com)](https://github.com/JeexiongRen/RoboCup-home-What-is-that)

---

>   17æ—¥

äº†è§£åˆ°ä¼¼ä¹Žä¸œå—å¤§å­¦é‚£ä¸ªä»“åº“ä¸å¤ªé€‚åˆï¼Œæ‰€ä»¥æ‰“ç®—å€ŸåŠ©æ—¥æœ¬é‚£ä¸ªä»“åº“æ¥å®žçŽ°ã€‚

## å‚è€ƒ `mrsupw_vison_publisher.py` ç¼–å†™&æµ‹è¯•ç®€å•çš„æ‘„åƒå¤´æ•°æ®çš„è¯»å–

ç”±äºŽæ‘„åƒå¤´çš„ä½¿ç”¨éœ€è¦è°ƒç”¨ `PyK4A` è¿™ä¸ªåŒ…ï¼Œæ‰€ä»¥å‚è€ƒå·²æœ‰çš„pythonæ–‡ä»¶ï¼Œæ‘˜å–å¿…è¦çš„ä»£ç è¿›è¡Œè¿è¡Œæµ‹è¯•

```python linenums="1"
#!/usr/bin/python3
# -*- coding: utf-8 -*-
import argparse
import sys
import time

import cv2
import rospy
from geometry_msgs.msg import *
from pyk4a import PyK4A, ColorResolution, Config
from std_msgs.msg import *
from xm_msgs.msg import *


def main():
    config = Config(color_resolution=ColorResolution.RES_720P)
    camera = PyK4A(config)
    camera.start()
    capture = camera.get_capture()
    
    cv2.imshow("Image", capture.color)
    print(type(capture.color))
    cv2.waitkey()

if __name__ == '__main__':
    main()
```

è¿è¡Œå‘çŽ°èƒ½æ˜¾ç¤ºæ‹ç…§çš„å›¾åƒï¼Œå¹¶ä¸” `capture.color` çš„ç±»åž‹æ˜¯ `numpy.ndarray` æ‰€ä»¥å¯ä»¥ç›´æŽ¥ä½¿ç”¨ `cv2.imshow` æ˜¾ç¤ºå‡ºæ¥ã€‚

>   (ä¸ªäººç†è§£)
>
>   ```python
>   config = Config(color_resolution=ColorResolution.RES_720P)
>   camera = PyK4A(config)
>   camera.start()
>   ```
>
>   æ˜¯è®¾ç½®æ‘„åƒå¤´å‚æ•°å¹¶å¯åŠ¨æ‘„åƒå¤´
>
>   ```python
>   capture = camera.get_capture()
>   ```
>
>   è¯»å–å®žæ—¶çš„å›¾åƒï¼Œ
>
>   è€Œåœ¨ `capture` ä¸­æœ‰
>
>   -   `color` : å½©è‰²å¸§
>   -   `depth` : æ·±åº¦å¸§
>
>   ---
>
>   å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æžœ `cv2.waitkey(n)` ä¸­è®¾ç½®çš„æ•°å€¼å¤ªå°ï¼Œæˆ–æ²¡æœ‰è®¾ç½®ï¼Œé‚£ä¹ˆä¼šæ˜¾ç¤ºä¸å‡ºå›¾åƒï¼Œ
>
>   ???+ example "å¯¹è¯è®°å½•"
>
>       User:
>                       
>       ```python
>       key = cv.waitKey(10)
>       ```
>                       
>       æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ
>                       
>       &nbsp;
>                       
>       Wenxin:
>                       
>       åœ¨OpenCVä¸­ï¼Œ`cv2.waitKey(delay)`å‡½æ•°ç”¨äºŽç­‰å¾…ä¸€ä¸ªé”®ç›˜äº‹ä»¶ã€‚å‚æ•°`delay`æ˜¯ä»¥æ¯«ç§’ä¸ºå•ä½çš„ç­‰å¾…æ—¶é—´ã€‚è¯¥å‡½æ•°è¿”å›žè¢«æŒ‰ä¸‹çš„é”®çš„ASCIIç ï¼Œæˆ–è€…å¦‚æžœåœ¨è¿™æ®µæ—¶é—´å†…æ²¡æœ‰æŒ‰é”®è¢«æŒ‰ä¸‹ï¼Œåˆ™è¿”å›ž-1ã€‚
>                       
>       åœ¨ä½ æä¾›çš„ä»£ç ç‰‡æ®µä¸­ï¼š
>                       
>       ```python
>       key = cv2.waitKey(10)
>       ```
>                       
>       `cv2.waitKey(10)`ä¼šç­‰å¾…10æ¯«ç§’ï¼Œç„¶åŽè¿”å›žè¢«æŒ‰ä¸‹çš„é”®çš„ASCIIç ï¼Œæˆ–è€…å¦‚æžœæ²¡æœ‰æŒ‰é”®è¢«æŒ‰ä¸‹åˆ™è¿”å›ž-1ã€‚è¿™é€šå¸¸ç”¨åœ¨å¾ªçŽ¯ä¸­ï¼Œä»¥ä¾¿æŒç»­æ˜¾ç¤ºå›¾åƒå¹¶æ£€æµ‹ç”¨æˆ·çš„æŒ‰é”®äº‹ä»¶ã€‚
>                       
>       ä¾‹å¦‚ï¼Œåœ¨æ˜¾ç¤ºè§†é¢‘æˆ–å®žæ—¶å›¾åƒæµæ—¶ï¼Œä½ å¯èƒ½ä¼šè¿™æ ·ä½¿ç”¨`cv2.waitKey`ï¼š
>                       
>       ```python
>       import cv2  
>                         
>       # å‡è®¾capæ˜¯ä¸€ä¸ªVideoCaptureå¯¹è±¡ï¼Œå®ƒæ‰“å¼€äº†è§†é¢‘æ–‡ä»¶æˆ–ç›¸æœºæµ  
>       cap = cv2.VideoCapture(0)  
>                         
>       while True:  
>           # ä»ŽVideoCaptureå¯¹è±¡ä¸­è¯»å–ä¸€å¸§å›¾åƒ  
>           ret, frame = cap.read()  
>                             
>           # å¦‚æžœæˆåŠŸè¯»å–å¸§ï¼Œretä¸ºTrue  
>           if ret:  
>               # æ˜¾ç¤ºå›¾åƒ  
>               cv2.imshow('Video', frame)  
>                                 
>               # ç­‰å¾…1æ¯«ç§’ï¼Œæ£€æµ‹æŒ‰é”®  
>               key = cv2.waitKey(1)  
>                                 
>               # å¦‚æžœæŒ‰ä¸‹'q'é”®ï¼Œé€€å‡ºå¾ªçŽ¯  
>               if key == ord('q'):  
>                   break  
>           else:  
>               # å¦‚æžœæ²¡æœ‰æˆåŠŸè¯»å–å¸§ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘æ’­æ”¾ç»“æŸï¼‰ï¼Œä¹Ÿé€€å‡ºå¾ªçŽ¯  
>               break  
>                         
>       # é‡Šæ”¾VideoCaptureå¯¹è±¡å¹¶é”€æ¯æ‰€æœ‰çª—å£  
>       cap.release()  
>       cv2.destroyAllWindows()
>       ```
>                       
>       åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`cv2.waitKey(1)`ä½¿ç¨‹åºæ¯1æ¯«ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰æŒ‰é”®è¢«æŒ‰ä¸‹ã€‚å¦‚æžœç”¨æˆ·æŒ‰ä¸‹äº†'q'é”®ï¼ˆå…¶ASCIIç ä¸º113ï¼‰ï¼Œåˆ™ç¨‹åºé€šè¿‡`break`è¯­å¥é€€å‡ºå¾ªçŽ¯ã€‚æ³¨æ„ï¼Œ`ord('q')`å‡½æ•°ç”¨äºŽèŽ·å–å­—ç¬¦'q'çš„ASCIIç ã€‚
>                       
>       `cv2.waitKey`å‡½æ•°åœ¨å®žæ—¶è§†é¢‘å¤„ç†å’Œç”¨æˆ·äº¤äº’ä¸­éžå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå…è®¸ç¨‹åºåœ¨æ˜¾ç¤ºå›¾åƒçš„åŒæ—¶å“åº”ç”¨æˆ·çš„è¾“å…¥ã€‚

ä»¿ç…§ `hand-gesture-recognition-using-mediapipe` ä¸­çš„ `app.py`

```python
def main():
    config = Config(color_resolution=ColorResolution.RES_720P)
    camera = PyK4A(config)
    camera.start()
    
    while True:
        capture = camera.get_capture()
    
        cv2.imshow("Image", capture.color)
        cv2.imshow("Image2", capture.depth)

        key = cv2.waitkey(10)
        if key == 27:
            break
```

æµ‹è¯•èƒ½æ­£å¸¸æ˜¾ç¤ºå½©è‰²å¸§å’Œæ·±åº¦å¸§çš„è§†é¢‘æµã€‚

---

## ä¿®æ”¹ `hand-gesture-recognition-using-mediapipe`

å…ˆæ˜¯è¯¢é—®äº†copilotä¸€ä¸‹è¯»å–å›¾åƒä¹‹åŽè¿›è¡Œå¤„ç†çš„éƒ¨åˆ†

???+ example "å¯¹è¯è®°å½•"

    SeazSu:
    
    `@workspace` `/explain` Write an explanation for the active selection as paragraphs of text.
    
    ```python
    # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
    ret, image = cap.read()
    if not ret:
        break
    image = cv.flip(image, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
    debug_image = copy.deepcopy(image)
    ```
    
    &nbsp;
    
    GitHub Copilot:
    
    ??? quote "å·²ä½¿ç”¨ 6 å¼•ç”¨"
    
        -   app.py: 110-116
        -   .../copy.py: 128
        -   .../cv2/__init__.pyi: 3312
        -   .../cv2/__init__.pyi: 3314
        -   .../cv2/__init__.pyi: 5614
        -   .../cv2/__init__.pyi: 5616
    
    è¿™æ®µä»£ç æ˜¯ä»Žæ‘„åƒå¤´æ•èŽ·å›¾åƒå¹¶è¿›è¡Œå¤„ç†çš„éƒ¨åˆ†ã€‚
    
    é¦–å…ˆï¼Œä½¿ç”¨ `cap.read()` æ–¹æ³•ä»Žæ‘„åƒå¤´æ•èŽ·ä¸€å¸§å›¾åƒã€‚è¿™ä¸ªæ–¹æ³•è¿”å›žä¸¤ä¸ªå€¼ï¼šä¸€ä¸ªå¸ƒå°”å€¼ `ret` å’Œä¸€ä¸ªå›¾åƒ `image`ã€‚å¦‚æžœ `ret` æ˜¯ `False`ï¼Œé‚£ä¹ˆè¯´æ˜Žæ²¡æœ‰æˆåŠŸè¯»å–åˆ°å›¾åƒï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‘„åƒå¤´æ²¡æœ‰æ­£ç¡®æ‰“å¼€æˆ–è€…å·²ç»æ²¡æœ‰æ›´å¤šçš„å¸§å¯ä»¥è¯»å–ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»£ç ä¼šæ‰§è¡Œ `break` è¯­å¥ï¼Œè·³å‡ºå½“å‰çš„å¾ªçŽ¯ã€‚
    
    å¦‚æžœæˆåŠŸè¯»å–åˆ°å›¾åƒï¼Œé‚£ä¹ˆä»£ç ä¼šä½¿ç”¨ OpenCV çš„ `cv.flip` å‡½æ•°å°†å›¾åƒè¿›è¡Œç¿»è½¬ã€‚è¿™é‡Œçš„å‚æ•° `1` è¡¨ç¤ºæ²¿ç€åž‚ç›´è½´ç¿»è½¬å›¾åƒï¼Œä¹Ÿå°±æ˜¯å®žçŽ°äº†é•œåƒæ•ˆæžœã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯è®©å›¾åƒçš„æ˜¾ç¤ºæ–¹å¼æ›´ç¬¦åˆäººçš„ç›´è§‰ã€‚
    
    æœ€åŽï¼Œä»£ç ä½¿ç”¨ `copy.deepcopy` å‡½æ•°åˆ›å»ºäº†å›¾åƒçš„ä¸€ä¸ªæ·±æ‹·è´ï¼Œä¿å­˜åœ¨ `debug_image` å˜é‡ä¸­ã€‚æ·±æ‹·è´æ„å‘³ç€ `debug_image` å’Œ `image` æ˜¯å®Œå…¨ç‹¬ç«‹çš„ä¸¤ä¸ªå¯¹è±¡ï¼Œä¿®æ”¹å…¶ä¸­ä¸€ä¸ªä¸ä¼šå½±å“å¦ä¸€ä¸ªã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯ä¸ºäº†åŽç»­çš„è°ƒè¯•æˆ–è€…å¤„ç†ï¼Œé¿å…ç›´æŽ¥ä¿®æ”¹åŽŸå§‹å›¾åƒã€‚

ç„¶åŽå°±ä¿®æ”¹äº†éƒ¨åˆ†ä»£ç 

```python
# ã‚«ãƒ¡ãƒ©æº–å‚™ ###############################################################
# cap = cv.VideoCapture(cap_device)
# cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)
# cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)
config = Config(color_resolution=ColorResolution.RES_720P)
camera = PyK4A(config)
camera.start()
```

```python
    while True:
        ...

        # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
        # ret, image = cap.read()
        # if not ret:
        #     break
        image = capture.color
        image = cv.flip(image, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
        debug_image = copy.deepcopy(image)
```

å°è¯•è¿”å›žæŒ‡å°–çš„æ·±åº¦å¸§æ•°æ®ï¼Œ

```python
# ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³åˆ†é¡ž
hand_sign_id = keypoint_classifier(pre_processed_landmark_list)
if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
    point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™
    fingerltip_y, fingerltip_x = andmark_list[8]
    print(capture.depth[fingerltip_x, fingerltip_y])
else:
    point_history.append([0, 0])
```

è¿è¡Œæµ‹è¯•ï¼Œèƒ½å‘æŒ¥ä¹‹é—´çš„æ·±åº¦æ•°æ®ï¼Œä½†æ˜¯å‘çŽ°å½©è‰²å¸§å’Œæ·±åº¦å¸§çš„å¤§å°å’ŒèŒƒå›´ä¸ä¸€æ ·(å› æ­¤æ˜¯é”™è¯¯çš„æ·±åº¦æ•°æ®)ï¼Œéœ€è¦è¿›è¡Œå¯¹é½

---

>   18æ—¥

## å¯¹é½å½©è‰²å¸§å’Œæ·±åº¦å¸§ï¼Œå®žçŽ°èŽ·å–æ­£ç¡®æ·±åº¦æ•°æ®

å‘çŽ° `capture` ä¸­æœ‰ `transformed_color` å’Œ `transformed_depth` ï¼Œ

åˆ†åˆ«æ˜¯è°ƒæ•´æˆå’Œæ·±åº¦å¸§ä¸€æ ·å¤§å°å¹¶**æŠŠæ— æ·±åº¦æ•°æ®ä½ç½®çš„åƒç´ åŽ»é™¤**çš„å½©è‰²å¸§ï¼Œå’Œè°ƒæ•´æˆå½©è‰²å¸§å¤§å°çš„æ·±åº¦å¸§ï¼Œ

äºŽæ˜¯æ‰“ç®—ä½¿ç”¨ `transformed_depth` ï¼Œå¹¶åšäº†ä¸€ä¸‹ä¿®æ”¹

```python hl_lines="6 8"
# ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
# ret, image = cap.read()
# if not ret:
#     break
image = capture.color
depth_image = capture.transformed_depth
image = cv.flip(image, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
depth_image = cv.flip(depth_image, 1)
debug_image = copy.deepcopy(image)
```

```python hl_lines="6"
# ãƒãƒ³ãƒ‰ã‚µã‚¤ãƒ³åˆ†é¡ž
hand_sign_id = keypoint_classifier(pre_processed_landmark_list)
if hand_sign_id == 2:  # æŒ‡å·®ã—ã‚µã‚¤ãƒ³
    point_history.append(landmark_list[8])  # äººå·®æŒ‡åº§æ¨™
    fingerltip_y, fingerltip_x = andmark_list[8]
    print(depth_image[fingerltip_x, fingerltip_y])
else:
    point_history.append([0, 0])
```

---

>   29æ—¥

## è§£å†³æ–°ç”µè„‘ `K4AException` çš„é”™è¯¯

ç”±äºŽæ›´æ¢ä½¿ç”¨æ–°çš„ä¸»æœºç”µè„‘ï¼Œåœ¨æ–°ç”µè„‘ä¸Šå°è¯•ä½¿ç”¨ `PyK4A` æ¥è°ƒç”¨æ‘„åƒå¤´æ—¶ï¼Œå´æ˜¾ç¤ºäº† `K4AException` çš„é”™è¯¯ï¼Œ

è€Œä½¿ç”¨opencvæ¥è°ƒç”¨æ‘„åƒå¤´( `cv2.VideoCapture(0)` )å´èƒ½æ˜¾ç¤ºå‡ºæ‹æ‘„çš„ç”»é¢ï¼Œ

äºŽæ˜¯å°è¯•è¯¢é—®copilot

???+ example "å¯¹è¯è®°å½•"

    SeazSu: 
    
    æˆ‘åœ¨å¯åŠ¨æ‘„åƒå¤´æ—¶ï¼Œå¼•èµ·äº† K4AException çš„é”™è¯¯ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ
    
    &nbsp;
    
    GitHub Copilot: 
    
    `K4AException` æ˜¯åœ¨ `_verify_error` å‡½æ•°ä¸­å¼•å‘çš„ï¼Œå½“ `res` å‚æ•°ç­‰äºŽ `Result.Failed` æ—¶ã€‚è¿™é€šå¸¸æ„å‘³ç€ Kinect for Azureï¼ˆK4Aï¼‰æ¨¡å—åœ¨å°è¯•æ‰§è¡ŒæŸé¡¹æ“ä½œæ—¶é‡åˆ°äº†é”™è¯¯ã€‚
    
    è¿™å¯èƒ½æ˜¯ç”±äºŽå¤šç§åŽŸå› ï¼Œä¾‹å¦‚ï¼š
    
    -   æ‘„åƒå¤´æœªæ­£ç¡®è¿žæŽ¥æˆ–é…ç½®
    -   é©±åŠ¨ç¨‹åºæˆ–åº“çš„é—®é¢˜
    -   ç¡¬ä»¶æ•…éšœ
    
    ä¸ºäº†æ›´å‡†ç¡®åœ°ç¡®å®šé—®é¢˜ï¼Œä½ å¯èƒ½éœ€è¦æŸ¥çœ‹å¼•å‘ `K4AException` çš„å…·ä½“ä¸Šä¸‹æ–‡ï¼Œæˆ–è€…æŸ¥çœ‹ä»»ä½•ç›¸å…³çš„é”™è¯¯æ—¥å¿—æˆ–æ¶ˆæ¯ã€‚
    
    &nbsp;
    
    SeazSu: 
    
    æˆ‘ä½¿ç”¨opencv videocaptureèƒ½æ­£å¸¸è°ƒç”¨æ‘„åƒå¤´è¯»å–è§†é¢‘ç”»é¢ï¼Œä½†æ˜¯ä½¿ç”¨pyk4aå´ä¸èƒ½è¯»å–å‡ºå›¾åƒç”»é¢
    
    &nbsp;
    
    GitHub Copilot: 
    
    è¿™å¯èƒ½æ˜¯ç”±äºŽä»¥ä¸‹å‡ ä¸ªåŽŸå› ï¼š
    
    1.   Kinect for Azureï¼ˆK4Aï¼‰çš„è®¾å¤‡ç´¢å¼•ä¸Ž OpenCV çš„è®¾å¤‡ç´¢å¼•ä¸åŒã€‚åœ¨ OpenCV ä¸­ï¼Œä½ å¯èƒ½ä½¿ç”¨çš„æ˜¯è®¾å¤‡ç´¢å¼• 0ï¼Œä½†åœ¨ K4A ä¸­ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒçš„è®¾å¤‡ç´¢å¼•ã€‚
    2.   K4A éœ€è¦ç‰¹å®šçš„é©±åŠ¨ç¨‹åºå’Œåº“ï¼Œè€Œè¿™äº›å¯èƒ½æ²¡æœ‰æ­£ç¡®å®‰è£…æˆ–é…ç½®ã€‚
    3.   K4A å¯èƒ½éœ€è¦æ›´é«˜çš„æƒé™æ‰èƒ½è®¿é—®æ‘„åƒå¤´ã€‚
    
    ä½ å¯ä»¥å°è¯•ä»¥ä¸‹æ­¥éª¤æ¥è§£å†³é—®é¢˜ï¼š
    
    1.   ç¡®ä¿ä½ å·²ç»æ­£ç¡®å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„ K4A é©±åŠ¨ç¨‹åºå’Œåº“ã€‚
    2.   å°è¯•ä½¿ç”¨ä¸åŒçš„è®¾å¤‡ç´¢å¼•æ¥åˆå§‹åŒ– K4Aã€‚
    3.   ç¡®ä¿ä½ çš„ç¨‹åºæœ‰è¶³å¤Ÿçš„æƒé™æ¥è®¿é—®æ‘„åƒå¤´ã€‚
    4.   æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸Ž K4A ç›¸å…³çš„é”™è¯¯æ—¥å¿—æˆ–æ¶ˆæ¯ï¼Œè¿™å¯èƒ½ä¼šæä¾›æ›´å¤šå…³äºŽé—®é¢˜çš„ä¿¡æ¯ã€‚
    
    å¦‚æžœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œä½ å¯èƒ½éœ€è¦æŸ¥çœ‹æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæˆ–è€…è”ç³» K4A çš„æ”¯æŒå›¢é˜Ÿä»¥èŽ·å–å¸®åŠ©ã€‚

äºŽæ˜¯æˆ‘æ€€ç–‘è°ƒç”¨è¿™ä¸ªæ‘„åƒå¤´è¿˜æœ‰å…¶ä»–çš„é©±åŠ¨ä¹‹ç±»çš„éœ€è¦å®‰è£…ï¼Œç„¶åŽåˆåœ¨ `PyK4A` ä»“åº“çš„ `REAME.md` ä¸­çœ‹åˆ°äº†

>   <h2>Prerequisites</h2>
>
>   The [Azure-Kinect-Sensor-SDK](https://github.com/microsoft/Azure-Kinect-Sensor-SDK) is required to build this library. To use the SDK, refer to the installation instructions [here](https://github.com/microsoft/Azure-Kinect-Sensor-SDK/blob/develop/docs/usage.md).

æ‰€ä»¥å‰åŽ»æŸ¥çœ‹è¿™ä¸ªSDKã€‚

æŸ¥çœ‹å®‰è£…è¯´æ˜Ž

>   <h2>Installation</h2>
>
>   To use the SDK, please refer to the installation instructions in [usage](https://github.com/microsoft/Azure-Kinect-Sensor-SDK/blob/develop/docs/usage.md)

