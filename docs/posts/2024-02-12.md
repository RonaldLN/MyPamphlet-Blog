---
draft: false
date: 2024-02-12
authors:
  - ronald_luo
categories:
  - Chronicle of Events
comments: true
---

# 2024æ•°æ¨¡ç¾èµ›å‚èµ›çºªå®(ç¼–ç¨‹æ‰‹è§†è§’)

>   2024-02-02 --- 2024-02-06

!!! tip

    æ¯”èµ›æ—¶é—´ä¸º 2å·æ—©ä¸Š6ç‚¹å…¬å¸ƒé¢˜ç›®ï¼Œ6å·æ—©ä¸Š9ç‚¹å¼€å§‹æäº¤è®ºæ–‡(åŒ—äº¬æ—¶é—´)

## Day 1

### é˜…è¯»é¢˜ç›® (æ—©)

æ—©ä¸Š8ç‚¹èµ·æ¥ï¼Œ6ç‚¹èµ·æ¥çš„é˜Ÿå‹å·²ç»å°†æ¯ä¸ªé¢˜ç›®éƒ½å¤§è‡´çœ‹äº†ä¸€éï¼Œç„¶åå¼€å§‹è¾¹åƒæ—©é¤è¾¹çœ‹é¢˜ç›®è¾¹å’Œé˜Ÿå‹è®¨è®ºé¢˜ç›®ã€‚

<!-- more -->

é˜Ÿå‹è¯´ï¼Œè¿˜æ˜¯æ‰“ç®—åœ¨MCMçš„ä¸‰é¢˜(A B Cé¢˜)é‡Œé¢é€‰ï¼Œæ‰€ä»¥æˆ‘å°±ç€é‡çœ‹äº†è¿™ä¸‰é¢˜ï¼ŒD E Fä¸‰é¢˜å°±åªæ˜¯ç®€å•æ‰«äº†ä¸€çœ¼äº†è§£äº†ä¸€ä¸‹ğŸ˜…ã€‚

Aé¢˜æ˜¯å…³äº *ä¸ƒé³ƒé³— lampreys* çš„ï¼Œå¤§æ¦‚æ˜¯ç”Ÿæ€ç›¸å…³çš„é—®é¢˜ã€‚

Bé¢˜æ˜¯å…³äºæ½œæ°´è‰‡ï¼Œä½†çœ‹å®Œåé¢çš„é—®é¢˜åï¼Œæˆ‘æ„Ÿè§‰åœ¨å»ºç«‹ç›¸å…³çš„æ¨¡å‹æ—¶ï¼Œä¼šéœ€è¦åœ°ç†ç›¸å…³çš„ä¸€äº›æ•°æ®ä»¥åŠçŸ¥è¯†ï¼Œæ„Ÿè§‰ä¸æ˜¯å¾ˆå¥½æåˆ°è¿™äº›æ•°æ®ã€‚ç„¶åé˜Ÿå‹ä¹Ÿè¯´åœ¨è¯»é¢˜ç›®æ—¶æœ‰ä¸€ä¸ªæœ¯è¯­éƒ½ä¸èƒ½å‡†ç¡®ç†è§£å…·ä½“æ˜¯ä»€ä¹ˆæ„æ€ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼˜å…ˆæ’é™¤äº†Bé¢˜ã€‚

Cé¢˜æ˜¯å…³äºç½‘çƒï¼Œå¤§æ¦‚æ˜¯éœ€è¦æ ¹æ®æä¾›çš„æ¯”èµ›æ—¶çš„æ•°æ®ï¼Œæ‰¾åˆ°æŸç§æ½œåœ¨çš„æ¯”èµ›çƒå‘˜èµ¢çƒçš„ ***åŠ¿å¤´ momentum***ã€‚

è¿™æ—¶æˆ‘å’Œé˜Ÿå‹ç»è¿‡è¾¹çœ‹é¢˜è¾¹è®¨è®ºï¼Œå¤§æ¦‚ç¡®å®šäº†é€‰Aé¢˜æˆ–Cé¢˜ï¼Œä½†è¿˜æ²¡æœ€åç¡®å®šé€‰å“ªä¸€é¢˜(é˜Ÿå‹è¯´ä»–æ‰“ç®—ç¬¬ä¸€å¤©ä¸ŠåˆæŠŠé¢˜ç›®ç¡®å®šå¥½å°±è¡Œäº†)ï¼Œç„¶åæˆ‘ä»¬å°±å¼€å§‹å„è‡ªå»æŸ¥æ‰¾èµ„æ–™(æŸ¥æ‰¾æ›´å¤šé¢˜ç›®ç›¸å…³çš„èµ„æ–™æ¥æ›´æ·±å…¥äº†è§£é¢˜ç›®ï¼Œä»¥ä¾¿æœ€ç»ˆçš„é€‰é¢˜ç¡®å®š)äº†ã€‚

### äº†è§£ç½‘çƒè§„åˆ™

ç”±äºæˆ‘ç†è§£çš„Cé¢˜æ˜¯éœ€è¦å¯»æ‰¾æŸç§æ½œåœ¨çš„å…³ç³»æˆ–è€…è¯´è§„å¾‹ï¼Œæ‰€ä»¥æˆ‘è§‰å¾—è¿™å¾ˆé€‚åˆç”¨**æœºå™¨å­¦ä¹ **æ¥åšï¼ŒæŠŠCé¢˜æ‰€ç»™çš„æ•°æ®æ‹¿å»è·‘ã€‚

äºæ˜¯æˆ‘å°±å¼€å§‹å»äº†è§£ç½‘çƒçš„è§„åˆ™(é¢˜ç›®éœ€è¦)ï¼Œåœ¨bç«™ä¸ŠæŸ¥çœ‹ä»‹ç»ç½‘çƒæ¯”èµ›è§„åˆ™çš„ç›¸å…³è§†é¢‘ï¼Œ

-   [ç½‘çƒè§„åˆ™è§£é‡Šï¼ˆå¾—åˆ†ï¼Œæœ¯è¯­ç­‰ï¼‰\_å“”å“©å“”å“©\_bilibili](https://b23.tv/oILc36E)
-   [ç½‘çƒè®°åˆ†è§„åˆ™ï½œAlexçš„çŸ¥è¯†åˆ†äº«ï¼Œå†…é™„Alexäº²è‡ªä¸Šåœºç¤ºèŒƒï½\_å“”å“©å“”å“©\_bilibili](https://b23.tv/WxzHiT1)

çœ‹å®Œè¿™ä¸¤ä¸ªè§†é¢‘ï¼Œæ˜ç™½äº†ç½‘çƒè§„åˆ™ä¸­çš„ä¸€äº›åŸºç¡€çš„æ¦‚å¿µï¼Œä½†è¿˜æƒ³å†æ‰¾æ›´å¤šçš„è§†é¢‘è¿›è¡Œç¡®è®¤ï¼Œè€Œbç«™ä¸Šæ„Ÿè§‰æ²¡æœ‰ä»€ä¹ˆæ¯”è¾ƒå¥½ä¸”çŸ­çš„ç½‘çƒè§„åˆ™ä»‹ç»è§†é¢‘äº†ï¼Œäºæ˜¯æˆ‘çªç„¶æƒ³åˆ°å¯ä»¥å»æ²¹ç®¡ä¸Šæ‰¾ï¼Œç„¶åå°±æ‰¾åˆ°ä¸€ä¸ªå¸®åŠ©å¾ˆå¤§çš„è§†é¢‘(ä¸»è¦æ˜¯è‹±æ–‡çš„åˆšå¥½èƒ½è·Ÿé¢˜ç›®ä¸­çš„æœ¯è¯­è¯æ±‡å¯¹åº”ä¸Š)ï¼Œ

-   [The Rules of Tennis - EXPLAINED! - YouTube](https://youtu.be/1P_Das6OjAk)

### ä¸è€å¸ˆè®¨è®ºï¼Œç¡®å®šé€‰Cé¢˜

åˆ°å·®ä¸å¤š11ç‚¹å·¦å³ï¼Œæˆ‘ä»¬è·ŸæŒ‡å¯¼è€å¸ˆè¿›è¡Œäº†è®¨è®ºï¼Œè€å¸ˆå½“æ—¶ä¸»è¦çœ‹Cé¢˜ï¼Œå¹¶ä¸”ä¹Ÿæåˆ°äº†å¯ä»¥è¿ç”¨æœºå™¨å­¦ä¹ å»å‘æ˜æ½œåœ¨çš„å…³äº*åŠ¿å¤´*çš„è§„å¾‹ï¼Œ(æˆ‘çš„æƒ³æ³•æ„Ÿè§‰è·Ÿè€å¸ˆä¸è°‹è€Œåˆ)äºæ˜¯æˆ‘ä¹Ÿè·Ÿè€å¸ˆè¯´æ„Ÿè§‰è¿™é¢˜é€‚åˆç”¨ ç¥ç»ç½‘ç»œ æœºå™¨å­¦ä¹  æ¥å¤„ç†ã€‚

(æˆ‘ç”±äºå¯¹æ¨¡å‹å’Œæƒé‡çš„æ¦‚å¿µè¿˜æœ‰äº›æ²¡å˜æ¸…ï¼Œäºæ˜¯å‘è€å¸ˆè¯¢é—®äº†ä¸€ä¸‹)è€å¸ˆå°±è¿›ä¸€æ­¥è¯´åˆ°ï¼Œè¿™é¢˜å¤§è‡´çš„æ–¹æ¡ˆå¯ä»¥æ˜¯å…ˆæ„å»ºå‡ºæ¨¡å‹ï¼Œç„¶åæ ¹æ®æ¯ä¸ªé€‰æ‰‹çš„æ•°æ®è®­ç»ƒå‡ºæƒé‡ï¼Œæœ€ååº”ç”¨æƒé‡åœ¨å…·ä½“çš„æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹/ä¼°ç®—ã€‚

å› æ­¤æˆ‘ä»¬æœ€åå†³å®šäº†å°±é€‰Cé¢˜ã€‚

### å¼„æ¸…æ¥šå­—å…¸ä¸­çš„å„ä¸ªæœ¯è¯­

ä»11ç‚¹åŠç»“æŸä¸è€å¸ˆçš„è®¨è®ºï¼Œæˆ‘ä»¬å¼€å§‹ç ”ç©¶å­—å…¸( `data_dictionary.csv` )ä¸­çš„é‚£äº›å˜é‡åç§°ï¼Œä¸€ç›´åˆ°12ç‚¹å·¦å³å„è‡ªå»åƒé¥­ã€‚å…¶ä¸­å‡ ä¸ªæ¯”è¾ƒå…³é”®çš„æ˜¯

-   `serve_no` å‘çƒæ¬¡æ•°ï¼Œå³(åœ¨å½“å‰åˆ†ä¸­)ç¬¬1æ¬¡è¿˜æ˜¯ç¬¬2æ¬¡å‘çƒ(ç¬¬2æ¬¡å‘çƒåŒ…å«*åŒè¯¯ double fault*çš„æƒ…å†µ)
-   `ace` å‘çƒå¾—åˆ†ï¼Œä»…é€šè¿‡å‘çƒç›´æ¥èµ¢å¾—è¿™ä¸€åˆ†(åœ¨æ²¹ç®¡è§†é¢‘ä¸­æœ‰ä»‹ç»)
-   `xx_winner` æ­£é¢èµ¢çƒï¼Œé€šè¿‡æ‰“å‡ºè®©å¯¹æ‰‹æ¥ä¸åˆ°çš„çƒè€Œå¾—åˆ†(ä¸ä¹‹ç›¸å¯¹çš„æ˜¯å¯¹æ‰‹å¤±è¯¯è€Œå¾—åˆ†)ï¼Œå› æ­¤å¯èƒ½å­˜åœ¨ p1 p2 éƒ½ä¸º 0 çš„æƒ…å†µ
-   `xx_double_fault` åŒè¯¯ï¼Œå‘çƒæ–¹åœ¨å½“å‰åˆ†å‘çƒä¸¤æ¬¡éƒ½å¤±è¯¯(ä¸‹ç½‘æˆ–å‘çƒå‡ºç•Œ)ï¼Œåˆ™ä¼šä½¿å¯¹æ–¹å¾—åˆ†
-   `xx_unf_err` *éå—è¿«æ€§å¤±è¯¯ unforced error* ï¼Œç±»æ¯”ç¾½æ¯›çƒä¸­çš„éå—è¿«æ€§å¤±è¯¯ï¼Œå¤§æ¦‚åº”è¯¥æ˜¯ä¸»åŠ¨å¤±è¯¯ä¸¢åˆ†ï¼Œåƒåœ¨**ä¸æ˜¯æ‹¿ç‚¹éå¸¸ä¸å¥½æˆ–è€…éå¸¸è¢«åŠ¨**çš„æƒ…å†µä¸‹è‡ªå·±å¤±è¯¯ä¸¢åˆ†
-   `xx_net_pt` ç½‘å‰å›çƒï¼Œé€‰æ‰‹æœ€åèµ¢/è¾“çš„é‚£ä¸€çƒæ˜¯åœ¨ç½‘å‰å›çš„
-   `xx_net_pt_won` ç½‘å‰å¾—åˆ†ï¼Œä¸ä¸Šé¢ç±»ä¼¼ï¼Œåªä¸è¿‡è¿™çƒæ˜¯èµ¢äº†(æ„Ÿè§‰å¯èƒ½æ„å‘³ç€æ‹¿ç‚¹å¾ˆå¥½ï¼Œå› æ­¤ä¼šæ¶¨æ°”åŠ¿æ¶¨ä¿¡å¿ƒ)
-   `xx_break_pt` åœ¨è¿™ä¸€å±€ä¸­ï¼Œä½œä¸ºæ¥å‘æ–¹ï¼Œå¹¶ä¸”å¦‚æœèµ¢ä¸‹å½“å‰åˆ†åˆ™èµ¢ä¸‹è¿™ä¸€å±€

>   å› æ­¤æœ€åæˆ‘å¼„æ¸…æ¥šçš„ç½‘çƒæ¯”åˆ†çš„è§„åˆ™å’Œæ¯”èµ›å¾—åˆ†çš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼š

??? tip

    ç½‘çƒæ¯”èµ›çš„è®°åˆ†ä»å¤§åˆ°å°æœ‰ ***ç›˜ set*** ***å±€ game*** ***åˆ† point*** ï¼Œé€šå¸¸æ˜¯
    
    -   **5ç›˜3èƒœ**(ç”·å•æ˜¯è¿™æ ·ï¼Œå…¶ä»–å¥½åƒæœ‰äº›ä¼šä¸ä¸€æ ·)ï¼Œå³èµ¢ä¸‹3ç›˜èµ¢å¾—è¿™åœºæ¯”èµ›
    
    -   æ¯ç›˜ä¸­ï¼Œèµ¢ä¸‹6å±€åˆ™èµ¢å¾—è¿™ç›˜æ¯”èµ›ï¼Œä½†æ˜¯**éœ€è¦é¢†å…ˆä¸¤å±€**ï¼Œå³å¦‚æœå‡ºç° 5:5 åˆ™éœ€è¦èµ¢ä¸‹7å±€(ç±»æ¯”ç¾½æ¯›çƒä¸­ 20:20 çš„åŠ åˆ†)ï¼Œè€Œ 6:6 åˆ™éœ€è¦æ‰“ ***å†³èƒœå±€ tiebreaker*** ï¼Œèµ¢ä¸‹*å†³èƒœå±€*å°±èµ¢å¾—è¿™ç›˜(ç±»æ¯”ç¾½æ¯›çƒä¸­çš„29:29)
    
    -   æ¯å±€ä¸­ï¼Œéœ€è¦èµ¢ä¸‹4åˆ†ï¼Œä¸”ä¹Ÿæœ‰ç±»ä¼¼äºé¢†å…ˆä¸¤åˆ†æ‰èƒ½èµ¢å¾—è¿™å±€çš„è§„å®šã€‚
    
        å…¶ä¸­ï¼Œè¿˜æœ‰ ***åˆ†æ•° score*** çš„æ¦‚å¿µ(æ¥å†å¯ä»¥çœ‹ä¸Šé¢çš„ç¬¬äºŒä¸ªè§†é¢‘)ï¼Œå¯¹åº”å…³ç³»æ˜¯
    
        | åˆ† point |   åˆ†æ•° score    |
        | :------: | :-------------: |
        |    0     | 0 (ä½†è¯»ä½œ love) |
        |    1     |       15        |
        |    2     |       30        |
        |    3     |       40        |
    
        å¦‚æœå¯¹æ‰‹è¿™ä¸€å±€æ²¡æœ‰æ‹¿åˆ°è¿‡3åˆ†ï¼Œé‚£ä¹ˆè‡ªå·±åªéœ€è¦èµ¢ä¸‹4åˆ†å³å¯èµ¢å¾—è¿™ä¸€å±€ï¼Œå¦‚æœå¯¹æ‰‹æ‹¿åˆ°äº†3åˆ†ï¼Œå³åˆ†æ•°ä¸­çš„40ï¼Œåˆ™åœ¨ä¹‹åçš„æ¯”åˆ†ä¸­ä¼šå‡ºç° **AD (advantage)** ï¼Œ**AD**å¯ä»¥ç›´æ¥å½“ä½œ**ç¬¬4åˆ†**ï¼Œ
    
        è€Œå‡ºç°äº†åˆ†æ•°40:40(å³3:3)ä¹‹åçš„è®°åˆ†è§„åˆ™æ˜¯ï¼Œåœ¨40:40(è¿™ä¸ªç‰¹æ®Šå±€é¢ä¹Ÿå«åš Deuceï¼Œä½†æ˜¯åœ¨è¿™ä¸€é¢˜çš„æ•°æ®ä¸­æ²¡æœ‰å‡ºç°ç›¸å…³çš„æ•°æ®)æ—¶ èµ¢ä¸‹è¿™åˆ†çš„è¿›å…¥ADï¼Œåœ¨ä¸‹ä¸€åˆ†ä¸­ï¼Œåˆ†æ•°ä¸ºADçš„å¦‚æœèµ¢åˆ™èµ¢å¾—è¿™ä¸€å±€ï¼Œå¦‚æœè¾“åˆ™é€€å›åˆ†æ•°40(å¯ä»¥ç†è§£æˆåˆ†æ•°ä¼šå‡å°‘ï¼Œå³å¦‚æœ4:3è¾“äº†å°±ä¼šå˜æˆ3:3)
    
        ---
    
        åœ¨*å†³èƒœå±€ tiebreaker*ä¸­ï¼Œåˆ™æ˜¯éœ€è¦èµ¢ä¸‹**è‡³å°‘**7åˆ†(å¦‚æœ*ç›˜*æ¯”ä¸º2:2ï¼Œåˆ™æ˜¯éœ€è¦èµ¢ä¸‹è‡³å°‘10åˆ†(æ‰€ä»¥ä¹Ÿè¢«å«åš**æŠ¢ä¸ƒ**æˆ–**æŠ¢å**))(**è‡³å°‘**ä¹Ÿæ˜¯å› ä¸ºéœ€è¦æ¯”å¯¹æ‰‹å¤šè‡³å°‘ä¸¤åˆ†ï¼Œå¦‚æœ7:6åˆ™éœ€è¦åƒç¾½æ¯›çƒä¸€æ ·åŠ åˆ†ï¼Œå¹¶ä¸”æ— ä¸Šé™)
    
    ---
    
    å‘çƒä¸æ¥å‘ï¼Œ**æ¯ä¸€å±€ä¸­å‘çƒæ–¹æ˜¯å›ºå®šçš„**ï¼Œè€Œä¸‹ä¸€å±€æ¢æˆå¯¹æ–¹ä½œä¸ºå‘çƒæ–¹ï¼Œæ— è®ºè¿™å±€è°èµ¢è°è¾“(å¯ä»¥å‚è€ƒä¸Šé¢ç¬¬äºŒä¸ªè§†é¢‘ååŠéƒ¨åˆ†çš„æ¼”ç¤º)
    
    ---
    
    åœ¨æ¯ä¸€åˆ†ä¸­ï¼Œå‘çƒæ–¹æœ‰ä¸¤æ¬¡å‘çƒçš„æœºä¼š(å³ç¬¬ä¸€æ¬¡å‘çƒå¦‚æœå¤±è¯¯ï¼Œè¿˜èƒ½å†æ¬¡å‘çƒï¼Œå³ç¬¬ä¸€æ¬¡å‘çƒå¤±è¯¯ä¸ä¸¢åˆ†)ï¼Œå¦‚æœä¸¤æ¬¡å‘çƒå‡å¤±è¯¯ï¼Œå³ä¸º*åŒè¯¯ double fault*ï¼Œåˆ™å¤±å»è¿™ä¸€åˆ†/å¯¹æ–¹å¾—åˆ†ï¼Œ
    
    è€Œå‘çƒæ–¹å‘çƒæˆåŠŸ/åˆæ³•å‘çƒä¹‹åï¼Œåˆ™è¿›å…¥æ­£å¸¸çš„å¯¹å±€ï¼Œå³å¦‚æœå“ªä¸€æ–¹æ²¡æœ‰æˆåŠŸå›çƒæˆ–è€…å›çƒå‡ºç•Œ/çŠ¯è§„ç­‰ï¼Œå¦ä¸€æ–¹å¾—åˆ†(å°±å’Œç¾½æ¯›çƒä¸€æ ·äº†)

### æŸ¥æ‰¾ç›¸å…³æ–‡çŒ®æ–‡ç«  (åˆ)

ä¸‹åˆä»2ç‚¹åŠå·¦å³åˆ°4ç‚¹åŠå·¦å³ï¼Œæˆ‘å’Œé˜Ÿå‹åœ¨æŸ¥æ‰¾è¿ç”¨ æœºå™¨å­¦ä¹  ç¥ç»ç½‘ç»œ æ¥å¯¹æ¯”èµ›å½¢åŠ¿è¿›è¡Œé¢„æµ‹çš„ç›¸å…³æ–‡çŒ®æ–‡ç« ï¼Œ

æˆ‘ä¸»è¦æ‰¾åˆ°äº†è¿™ä¸¤ç¯‡æ–‡çŒ®ï¼Œ

>   -   [A deep learning framework for football match prediction | Discover Applied Sciences (springer.com)](https://link.springer.com/article/10.1007/s42452-019-1821-5)
>   -   [Computers | Free Full-Text | On Predicting Soccer Outcomes in the Greek League Using Machine Learning (mdpi.com)](https://www.mdpi.com/2073-431X/11/9/133)

æœ¬æ¥æˆ‘æ˜¯åƒé€šè¿‡æŸ¥æ‰¾æ–‡çŒ®æ¥çœ‹èƒ½ä¸èƒ½æ‰¾åˆ°ä¸€äº›å¯¹äºç¼–ç¨‹æ–¹é¢æœ‰ç”¨çš„ä¸€äº›ä¸œè¥¿ï¼Œä½†æ˜¯å‘ç°ä¸å¤ªè¡Œï¼Œ

æ‰€ä»¥åœ¨è·Ÿé˜Ÿå‹è®¨è®ºäº†ä¸€ä¸‹è¿™ä¸¤ç¯‡ä¹‹åï¼Œå°±è®©ä»–å»å‚è€ƒè¿™ä¸¤ç¯‡æ–‡ç« å»å¼„å†™ä½œçš„äº‹æƒ…å»äº†ï¼Œ

### åˆæ­¥å¤ä¹ ä»¥åŠè¿›ä¸€æ­¥äº†è§£æœºå™¨å­¦ä¹ /ç¥ç»ç½‘ç»œ

ç„¶åæˆ‘å¼€å§‹åœ¨bç«™ä¸Šæœç´¢ æœºå™¨å­¦ä¹ /ç¥ç»ç½‘ç»œ ç›¸å…³çš„å­¦ä¹ è§†é¢‘ï¼Œæˆ‘è¿™æ—¶çš„æƒ³æ³•æ˜¯ï¼Œæ‰¾åˆ°ä¸€ä¸ªæ•™æ€ä¹ˆå»å®ç°ä½¿ç”¨æœºå™¨å­¦ä¹ /ç¥ç»ç½‘ç»œçš„è§†é¢‘ï¼Œç„¶åæˆ‘å…ˆè·Ÿç€è§†é¢‘å»å®ç°/å¤ç°ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œç„¶åå†æ ¹æ®é¢˜ç›®çš„è¦æ±‚å»ä¿®æ”¹ä»£ç ï¼Œ==æœ€ç»ˆèƒ½å°†é¢˜ç›®æä¾›çš„æ•°æ®ç”¨æ¥è®­ç»ƒ==

ç»è¿‡ä¸€ç•ªæœç´¢ï¼Œæˆ‘å‘ç°äº†**è¿™ä¸ªè¯¾ç¨‹**(ä¸€ä¸ªéå¸¸å¥½çš„è¯¾ç¨‹ï¼Œä¹‹å‰åœ¨23å¹´4æœˆåœ¨å®ŒæˆåŸºåœ°å¸ƒç½®çš„è€ƒæ ¸ä»»åŠ¡æ—¶ï¼Œä»»åŠ¡ä¸­ä¹Ÿæœ‰æåˆ°è¿™ä¸ªè¯¾ç¨‹)ï¼Œ

[åˆé›†Â·æ–¯å¦ç¦2021ç§‹å­£Â·å®ç”¨æœºå™¨å­¦ä¹ ã€ä¸­æ–‡ã€‘ã€åˆé›†ã€‘-å“”å“©å“”å“©](https://b23.tv/9N119Ny)

æµè§ˆäº†ä¸€ä¸‹æ¯èŠ‚è¯¾çš„æ ‡é¢˜ï¼Œç„¶åç›´æ¥ä»[3.1èŠ‚ 8åˆ†é’Ÿæœºå™¨å­¦ä¹ ä»‹ç»](https://b23.tv/Ovnm772)å¼€å§‹çœ‹ã€‚

>   çœ‹åˆ°äº†5ç‚¹å¤šï¼Œç„¶å6ç‚¹é˜Ÿå‹æƒ³è·Ÿæˆ‘è®¨è®ºå¦ä¸€ç¯‡æ–‡çŒ®ï¼Œè®¨è®ºäº†å¤§çº¦åŠå°æ—¶ä¹‹åå»åƒé¥­äº†ï¼Œåƒå®Œé¥­åˆæ„Ÿè§‰æœ‰ç‚¹å›°ä¸å°å¿ƒç¡ç€äº†ğŸ˜…ï¼Œåˆ°9ç‚¹å·¦å³é†’è¿‡æ¥äº†ï¼Œäºæ˜¯ç»§ç»­å¼€å§‹çœ‹

æœ€ååˆ°è¿™å¤©æ™šä¸Šæœ€åï¼Œå¤§ä½“çœ‹äº†è¿™5èŠ‚è¯¾

-   [3.1 8åˆ†é’Ÿæœºå™¨å­¦ä¹ ä»‹ç»](https://b23.tv/Ovnm772)
-   [3.3 æœ€ç®€å•ä¹ŸåŒæ ·æœ€å¸¸ç”¨çš„çº¿æ€§æ¨¡å‹](https://b23.tv/18zab4N)
-   [3.5 å¤šå±‚æ„ŸçŸ¥æœº](https://b23.tv/a8ERuyM)
-   [3.6 å·ç§¯ç¥ç»ç½‘ç»œ](https://b23.tv/fzoPdHl)
-   [3.7 å¾ªç¯ç¥ç»ç½‘ç»œ](https://b23.tv/qS1V9QJ)

é‡æ–°ç†Ÿæ‚‰äº†ä¸€éç¥ç»ç½‘ç»œçš„åŸºæœ¬çš„ä¸€äº›ç›¸å…³æ¦‚å¿µå’Œå¤§è‡´ç»“æ„ï¼Œç„¶åè¿˜æ–°äº†è§£äº† ***å·ç§¯ç¥ç»ç½‘ç»œCNN*** ***é€’å½’ç¥ç»ç½‘ç»œRNN*** (ä¹‹å‰åªæ˜¯çŸ¥é“è¿™ä¸ªåå­—ï¼Œä½†ä¸çŸ¥é“å…·ä½“çš„ä¸ä¸€èˆ¬ç¥ç»ç½‘ç»œçš„åŒºåˆ«ä»¥åŠå„è‡ªçš„é€‚ç”¨ç”¨é€”)ã€‚

æ ¹æ®æˆ‘çš„çœ‹å®Œä¹‹åçš„ç†è§£ï¼Œå¤§è‡´æ˜¯è¿™æ ·ï¼ŒCNNå’ŒRNNéƒ½æ˜¯å¯¹äºæ™®é€šç¥ç»ç½‘ç»œåœ¨ç‰¹å®šç”¨é€”ä¸‹è¿›è¡Œçš„æ”¹è¿›çš„äº§ç‰© æˆ–è€…è¯´åˆ†æ”¯ï¼Œ

å…¶ä¸­CNNæ˜¯é€‚ç”¨äºå¤„ç†å›¾åƒçš„ç¥ç»ç½‘ç»œã€‚ç”±äºä¸€ä¸ªå›¾åƒåŒ…å«çš„æ•°æ®å¾ˆå¤§(300x300çš„åƒç´ å°±æœ‰9ä¸‡ä¸ªæ•°æ®)ï¼Œå› æ­¤å¦‚æœä½¿ç”¨ä¸€èˆ¬çš„ç¥ç»ç½‘ç»œï¼Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿå¾ˆå¤šçš„å‚æ•°ã€‚è€Œå›¾åƒåˆæœ‰ä¸€å®šçš„è¿ç»­æ€§(æˆ‘çš„ç†è§£å¥½åƒæ˜¯è¿™æ ·çš„ï¼Œä½†ä¸çŸ¥é“æ˜¯å¦å‡†ç¡®)ï¼Œå› æ­¤ä½¿ç”¨***å·ç§¯***å¯ä»¥å‡å°æ•°æ®çš„é‡ã€‚

![cnn_features](../images/cnn_features.png){ loading=lazy }

æ‰€ä»¥CNNå¯¹äºè¿™é¢˜ä¸­çš„æ•°æ®å°±ç”¨ä¸ä¸Šã€‚

è€ŒRNNæ˜¯é€‚ç”¨äºå¤„ç†**æœ‰æ—¶é—´åºåˆ—ä¿¡æ¯çš„æ•°æ®**(æˆ‘è§‰å¾—å¯ä»¥ç›´æ¥ç†è§£æˆï¼Œæ•°æ®æ˜¯ä¸€ä¸²æˆ–å¤šä¸²è¿ç»­çš„åºåˆ—çš„æƒ…å†µ)ï¼Œå¦‚ æ­£å¸¸è¯­å¥çš„å­—ç¬¦ä¸² æˆ–è€… ç”¨äºä»·æ ¼é¢„æµ‹çš„å†å²æ•°æ® ä¹‹ç±»çš„ï¼Œå°±æ˜¯è¯´ **==é€‚ç”¨äº ä¹‹å‰çš„æ•°æ®æœ‰å¯èƒ½ä¼šå½±å“åˆ°ä¹‹åçš„æ•°æ® çš„æƒ…å†µ==**

![rnn_features](../images/rnn_features.png){ loading=lazy }

æ‰€ä»¥è§‰å¾—RNNé€‚åˆç”¨äºè¿™é¢˜çš„æ•°æ®å¤„ç†ã€‚

è¿˜äº†è§£åˆ°**RNNæœ‰ä¸¤ä¸ªä¼˜åŒ–çš„äº§ç‰© GRU å’Œ LSTM** ã€‚

ä½†è¿™ä¸ªè¯¾ç¨‹ä¸­çš„è¿™å‡ èŠ‚è¯¾æ„Ÿè§‰è®²çš„æ¯”è¾ƒå¤šçš„æ˜¯ä¸€äº›æ¦‚å¿µæ€§çš„ä¸œè¥¿ï¼Œæ²¡æœ‰å¤ªå¤šçš„å®ç°ï¼Œç„¶åæˆ‘å°±æ³¨æ„åˆ°è¿™ä¸ªå¤§ä½¬upä¸»è¿˜æœ‰ä¸€é—¨ä¸»è¦æ•™æˆå„ç§ç¥ç»ç½‘ç»œçš„å®ç°(ä¸»è¦ä½¿ç”¨pytorchæ¥å®ç°)çš„è¯¾ [åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2](https://b23.tv/yec4GOm)ï¼Œç„¶åé€šè¿‡[åœ¨ä»–ä¸»é¡µä¸Šæœç´¢ `rnn` ](https://space.bilibili.com/1567748478/search/video?keyword=rnn)ï¼Œå‘ç°äº†ä¸¤èŠ‚è¯¾æ˜¯è®²rnnçš„ï¼Œæˆ‘è§‰å¾—å…¶ä¸­åº”è¯¥ä¼šæœ‰å®ç°çš„ä»£ç ï¼Œ

-   [54 å¾ªç¯ç¥ç»ç½‘ç»œ RNNã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘\_å“”å“©å“”å“©\_bilibili](https://b23.tv/Fr7Ws3f)
-   [55 å¾ªç¯ç¥ç»ç½‘ç»œ RNN çš„å®ç°ã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘\_å“”å“©å“”å“©\_bilibili](https://b23.tv/w6dXNqj)

ä½†ç”±äºè¿™å¤©å·²ç»æ¯”è¾ƒç´¯äº†ï¼Œæ‰€ä»¥å°±æ‰“ç®—æ˜å¤©çœ‹è¿™ä¸¤èŠ‚è¯¾æ¥å­¦ä¹ æ€ä¹ˆå®ç°RNN

## Day 2

### äº†è§£é€’å½’ç¥ç»ç½‘ç»œRNNçš„pythonå®ç° (æ—©)

æ—©ä¸Šèµ·æ¥ï¼Œæ¥ç€æ˜¨å¤©çš„è¿›åº¦çœ‹ã€‚

åœ¨çœ‹ [54 å¾ªç¯ç¥ç»ç½‘ç»œ RNN](https://b23.tv/Fr7Ws3f) æ—¶ï¼Œçœ‹åˆ°ä»‹ç»RNNçš„åº”ç”¨çš„éƒ¨åˆ†ï¼Œå°±æ„Ÿè§‰Cé¢˜çš„æ•°æ®åº”è¯¥ç¡®å®æ˜¯å¯ä»¥ç”¨RNNæ¥å¤„ç†çš„ï¼Œæ‰€ä»¥å°±å·®ä¸å¤šå†³å®šè¦ä½¿ç”¨RNNäº†

![rnn_application](../images/rnn_application.png){ loadind=lazy }

è€Œåœ¨çœ‹ [55 å¾ªç¯ç¥ç»ç½‘ç»œ RNN çš„å®ç°](https://b23.tv/w6dXNqj) æ—¶ï¼Œæœ¬æ¥æ˜¯æƒ³è¾¹çœ‹è¾¹æŠŠè§†é¢‘ä¸­çš„ä»£ç æ•²å‡ºæ¥ï¼Œç„¶åç†è§£ä»£ç çš„æ¯ä¸ªéƒ¨åˆ†çš„ä½œç”¨ï¼Œä½†æ˜¯çœ‹åˆ°[***è®­ç»ƒ***](https://www.bilibili.com/video/BV1kq4y1H7sw/?t=2038)éƒ¨åˆ†çš„æ—¶å€™ï¼Œå‘ç°æœ‰ç‚¹å¤æ‚ï¼Œå¼€å§‹ä¸æ˜¯é‚£ä¹ˆå¥½ç†è§£äº† æœ‰ç‚¹éš¾æ‡‚ğŸ˜…ï¼Œäºæ˜¯å°±å»çœ‹[p2çš„ç®€æ´å®ç°](https://www.bilibili.com/video/BV1kq4y1H7sw?p=2)äº†ã€‚

ç®€æ´å®ç° ä¸­çš„ä»£ç å°±æ²¡æœ‰é‚£ä¹ˆå¤šé‚£ä¹ˆé•¿äº†ï¼Œäºæ˜¯æˆ‘å°è¯•è¿è¡Œï¼Œä½†å‘ç”Ÿäº†æŠ¥é”™

```python
AttributeError: module 'd2l.torch' has no attribute 'load_data_time_machine'
```

æ˜¾ç¤º `d2l` åŒ…ä¸­æ²¡æœ‰ `load_data_time_machine` è¿™ä¸ªå‡½æ•°æˆ–ç±»

>   `d2l` åŒ…åº”è¯¥æ˜¯è¿™ä¸ªè¯¾ç¨‹å¯¹åº”çš„ä¸“é—¨ç”¨äºæ•™å­¦çš„åŒ…ï¼Œå› ä¸ºè¯¾ç¨‹çš„æ•™æç½‘ç«™åå°±å« d2l
>
>   >   -   ä¸­æ–‡ç‰ˆç½‘ç«™ [ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ â€” åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹  2.0.0 documentation (d2l.ai)](https://zh-v2.d2l.ai/)
>   >   -   è‹±æ–‡ç‰ˆç½‘ç«™ [Dive into Deep Learning â€” Dive into Deep Learning 1.0.3 documentation (d2l.ai)](https://d2l.ai/)

æœ€åçŒœæµ‹å¯èƒ½æ˜¯ä¸­æ–‡ç‰ˆæ•™æä¸­ `d2l` åŒ…å¯¹åº”çš„ç‰ˆæœ¬æ¯”è¾ƒè€äº†ï¼Œæ–°ç‰ˆçš„åŒ…å¯èƒ½å·²ç»åšäº†ä¿®æ”¹ï¼Œäºæ˜¯å»æŸ¥çœ‹äº†[è‹±æ–‡ç‰ˆæ•™æä¸­çš„ä»£ç ](https://d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html)ï¼Œå¤åˆ¶å‡ºæ¥ç„¶åå°è¯•è¿è¡Œ(æˆ‘åœ¨ä¸¤å¤„ `predict` çš„åœ°æ–¹åŠ äº† `print` ï¼Œå¦åˆ™ä¸ä¼šæ˜¾ç¤ºå‡ºç»“æœ)

```python linenums="1"
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

class RNN(d2l.Module):
    """The RNN model implemented with high-level APIs."""
    def __init__(self, num_inputs, num_hiddens):
        super().__init__()
        self.save_hyperparameters()
        self.rnn = nn.RNN(num_inputs, num_hiddens)

    def forward(self, inputs, H=None):
        return self.rnn(inputs, H)
    
class RNNLM(d2l.RNNLMScratch):
    """The RNN-based language model implemented with high-level APIs."""
    def init_params(self):
        self.linear = nn.LazyLinear(self.vocab_size)

    def output_layer(self, hiddens):
        return self.linear(hiddens).swapaxes(0, 1)
    
data = d2l.TimeMachine(batch_size=1024, num_steps=32)
rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)
model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)
# model.predict('it has', 20, data.vocab)
print(model.predict('it has', 20, data.vocab))

trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
trainer.fit(model, data)

# model.predict('it has', 20, data.vocab, d2l.try_gpu()
print(model.predict('it has', 20, data.vocab, d2l.try_gpu()))
```

è¿è¡Œçš„ç»“æœ

```bash
it hasiiidddyiididyiididyi
<Figure size 350x250 with 1 Axes>
<Figure size 350x250 with 1 Axes>
...
it has and the proment the
```

ä½†è¯¾ç¨‹ä¸­ä¹Ÿæœ‰æåˆ°GRUå’ŒLSTMæ˜¯ä¸€èˆ¬RNNçš„æ”¹è¿›ï¼Œä¸¤è€…æ•ˆæœå·®ä¸å¤šï¼Œä½†GRUä¼¼ä¹æ¯”LSTMç®€å•ä¸€äº›(LSTMå¥½åƒè¦æ›´å¤æ‚ä¸€äº›)ï¼Œæ‰€ä»¥å†³å®šæœ€åè¦ä½¿ç”¨GRUæ¥å¤„ç†é¢˜ç›®æ‰€ç»™çš„æ•°æ®

### å°è¯•å®ç°GRU (åˆ)

æˆ‘ç›´æ¥æ‰“å¼€äº†è‹±æ–‡æ•™æä¸­çš„GRUéƒ¨åˆ† [10.2. Gated Recurrent Units (GRU)](https://d2l.ai/chapter_recurrent-modern/gru.html) ï¼Œå°†ä»£ç å¤åˆ¶å‡ºæ¥

=== "ä»é›¶å®ç°"

    ```python linenums="1"
    import torch
    from torch import nn
    from d2l import torch as d2l
    
    class GRUScratch(d2l.Module):
        def __init__(self, num_inputs, num_hiddens, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
    
            init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              nn.Parameter(torch.zeros(num_hiddens)))
            self.W_xz, self.W_hz, self.b_z = triple()  # Update gate
            self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate
            self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state
    
    @d2l.add_to_class(GRUScratch)
    def forward(self, inputs, H=None):
        if H is None:
            # Initial state with shape: (batch_size, num_hiddens)
            H = torch.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
        outputs = []
        for X in inputs:
            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +
                            torch.matmul(H, self.W_hz) + self.b_z)
            R = torch.sigmoid(torch.matmul(X, self.W_xr) +
                            torch.matmul(H, self.W_hr) + self.b_r)
            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +
                               torch.matmul(R * H, self.W_hh) + self.b_h)
            H = Z * H + (1 - Z) * H_tilde
            outputs.append(H)
        return outputs, H
    
    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)
    
    print(model.predict('it has', 20, data.vocab, d2l.try_gpu()))
    ```

=== "ç®€æ´å®ç°"

    ```python linenums="1"
    import torch
    from torch import nn
    from d2l import torch as d2l
    
    class GRU(d2l.RNN):
        def __init__(self, num_inputs, num_hiddens):
            d2l.Module.__init__(self)
            self.save_hyperparameters()
            self.rnn = nn.GRU(num_inputs, num_hiddens)
    
    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)
    
    print(model.predict('it has', 20, data.vocab, d2l.try_gpu()))
    ```

å°è¯•è¿è¡Œå¹¶è¿è¡ŒæˆåŠŸã€‚

ç”±äºè¯¾ç¨‹æ˜¯ä½¿ç”¨çš„GRUæ¥è¿›è¡Œæ–‡æœ¬çš„é¢„æµ‹ï¼Œæ‰€ä»¥æ•°æ®çš„æ ¼å¼è‚¯å®šå’Œé¢˜ç›®æ‰€ç»™çš„è¡¨æ ¼ç±»å‹ä¸å¤ªä¸€æ ·ï¼Œäºæ˜¯æˆ‘æƒ³çœ‹çœ‹èƒ½ä¸èƒ½ä¿®æ”¹æ•°æ®çš„æ ¼å¼å¹¶ä¸”è¿˜ç”¨æ•™æä¸Šçš„ä»£ç æ¥è¿›è¡Œé¢„æµ‹ï¼Œ

äºæ˜¯å°è¯•è¯¢é—®copilot

???+ example "chatlog"

    **RonaldLN**: 
    
    å¦‚æœæˆ‘ä¸æƒ³è®­ç»ƒæ–‡æœ¬ä»¥åŠé¢„æµ‹æ–‡æœ¬ï¼Œæˆ‘æƒ³é€šè¿‡RNNè®­ç»ƒç±»ä¼¼äºçŸ©é˜µæˆ–è€…å‘é‡çš„æ•°æ®ï¼Œæ¥è·å¾—æ•°å­¦çš„æ¨¡å‹ï¼Œåº”è¯¥å¦‚ä½•ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶ä¸­çš„ä»£ç ï¼Ÿ
    
    &nbsp;
    
    **GitHub Copilot**: 
    
    å¦‚æœä½ æƒ³ä½¿ç”¨ RNN æ¥è®­ç»ƒçŸ©é˜µæˆ–å‘é‡æ•°æ®ï¼Œä½ éœ€è¦åšçš„ä¸»è¦æ˜¯ä¿®æ”¹æ•°æ®åŠ è½½å’Œé¢„å¤„ç†çš„éƒ¨åˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œä½ éœ€è¦å°† `d2l.TimeMachine` æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°éœ€è¦è¿”å›ä¸€ä¸ªå¯ä»¥è¿­ä»£çš„æ•°æ®é›†ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º `(batch_size, sequence_length, num_features)` çš„å¼ é‡ï¼Œå…¶ä¸­ `num_features` æ˜¯ä½ çš„ç‰¹å¾æ•°é‡ã€‚
    
    æ­¤å¤–ï¼Œä½ å¯èƒ½è¿˜éœ€è¦ä¿®æ”¹æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºå¤§å°ä»¥åŒ¹é…ä½ çš„æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œ`num_inputs` åº”è¯¥ç­‰äºä½ çš„ç‰¹å¾æ•°é‡ï¼Œè€Œ `vocab_size` åº”è¯¥ç­‰äºä½ çš„ç›®æ ‡å€¼çš„æ•°é‡ã€‚
    
    ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¿®æ”¹åçš„ä»£ç ç¤ºä¾‹ï¼š
    
    ```python
    import torch
    from torch import nn
    from d2l import torch as d2l
    
    class GRU(d2l.RNN):
        def __init__(self, num_inputs, num_hiddens):
            d2l.Module.__init__(self)
            self.save_hyperparameters()
            self.rnn = nn.GRU(num_inputs, num_hiddens)
    
    # åŠ è½½ä½ çš„æ•°æ®
    # data åº”è¯¥æ˜¯ä¸€ä¸ªå¯ä»¥è¿­ä»£çš„æ•°æ®é›†ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (batch_size, sequence_length, num_features) çš„å¼ é‡
    data = load_your_data()
    
    num_features = data[0].shape[-1]  # ç‰¹å¾æ•°é‡
    num_targets = 1  # ç›®æ ‡å€¼çš„æ•°é‡ï¼Œè¿™é‡Œå‡è®¾æˆ‘ä»¬åªæœ‰ä¸€ä¸ªç›®æ ‡å€¼
    
    gru = GRU(num_inputs=num_features, num_hiddens=32)
    model = d2l.RNNLM(gru, vocab_size=num_targets, lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)
    ```
    
    è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªåŸºæœ¬çš„ç¤ºä¾‹ï¼Œä½ å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚æ¥è¿›ä¸€æ­¥ä¿®æ”¹è¿™ä¸ªä»£ç ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½éœ€è¦ä¿®æ”¹æ¨¡å‹çš„æ¶æ„ï¼Œæˆ–è€…ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ã€‚

ç„¶åæˆ‘å°±è§‰å¾—ï¼Œå¦‚æœè¦å¯¹æ•™æä¸­çš„ä»£ç è¿›è¡Œä¿®æ”¹æ¥é€‚é…è¿™é¢˜çš„æ•°æ®çš„ä½¿ç”¨çš„è¯ï¼Œæ„Ÿè§‰ä¸æ˜¯é‚£ä¹ˆæ–¹ä¾¿ï¼Œæ‰€ä»¥å…ˆæ”¾å¼ƒäº†è¿™ä¸ªæƒ³æ³•

### ç†è§£22å¹´Cé¢˜Få¥–GRUç›¸å…³ä»£ç 

ç”±äºåœ¨ç¾èµ›å¼€å§‹çš„ä¸€å‘¨å‰ï¼Œé˜Ÿå‹ç»™æˆ‘åˆ†äº«äº†ä¸€ä¸ªgithubä»“åº“

[jishengpeng/2022MCM-C-problem: 2022ç¾èµ›Cé¢˜ï¼ˆMCM/ICMï¼‰Få¥–æºç æ•°æ®å…¬å¼€ (github.com)](https://github.com/jishengpeng/2022MCM-C-problem)

æ˜¯ä¸€æ”¯åœ¨22å¹´ç¾èµ›é€‰æ‹©Cé¢˜å¹¶æœ€ç»ˆè·å¾—Få¥–çš„é˜Ÿä¼ï¼Œä»“åº“å†…æœ‰ä»–ä»¬çš„è®ºæ–‡ä»¥åŠå¤„ç†æ•°æ®çš„ä»£ç 

!!! quote

    <big>**å…³äºç¾èµ›ç”¨åˆ°çš„æ¨¡å‹**</big>
    
    ä¸»è¦ç”¨åˆ°çš„æ¨¡å‹æœ‰å°æ³¢é™å™ªæ¨¡å‹ï¼ŒGRUç¥ç»ç½‘ç»œä¸ºåŸºç¡€çš„é¢„æµ‹æ¨¡å‹ã€‚ç„¶ååˆ©ç”¨è´ªå¿ƒ+è§„åˆ’çš„æ€æƒ³å†™äº†ä¸€ä¸ªä¹°å–æ¯”ç‰¹å¸çš„ç­–ç•¥ã€‚

**æ‰€ä»¥å¦ä¸€ä¸ªæ€è·¯å°±æ˜¯ä¿®æ”¹ä»–ä»¬çš„ä»£ç ï¼Œä½¿å¾—èƒ½é€‚ç”¨äºæˆ‘ä»¬çš„é¢˜ç›®æ‰€ç»™çš„æ•°æ®**

### ç¡®å®šè¡¡é‡*åŠ¿å¤´*çš„æ–¹æ³•

ä¸‹åˆ4ç‚¹å·¦å³ï¼Œå› ä¸ºåœ¨çœ‹ç€22å¹´få¥–çš„ä»£ç æ—¶ï¼Œæƒ³åˆ°å¦‚æœè¦å°†é¢˜ç›®ç»™çš„æ¯”èµ›æ—¶çš„å„ç§çŠ¶æ€çš„ä¿¡æ¯ä»¥åŠæ•°æ®è¾“å…¥è¿›è¡Œè®­ç»ƒ(æˆ–è€…è¯´*ç›‘ç£å­¦ä¹  supervised learning*)ï¼Œé‚£ä¹ˆå°±è¿˜**éœ€è¦æœ‰ç¡®å®šçš„å®é™…çš„*åŠ¿å¤´*ç»“æœ**ç”¨äºè®­ç»ƒæ—¶çš„æ£€éªŒï¼Œæ‰€ä»¥æˆ‘çªç„¶æœ‰äº†ä¸€ä¸ªæƒ³æ³•ï¼Œäºæ˜¯å¼€å§‹å’Œé˜Ÿå‹è®¨è®ºã€‚

ä¹‹å‰é˜Ÿå‹æƒ³çš„å°±æ˜¯ç”¨ä¸‹ä¸€åˆ†çš„è¾“èµ¢ä½œä¸º*åŠ¿å¤´*è¯„åˆ¤çš„æ ‡å‡†(å³æŠŠä¸‹ä¸€åˆ†èµ¢çš„æ¦‚ç‡ä½œä¸º*åŠ¿å¤´*)ï¼Œä½†æˆ‘æƒ³åˆ°***==åŠ¿å¤´==*==åº”è¯¥ä¸æ­¢æ˜¯å¯¹ä¸‹ä¸€åˆ†æœ‰å½±å“ï¼Œåº”è¯¥è¿˜æœ‰å¯èƒ½å½±å“åˆ°ä¹‹åçš„å‡ åˆ†==**ï¼Œäºæ˜¯å°±å’Œé˜Ÿå‹è¿›è¡Œäº†è®¨è®ºã€‚

æˆ‘ä»¬å¤§è‡´å‡è®¾åŠ¿å¤´åªä½“ç°åœ¨ä¹‹å3åˆ†ä¸Š(æˆ–è€…è¯´å†å¾€åçš„åˆ†ç›¸å…³æ€§ä¸æ˜¯é‚£ä¹ˆå¤§äº†ï¼Œæ‰€ä»¥å°±å¿½ç•¥æ‰)ï¼Œè®¾ $n_1$ $n_2$ $n_3$ åˆ†åˆ«ä¸ºä¹‹åç¬¬1 2 3åˆ†çš„è¾“èµ¢ï¼Œèµ¢ä¸º1 è¾“ä¸º0ï¼Œå†å‡è®¾åŠ¿å¤´ä¸è¿™3åˆ†ç»“æœçš„å…³ç³»æ˜¯è¿™æ ·çš„(æœ€ç®€å•çš„çº¿æ€§å…³ç³»ï¼Œéçº¿æ€§æ„Ÿè§‰ä¸å¥½è€ƒè™‘)
$$
åŠ¿å¤´ = a \times n_1 + b \times n_2 + c \times n_3
$$
ç„¶åæˆ‘ä»¬æ‰“ç®—é€šè¿‡**åˆ†æä¸€äº›æ¯”è¾ƒæ˜¾ç„¶çš„æƒ…å†µ**æ¥ç¡®å®šå‚æ•° $a$ $b$ $c$ å…·ä½“çš„å¤§å°(æˆ‘è®¤ä¸ºç›´æ¥å‡­æ„Ÿè§‰ç¡®å®šå¯èƒ½ä¸ä¸€å®šæ˜¯é‚£ä¹ˆå‡†ç¡®)ï¼Œ

-   é¦–å…ˆå¯ä»¥ç¡®å®šçš„æ˜¯åä¸‰åˆ†éƒ½èµ¢æˆ–è€…åä¸‰åˆ†éƒ½è¾“çš„æƒ…å†µï¼Œå³ $n_1=n_2=n_3=1$ å’Œ $n_1=n_2=n_3=0$ 

    1.   ä¸‰åˆ†éƒ½èµ¢æ—¶ï¼Œæˆ‘ä»¬æƒ³å®šåŠ¿å¤´ä¸º1ï¼Œå³ $n_1=n_2=n_3=1$ æ—¶ $åŠ¿å¤´=1$ ï¼Œé‚£ä¹ˆå°±èƒ½å¾—åˆ°
         $$
         a+b+c=1
         $$

    2.   ä¸‰åˆ†éƒ½è¾“æ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºåŠ¿å¤´åº”è¯¥ä¸º0ï¼Œè¿™ä¸ªå…¶å®ä¸Šåªèƒ½å¾—åˆ°*åŠ¿å¤´*çš„å¸¸æ•°é¡¹ä¸º0ï¼Œè€Œæˆ‘ä»¬å‡è®¾çš„å…³ç³»å¼ä¸­å…¶å®å·²ç»åŒ…å«äº†è¿™ä¸ªæ¡ä»¶

-   å…¶æ¬¡è¿˜å¯ä»¥ç¡®å®šçš„æ¡ä»¶æ˜¯ï¼Œåœ¨åä¸‰åˆ†ä¸­åªèµ¢äº†1åˆ†çš„æƒ…å†µä¸‹ï¼Œé‚£ä¹ˆåº”è¯¥æœ‰ **èµ¢ç¬¬1åˆ†çš„åŠ¿å¤´ > èµ¢ç¬¬2åˆ†çš„åŠ¿å¤´ > èµ¢ç¬¬3åˆ†çš„åŠ¿å¤´** ï¼Œé‚£ä¹ˆå°±å¯ä»¥å¾—åˆ°
    $$
    a > b > c > 0
    $$

-   ç„¶åæˆ‘ä»¬å†å–äº†ä¸€äº›ç‰¹å®šçš„æƒ…å†µæ¥è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ­¤æ¥è¿›ä¸€æ­¥ç¡®å®š3ä¸ªå‚æ•°çš„å¤§å°ã€‚

    æˆ‘ä»¬å¯¹**åªèµ¢ç¬¬1åˆ†**å’Œ**åªè¾“ç¬¬1åˆ†(å³èµ¢äº†ç¬¬2 3åˆ†)**çš„æƒ…å†µè¿›è¡Œåˆ†ææ¯”è¾ƒï¼Œæœ€åè®¤ä¸º åªèµ¢ç¬¬1åˆ†çš„åŠ¿å¤´ åº”è¯¥**ç•¥å¤§äº** åªè¾“ç¬¬1åˆ†çš„åŠ¿å¤´ï¼Œäºæ˜¯å¾—åˆ°
    $$
    a > b + c
    $$

äºæ˜¯æœ€åæˆ‘ä»¬å°†ç³»æ•°ç¡®å®šæˆ $a=0.55, b=0.25, c=0.2$ ï¼Œå³
$$
åŠ¿å¤´ = 0.55 n_1 + 0.25 n_2 + 0.2 n_3
$$
ç„¶åæˆ‘è®©é˜Ÿå‹éšä¾¿æŒ‘ä¸€åœºæ¯”èµ›çš„æ•°æ®ï¼Œå¤„ç†åæŒ‰ç…§å…¬å¼ç®—ä¸€ç®—ï¼Œç„¶åé˜Ÿå‹é€‰äº†å†³èµ›ï¼Œä¸‹é¢æ˜¯å‰100åˆ†å† å†›çš„åŠ¿å¤´

![momentum_on_final](../images/momentum_on_final.svg){ loading=lazy }

æˆ‘æ„Ÿè§‰çœ‹èµ·æ¥è¿˜è›®okçš„ï¼Œè¿™ä¸ªæ³¢åŠ¨æ„Ÿè§‰è¿˜ç®—æ¯”è¾ƒçœŸå®ã€‚

è¿™æ—¶å·²ç»æ˜¯æ™šä¸Šï¼Œç´¯äº†ä¸€å¤©å°±æ²¡æœ‰å†åšä»€ä¹ˆäº†

## Day 3

### ç»§ç»­ç†è§£22å¹´Cé¢˜Få¥–GRUç›¸å…³ä»£ç  (æ—©)

ç»§ç»­å‰ä¸€å¤©çš„è¿›åº¦ï¼Œ

??? info "`GRUæ¨¡å‹.py` å®Œæ•´ä»£ç "

    ```python title="GRUæ¨¡å‹.py" linenums="1"
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import time
    from sklearn import linear_model
    from d2l import torch as d2l
    import torch
    import torch.nn as nn
    import csv
    path="/home/jishengpeng/ç¾èµ›çš„æ¨¡æ‹Ÿç»ƒä¹ "
    BCHAIN_MKPRU=pd.read_csv(path+"/BCHAIN-MKPRU.csv",dtype={"Date":np.str,"Value":np.float64})
    LBMA_GOLD=pd.read_csv(path+"/LBMA-GOLD.csv",dtype={"Date":np.str,"Value":np.float64})
    Data=pd.read_csv(path+"/Cé¢˜å¤„ç†åçš„ä¸­é—´æ–‡ä»¶2.csv")
    
    def to_timestamp(date):
        return int(time.mktime(time.strptime(date,"%m/%d/%y")))
    
    #å°†æ—¥æœŸå˜ä¸ºè‡ªç„¶æ•°
    start_timestamp=to_timestamp(Data.iloc[0,0])
    for i in range(Data.shape[0]):
        Data.iloc[i,0]=(to_timestamp(Data.iloc[i,0])-start_timestamp)/86400
    print(Data)
    
    batch_size=1 # åº”è¯¥åªèƒ½ä¸º1
    start_input=30
    input_size=Data.shape[0]#è®­ç»ƒï¼šé€šè¿‡å‰input_sizeå¤©é¢„æµ‹input_size+1å¤©ï¼Œé¢„æµ‹ï¼šé€šè¿‡2åˆ°input_size+1å¤©é¢„æµ‹ç¬¬input_size+2å¤©
    hidden_size=20
    # input_size=200
    output_size=1
    layers_size=3
    lr=10
    num_epochs=1000
    
    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, output_size, layers_size):
            super().__init__()
            self.GRU_layer = nn.GRU(input_size, hidden_size, layers_size)
            self.linear = nn.Linear(hidden_size, output_size)
    
        def forward(self, x):
            x, _ = self.GRU_layer(x)
            x = self.linear(x)
            return x
    
    device=torch.device("cuda")
    
    gru=GRUModel(30, hidden_size, output_size, layers_size).to(device)
    
    criterion = nn.L1Loss()
    optimizer = torch.optim.Adam(gru.parameters(), lr)
    
    ji=np.array(Data.iloc[0:input_size,3].dropna())
    input_size=ji.shape[0]-2
    
    trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30)).to(torch.float32).to(device)
    trainB_y=torch.from_numpy(ji[input_size].reshape(-1,batch_size,output_size)).to(torch.float32).to(device)
    
    losses = []
    
    for epoch in range(num_epochs):
        output = gru(trainB_x).to(device)
        loss = criterion(output, trainB_y)
        losses.append(loss)
    
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print("loss" + str(epoch) + ":", loss.item())
    
    # é¢„æµ‹ï¼Œä»¥æ¯”ç‰¹å¸ä¸ºä¾‹
    # pred_x_train=torch.from_numpy(np.array(Data.iloc[1:input_size+1,1]).reshape(-1,1,input_size)).to(torch.float32).to(device)
    pred_x_train=torch.from_numpy(ji[input_size-29:input_size+1]).reshape(-1,1,30).to(torch.float32).to(device)
    pred_y_train=gru(pred_x_train).to(device)
    print("prediction:",pred_y_train.item())
    print("actual:",ji[input_size+1])
    
    # é¢„æµ‹ä»£ç 
    losses = []
    predictions = []
    actuals = []
    for i in range(start_input, input_size + 1):
        print("è¿›è¡Œåˆ°input_size=", i)
        # gru=GRUModel(i, hidden_size, output_size, layers_size).to(device)
        gru = GRUModel(30, hidden_size, output_size, layers_size).to(device)
        criterion = nn.L1Loss()
        optimizer = torch.optim.Adam(gru.parameters(), lr)
    
        # æ•°æ®ï¼Œä»¥æ¯”ç‰¹å¸ä¸ºä¾‹
        trainB_x = torch.from_numpy(ji[i - 30:i].reshape(-1, batch_size, 30)).to(torch.float32).to(device)
        trainB_y = torch.from_numpy(ji[i].reshape(-1, batch_size, output_size)).to(torch.float32).to(device)
    
        loss = None
    
        for epoch in range(num_epochs):
            output = gru(trainB_x).to(device)
            loss = criterion(output, trainB_y)
    
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # print("loss"+str(epoch)+":", loss.item())
        losses.append(loss.item())
    
        # é¢„æµ‹ï¼Œä»¥æ¯”ç‰¹å¸ä¸ºä¾‹
        pred_x_train = torch.from_numpy(ji[i - 29:i + 1].reshape(-1, 1, 30)).to(torch.float32).to(device)
        pred_y_train = gru(pred_x_train).to(device)
        # print("prediction:",pred_y_train.item())
        # print("actual:",Data.iloc[i+1,1])
        predictions.append(pred_y_train.item())
        actuals.append(ji[i + 1])
    plt.plot(losses)
    
    plt.plot(predictions)
    plt.plot(actuals)
    
    print(np.array(predictions).shape[0])
    print(np.array(actuals).shape[0])
    print(input_size-29)
    
    f=open(path+'/å‘¨æœŸlstmé»„é‡‘é¢„æµ‹1000ç‰ˆæœ¬.csv','w',encoding='utf-8',newline="")
    csv_writer=csv.writer(f)
    csv_writer.writerow(["å®é™…ä»·æ ¼","é¢„æµ‹ä»·æ ¼"])
    for i in range(0,input_size-29):
        tmp=[]
        tmp.append(actuals[i])
        tmp.append(round(predictions[i],2))
        csv_writer.writerow(tmp)
    f.close()
    ```

ç»è¿‡ä¸€æ®µæ—¶é—´çš„æŸ¥é˜…åŠåˆ†æï¼Œå¤§è‡´ç†è§£äº†ä»£ç å„ä¸ªéƒ¨åˆ†çš„ä½œç”¨ï¼Œ

```python linenums="10"
path="/home/jishengpeng/ç¾èµ›çš„æ¨¡æ‹Ÿç»ƒä¹ "
BCHAIN_MKPRU=pd.read_csv(path+"/BCHAIN-MKPRU.csv",dtype={"Date":np.str,"Value":np.float64})
LBMA_GOLD=pd.read_csv(path+"/LBMA-GOLD.csv",dtype={"Date":np.str,"Value":np.float64})
Data=pd.read_csv(path+"/Cé¢˜å¤„ç†åçš„ä¸­é—´æ–‡ä»¶2.csv")

def to_timestamp(date):
    return int(time.mktime(time.strptime(date,"%m/%d/%y")))

#å°†æ—¥æœŸå˜ä¸ºè‡ªç„¶æ•°
start_timestamp=to_timestamp(Data.iloc[0,0])
for i in range(Data.shape[0]):
    Data.iloc[i,0]=(to_timestamp(Data.iloc[i,0])-start_timestamp)/86400
print(Data)
```

è¿™éƒ¨åˆ†ä»£ç æ˜¯åœ¨å¯¹æ•°æ®è¿›è¡ŒåŠ è½½å’Œé¢„å¤„ç†ï¼Œ

```python linenums="24"
batch_size=1 # åº”è¯¥åªèƒ½ä¸º1
start_input=30
input_size=Data.shape[0]#è®­ç»ƒï¼šé€šè¿‡å‰input_sizeå¤©é¢„æµ‹input_size+1å¤©ï¼Œé¢„æµ‹ï¼šé€šè¿‡2åˆ°input_size+1å¤©é¢„æµ‹ç¬¬input_size+2å¤©
hidden_size=20
# input_size=200
output_size=1
layers_size=3
lr=10
num_epochs=1000
```

è¿™é‡Œæ˜¯åœ¨è®¾ç½®æ¨¡å‹çš„ä¸€äº›å‚æ•°ï¼Œå…¶ä¸­æˆ‘è®¤ä¸ºæœ€åéœ€è¦æ”¹åŠ¨çš„åªæœ‰ `input_size` è¾“å…¥æ•°æ®çš„ç»´æ•° å’Œ `num_epochs` è®­ç»ƒæ‰¹æ¬¡ã€‚

ç”±äºä¹‹å‰åœ¨æµ‹è¯•è¿è¡ŒåŸæœ¬çš„ä»£ç æ—¶ï¼Œè®­ç»ƒæ—¶é—´æ¯”è¾ƒä¹…ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿æµ‹è¯•æˆ‘å°±å°† `num_epochs` æ”¹æˆäº† `1` ã€‚

```python linenums="34"
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, layers_size):
        super().__init__()
        self.GRU_layer = nn.GRU(input_size, hidden_size, layers_size)
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x, _ = self.GRU_layer(x)
        x = self.linear(x)
        return x

device=torch.device("cuda")

gru=GRUModel(30, hidden_size, output_size, layers_size).to(device)

criterion = nn.L1Loss()
optimizer = torch.optim.Adam(gru.parameters(), lr)
```

è¿™ä¸€å—ä»£ç æ˜¯åœ¨è®¾ç½®GRUæ¨¡å‹çš„å‡½æ•°ï¼Œä½†ä¼¼ä¹ç”¨çš„åªæ˜¯çº¿æ€§çš„å‡½æ•°( `nn.Linear` è¿™ä¸€è¡Œ)ï¼Œå¹¶æ²¡æœ‰æ·»åŠ éçº¿æ€§çš„æ¿€æ´»å‡½æ•°ã€‚

```python linenums="52"
ji=np.array(Data.iloc[0:input_size,3].dropna())
input_size=ji.shape[0]-2

trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30)).to(torch.float32).to(device)
trainB_y=torch.from_numpy(ji[input_size].reshape(-1,batch_size,output_size)).to(torch.float32).to(device)

losses = []

for epoch in range(num_epochs):
    output = gru(trainB_x).to(device)
    loss = criterion(output, trainB_y)
    losses.append(loss)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print("loss" + str(epoch) + ":", loss.item())

# é¢„æµ‹ï¼Œä»¥æ¯”ç‰¹å¸ä¸ºä¾‹
# pred_x_train=torch.from_numpy(np.array(Data.iloc[1:input_size+1,1]).reshape(-1,1,input_size)).to(torch.float32).to(device)
pred_x_train=torch.from_numpy(ji[input_size-29:input_size+1]).reshape(-1,1,30).to(torch.float32).to(device)
pred_y_train=gru(pred_x_train).to(device)
print("prediction:",pred_y_train.item())
print("actual:",ji[input_size+1])
```

è¿™ä¸€æ®µä»£ç ä¼¼ä¹æ˜¯è¿›è¡Œçš„ä¸€æ¬¡è®­ç»ƒå’Œé¢„æµ‹çš„æµ‹è¯•(å› ä¸ºåœ¨ä¹‹åçš„ä»£ç ä¸­ï¼Œä¸è¿™æ®µä»£ç ç»“æ„éå¸¸ç›¸ä¼¼çš„ä»£ç è¢«æ”¾åœ¨äº† `for` å¾ªç¯ä¸­)ã€‚

ç”±äºä¸€å¼€å§‹æ²¡æœ‰çœ‹æ‡‚ä»£ç ğŸ˜…ï¼Œæ‰€ä»¥æŠŠå¤´5è¡Œä»£ç æ‹¿æ¥è¯¢é—®copilot

???+ example "chatlog"

    **RonaldLN**:
    
    `@workspace` `/explain` Write an explanation for the active selection as paragraphs of text.
    
    ```python
    ji=np.array(Data.iloc[0:input_size,3].dropna())
    input_size=ji.shape[0]-2
    
    trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30)).to(torch.float32).to(device)
    trainB_y=torch.from_numpy(ji[input_size].reshape(-1,batch_size,output_size)).to(torch.float32).to(device)
    ```
    
    &nbsp;
    
    **GitHub Copilot**: 
    
    The selected code is part of a larger Python script that uses the PyTorch library to train a Gated Recurrent Unit (GRU) model for time series prediction. The GRU model is a type of recurrent neural network (RNN) that is often used for sequence prediction problems.
    
    The first line of the selected code creates a numpy array `ji` from the third column of the DataFrame `Data`, starting from the first row up to `input_size` rows. The `dropna()` function is used to remove any missing values from the data.
    
    ```python
    ji=np.array(Data.iloc[0:input_size,3].dropna())
    ```
    
    The `input_size` is then updated to be two less than the number of elements in `ji`. This is likely done to account for the fact that the GRU model will be trained using a sliding window of 30 days, and the last two days of data will be used for prediction rather than training.
    
    ```python
    input_size=ji.shape[0]-2
    ```
    
    The next two lines of code create PyTorch tensors `trainB_x` and `trainB_y` from the numpy array `ji`. `trainB_x` is a 3D tensor that contains 30-day windows of data from `ji`, and `trainB_y` is a 1D tensor that contains the corresponding target values for each 30-day window. The `-1` in the `reshape()` function allows numpy to automatically calculate the size of that dimension based on the size of the input data and the other dimensions.
    
    ```python
    trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30)).to(torch.float32).to(device)
    trainB_y=torch.from_numpy(ji[input_size].reshape(-1,batch_size,output_size)).to(torch.float32).to(device)
    ```
    
    The `to(torch.float32)` function is used to convert the data to 32-bit floating point numbers, which are the standard data type for neural network weights and biases in PyTorch. The `to(device)` function is used to move the tensors to the GPU if one is available. This allows the model to be trained much faster than if it were trained on the CPU.
    
    ??? quote "Using 1 Reference"
    
        -   GRUæ¨¡å‹.py
        
    ---
    
    ChatGPTçš„ç¿»è¯‘ï¼š
    
    æ‰€é€‰ä»£ç æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„Pythonè„šæœ¬çš„ä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨PyTorchåº“è®­ç»ƒé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰æ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚GRUæ¨¡å‹æ˜¯ä¸€ç§ç»å¸¸ç”¨äºåºåˆ—é¢„æµ‹é—®é¢˜çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç±»å‹ã€‚
    
    æ‰€é€‰ä»£ç çš„ç¬¬ä¸€è¡Œä»DataFrame `Data` çš„ç¬¬ä¸‰åˆ—åˆ›å»ºä¸€ä¸ªnumpyæ•°ç»„ `ji` ï¼Œä»ç¬¬ä¸€è¡Œå¼€å§‹åˆ° `input_size` è¡Œã€‚ `dropna()` å‡½æ•°ç”¨äºä»æ•°æ®ä¸­åˆ é™¤ä»»ä½•ç¼ºå¤±å€¼ã€‚
    
    ```python
    ji = np.array(Data.iloc[0:input_size, 3].dropna())
    ```
    
    ç„¶åï¼Œå°† `input_size` æ›´æ–°ä¸º `ji` å…ƒç´ æ•°é‡å‡2ã€‚è¿™å¯èƒ½æ˜¯ä¸ºäº†è€ƒè™‘åˆ°GRUæ¨¡å‹å°†ä½¿ç”¨ä¸€ä¸ª30å¤©çš„æ»‘åŠ¨çª—å£è¿›è¡Œè®­ç»ƒï¼Œè€Œæœ€åä¸¤å¤©çš„æ•°æ®å°†ç”¨äºé¢„æµ‹è€Œä¸æ˜¯è®­ç»ƒã€‚
    
    ```python
    input_size = ji.shape[0] - 2
    ```
    
    æ¥ä¸‹æ¥çš„ä¸¤è¡Œä»£ç ä»numpyæ•°ç»„ `ji` åˆ›å»ºPyTorchå¼ é‡ `trainB_x` å’Œ `trainB_y` ã€‚ `trainB_x` æ˜¯ä¸€ä¸ª3Då¼ é‡ï¼ŒåŒ…å«æ¥è‡ª `ji` çš„30å¤©çª—å£çš„æ•°æ®ï¼Œè€Œ `trainB_y` æ˜¯ä¸€ä¸ªåŒ…å«æ¯ä¸ª30å¤©çª—å£å¯¹åº”ç›®æ ‡å€¼çš„1Då¼ é‡ã€‚ `reshape()` å‡½æ•°ä¸­çš„ `-1` å…è®¸numpyæ ¹æ®è¾“å…¥æ•°æ®å’Œå…¶ä»–ç»´åº¦çš„å¤§å°è‡ªåŠ¨è®¡ç®—è¯¥ç»´åº¦çš„å¤§å°ã€‚
    
    ```python
    trainB_x = torch.from_numpy(ji[input_size-30:input_size].reshape(-1, batch_size, 30)).to(torch.float32).to(device)
    trainB_y = torch.from_numpy(ji[input_size].reshape(-1, batch_size, output_size)).to(torch.float32).to(device)
    ```
    
    `to(torch.float32)` å‡½æ•°ç”¨äºå°†æ•°æ®è½¬æ¢ä¸º32ä½æµ®ç‚¹æ•°ï¼Œè¿™æ˜¯PyTorchä¸­ç¥ç»ç½‘ç»œæƒé‡å’Œåå·®çš„æ ‡å‡†æ•°æ®ç±»å‹ã€‚ `to(device)` å‡½æ•°ç”¨äºå°†å¼ é‡ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚è¿™ä½¿å¾—æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦æ¯”åœ¨CPUä¸Šè®­ç»ƒè¦å¿«å¾—å¤šã€‚

äºæ˜¯å¤§æ¦‚ç†è§£äº†ä»£ç çš„å«ä¹‰ï¼Œå…ˆæ˜¯ä»csvæ–‡ä»¶ä¸­å–å‡º `é»„é‡‘ä»·å€¼` è¿™ä¸€åˆ—çš„æ•°æ®ï¼Œ

>   ```css title="Cé¢˜å¤„ç†åçš„ä¸­é—´æ–‡ä»¶2.csv" linenums="1"
>   æ—¥æœŸ(æœˆ/æ—¥/å¹´),æ¯”ç‰¹å¸ä»·å€¼,æ˜¯å¦å¯ä»¥ä¹°å–é»„é‡‘,é»„é‡‘ä»·å€¼
>   9/11/16,621.65,1
>   9/12/16,609.67,0,1324.6
>   9/13/16,610.92,0,1323.65
>   ...
>   ```

ç„¶åå–**å€’æ•°ç¬¬32åˆ°å€’æ•°ç¬¬3çš„30ä¸ªæ•°æ®**å’Œ**å€’æ•°ç¬¬2ä¸ªæ•°æ®**ä½œä¸ºè®­ç»ƒçš„è¾“å…¥å’Œè¾“å‡ºï¼Œ

>   ä»–ä»¬å¥½åƒæ˜¯éœ€è¦**==ä½¿ç”¨å‰30å¤©çš„ä»·æ ¼æ¥é¢„æµ‹ä¸‹ä¸€å¤©çš„ä»·æ ¼==**ï¼Œå› æ­¤å¦‚æœè¦é¢„æµ‹å“ªä¸€å¤©ï¼Œå°±éœ€è¦å–é‚£ä¸€å¤©å¾€å‰æ•°30å¤©çš„æ•°æ®ä½œä¸ºè¾“å…¥

æœ€åå†å–**å€’æ•°ç¬¬31åˆ°å€’æ•°ç¬¬2çš„30ä¸ªæ•°æ®**è¾“å…¥æ¨¡å‹æ¥å¯¹æœ€åä¸€å¤©çš„ä»·æ ¼è¿›è¡Œé¢„æµ‹ï¼Œæœ€åå†å’Œå®é™…çš„ä»·æ ¼ä¸€èµ·æ˜¾ç¤ºè¿›è¡Œæ¯”è¾ƒã€‚

è€Œä» `# é¢„æµ‹ä»£ç ` å¼€å§‹çš„å‰©ä¸‹çš„ä»£ç ï¼Œå°±æ˜¯ä¾æ¬¡ä»ç¬¬31å¤©åˆ°æœ€å1å¤©ï¼Œæ¯å¤©å–å…¶å‰30å¤©ä½œä¸ºè¾“å…¥åºåˆ—è¿›è¡Œé¢„æµ‹ã€‚

### å°è¯•å¯¹ `GRUæ¨¡å‹.py` çš„ä»£ç è¿›è¡Œä¿®æ”¹ä»¥è¾“å…¥å¤šç»´æ•°æ® (åˆ)

ç†è§£äº† `GRUæ¨¡å‹.py` ä¸­çš„ä»£ç ä¹‹åï¼Œæˆ‘å¼€å§‹å°è¯•åœ¨chatgptçš„å¸®åŠ©ä¸‹ï¼Œä¿®æ”¹ä»–ä»¬çš„ä»£ç æ¥å®ç°å¤šç»´æ•°æ®çš„è¾“å…¥ï¼Œ

ä½†å¤§æ¦‚å°è¯•äº†ä¸€ä¸‹æ„Ÿè§‰ä¸æ˜¯å¾ˆå¥½æ”¹ï¼Œäºæ˜¯æœ€åè¿˜æ˜¯æ”¾å¼ƒäº†ã€‚

??? example "chatlog"

    **User**:
    
    ??? note "`my_test.py`"
        
        ```python linenums="1"
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import time
        from sklearn import linear_model
        from d2l import torch as d2l
        import torch
        import torch.nn as nn
        import csv
        path="."
        BCHAIN_MKPRU=pd.read_csv(path+"/BCHAIN-MKPRU.csv",dtype={"Date":np.str,"Value":np.float64})
        LBMA_GOLD=pd.read_csv(path+"/LBMA-GOLD.csv",dtype={"Date":np.str,"Value":np.float64})
        Data=pd.read_csv(path+"/Cé¢˜å¤„ç†åçš„ä¸­é—´æ–‡ä»¶2.csv")
    
        def to_timestamp(date):
            return int(time.mktime(time.strptime(date,"%m/%d/%y")))
    
        #å°†æ—¥æœŸå˜ä¸ºè‡ªç„¶æ•°
        start_timestamp=to_timestamp(Data.iloc[0,0])
        for i in range(Data.shape[0]):
            Data.iloc[i,0]=(to_timestamp(Data.iloc[i,0])-start_timestamp)/86400
        print(Data)
    
        batch_size=1 # åº”è¯¥åªèƒ½ä¸º1
        start_input=30
        input_size=Data.shape[0]#è®­ç»ƒï¼šé€šè¿‡å‰input_sizeå¤©é¢„æµ‹input_size+1å¤©ï¼Œé¢„æµ‹ï¼šé€šè¿‡2åˆ°input_size+1å¤©é¢„æµ‹ç¬¬input_size+2å¤©
        hidden_size=20
        # input_size=200
        output_size=1
        layers_size=3
        lr=10
        num_epochs=1
    
        # exit(0)
    
        class GRUModel(nn.Module):
            def __init__(self, input_size, hidden_size, output_size, layers_size):
                """
                Initializes a GRUModel instance.
    
                Args:
                    input_size (int): The number of expected features in the input x
                    hidden_size (int): The number of features in the hidden state h
                    output_size (int): The number of output features
                    layers_size (int): Number of recurrent layers
    
                """
                super().__init__()
                self.GRU_layer = nn.GRU(input_size, hidden_size, layers_size)
                self.linear = nn.Linear(hidden_size, output_size)
    
            def forward(self, x):
                """
                Forward pass of the GRUModel.
    
                Args:
                    x (Tensor): Input tensor of shape (seq_len, batch, input_size)
    
                Returns:
                    Tensor: Output tensor of shape (seq_len, batch, output_size)
    
                """
                x, _ = self.GRU_layer(x)
                x = self.linear(x)
                return x
    
        device=torch.device("cuda")
    
        gru=GRUModel(30, hidden_size, output_size, layers_size).to(device)
    
        criterion = nn.L1Loss()
        optimizer = torch.optim.Adam(gru.parameters(), lr)
    
        ji=np.array(Data.iloc[0:input_size,3].dropna())
        input_size=ji.shape[0]-2
    
        trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30)).to(torch.float32).to(device)
        trainB_y=torch.from_numpy(ji[input_size].reshape(-1,batch_size,output_size)).to(torch.float32).to(device)
    
        losses = []
    
        for epoch in range(num_epochs):
            output = gru(trainB_x).to(device)
            loss = criterion(output, trainB_y)
            losses.append(loss)
    
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            print("loss" + str(epoch) + ":", loss.item())
    
        # é¢„æµ‹ï¼Œä»¥æ¯”ç‰¹å¸ä¸ºä¾‹
        # pred_x_train=torch.from_numpy(np.array(Data.iloc[1:input_size+1,1]).reshape(-1,1,input_size)).to(torch.float32).to(device)
        pred_x_train=torch.from_numpy(ji[input_size-29:input_size+1]).reshape(-1,1,30).to(torch.float32).to(device)
        pred_y_train=gru(pred_x_train).to(device)
        print("prediction:",pred_y_train.item())
        print("actual:",ji[input_size+1])
        ```
    
    è¿™ä¸ªpythonè„šæœ¬ä¸­ï¼Œè¯»å–çš„è¦è¾“å…¥çš„æ•°æ®ä¼¼ä¹éƒ½æ˜¯å•ç‹¬çš„æ•°å­—ï¼Œå¦‚æœæˆ‘æƒ³ç»™GRUè¾“å…¥çš„æ˜¯ä¸€ä¸ªnç»´çš„å‘é‡ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å»ä¿®æ”¹ä»£ç ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    è‹¥è¦ä¿®æ”¹ä»£ç ä»¥å¤„ç†nç»´å‘é‡è€Œä¸æ˜¯å•ä¸ªæ•°å­—ï¼Œéœ€è¦å¯¹è¾“å…¥æ•°æ®çš„å¤„ç†è¿›è¡Œç›¸åº”çš„è°ƒæ•´ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„ä¿®æ”¹ï¼š
    
    1.   **ä¿®æ”¹æ•°æ®é¢„å¤„ç†ï¼š**
         -   ç¡®ä¿`Data`ä¸­çš„æ¯ä¸ªè¾“å…¥æ ·æœ¬æ˜¯ä¸€ä¸ªnç»´å‘é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°å­—ã€‚ä½ å¯èƒ½éœ€è¦è°ƒæ•´`Data`çš„ç»“æ„ï¼Œä»¥åŒ…å«ä¸€ä¸ªnç»´å‘é‡çš„åˆ—ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„æ•°å­—ã€‚
    
    2.   **ä¿®æ”¹GRUæ¨¡å‹çš„è¾“å…¥å’Œåˆå§‹åŒ–ï¼š**
         -   ä¿®æ”¹`GRUModel`ç±»çš„`__init__`æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†nç»´å‘é‡è€Œä¸ä»…ä»…æ˜¯å•ä¸ªæ•°å­—ã€‚è¿™æ¶‰åŠåˆ°ä¿®æ”¹`input_size`å’Œ`nn.GRU`çš„åˆå§‹åŒ–å‚æ•°ã€‚
         -   å¦‚æœè¾“å…¥æ˜¯nç»´å‘é‡ï¼Œå°†`input_size`ä¿®æ”¹ä¸ºnã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‘é‡é•¿åº¦æ˜¯10ï¼Œé‚£ä¹ˆå°†`GRUModel`çš„åˆå§‹åŒ–ä¿®æ”¹ä¸ºï¼š`gru = GRUModel(10, hidden_size, output_size, layers_size).to(device)`
    
    3.   **ä¿®æ”¹æ•°æ®åŠ è½½å’Œå¤„ç†çš„éƒ¨åˆ†ï¼š**
         -   ç¡®ä¿`trainB_x`å’Œ`trainB_y`åŒ…å«nç»´å‘é‡çš„ä¿¡æ¯ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°å¯¹`ji`çš„å¤„ç†æ–¹å¼çš„ä¿®æ”¹ã€‚
    
    ä»¥ä¸‹æ˜¯å¯èƒ½çš„ä¿®æ”¹ä»£ç ç‰‡æ®µï¼š
    
    ```python
    # å‡è®¾æ¯ä¸ªè¾“å…¥æ ·æœ¬æ˜¯ä¸€ä¸ªé•¿åº¦ä¸ºnçš„å‘é‡
    n = 10  # è¯·æ ¹æ®ä½ çš„å®é™…éœ€æ±‚è®¾ç½®å‘é‡çš„ç»´åº¦
    
    # ...
    
    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, output_size, layers_size):
            super().__init__()
            self.GRU_layer = nn.GRU(input_size, hidden_size, layers_size)
            self.linear = nn.Linear(hidden_size, output_size)
    
        def forward(self, x):
            x, _ = self.GRU_layer(x)
            x = self.linear(x)
            return x
    
    # ...
    
    # æ•°æ®å¤„ç†éƒ¨åˆ†
    ji = np.array(Data.iloc[0:input_size, 3:n+3].dropna())  # ä¿®æ”¹ä¸ºåŒ…å«nç»´å‘é‡çš„å¤„ç†æ–¹å¼
    input_size = ji.shape[0] - 2
    
    trainB_x = torch.from_numpy(ji[input_size-30:input_size].reshape(-1, batch_size, 30, n)).to(torch.float32).to(device)
    trainB_y = torch.from_numpy(ji[input_size].reshape(-1, batch_size, output_size)).to(torch.float32).to(device)
    
    # ...
    
    # é¢„æµ‹éƒ¨åˆ†
    pred_x_train = torch.from_numpy(ji[input_size-29:input_size+1].reshape(-1, 1, 30, n)).to(torch.float32).to(device)
    ```
    
    è¿™é‡Œçš„å…³é”®ç‚¹æ˜¯ç¡®ä¿`ji`å’Œ`Data`ä¸­åŒ…å«nç»´å‘é‡çš„ä¿¡æ¯ï¼Œå¹¶ä¸”ç›¸åº”åœ°è°ƒæ•´æ¨¡å‹çš„è¾“å…¥å¤§å°å’Œåˆå§‹åŒ–ã€‚
    
    &nbsp;
    
    ......
    
    **User**:
    
    æˆ‘å‡è®¾äº†è¿™æ ·ä¸€ä¸ªå…¬å¼ï¼Œ
    
    $$
    M(x, y, z) = ...
    $$
    
    å¹¶ä¸”æœ‰ä¸€ä¸ªå…³äº x, y, z çš„ä¸‰ç»´å‘é‡çš„ä¸€ä¸ªåºåˆ—ï¼Œæˆ‘æƒ³é€šè¿‡ GRU æ¨¡å‹æ¥é¢„æµ‹ M çš„å€¼ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å†™ä»£ç ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    è‹¥æƒ³é€šè¿‡GRUæ¨¡å‹æ¥é¢„æµ‹$M$çš„å€¼ï¼Œä½ éœ€è¦è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š
    
    1.   **ä¿®æ”¹æ•°æ®é¢„å¤„ç†ï¼š**
         -   ç¡®ä¿`Data`ä¸­çš„æ¯ä¸ªè¾“å…¥æ ·æœ¬æ˜¯ä¸€ä¸ªå…³äº$x, y, z$çš„ä¸‰ç»´å‘é‡åºåˆ—ï¼Œè€Œä¸ä»…ä»…æ˜¯å•ä¸ªæ•°å­—ã€‚ä½ å¯èƒ½éœ€è¦è°ƒæ•´`Data`çš„ç»“æ„ï¼Œä»¥åŒ…å«ä¸€ä¸ªåºåˆ—çš„åˆ—ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªä¸‰ç»´å‘é‡ã€‚
    
    2.   **ä¿®æ”¹GRUæ¨¡å‹çš„è¾“å…¥å’Œåˆå§‹åŒ–ï¼š**
         -   ä¿®æ”¹`GRUModel`ç±»çš„`__init__`æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ•´ä¸ªåºåˆ—è€Œä¸ä»…ä»…æ˜¯å•ä¸ªæ•°å­—ã€‚è¿™æ¶‰åŠåˆ°ä¿®æ”¹`input_size`å’Œ`nn.GRU`çš„åˆå§‹åŒ–å‚æ•°ã€‚
         -   å¦‚æœæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªä¸‰ç»´å‘é‡ï¼Œå°†`input_size`ä¿®æ”¹ä¸ºå‘é‡çš„ç»´åº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå‘é‡ç»´åº¦æ˜¯3ï¼Œé‚£ä¹ˆå°†`GRUModel`çš„åˆå§‹åŒ–ä¿®æ”¹ä¸ºï¼š`gru = GRUModel(3, hidden_size, output_size, layers_size).to(device)`
    
    3.   **ä¿®æ”¹æ•°æ®åŠ è½½å’Œå¤„ç†çš„éƒ¨åˆ†ï¼š**
         -   ç¡®ä¿`trainB_x`å’Œ`trainB_y`åŒ…å«æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ã€‚è¿™å¯èƒ½æ¶‰åŠåˆ°å¯¹`ji`çš„å¤„ç†æ–¹å¼çš„ä¿®æ”¹ã€‚
    
    ä»¥ä¸‹æ˜¯å¯èƒ½çš„ä¿®æ”¹ä»£ç ç‰‡æ®µï¼š
    
    ```python
    # å‡è®¾æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º3çš„ä¸‰ç»´å‘é‡
    n = 3  # è¯·æ ¹æ®ä½ çš„å®é™…éœ€æ±‚è®¾ç½®å‘é‡çš„ç»´åº¦
    
    # ...
    
    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, output_size, layers_size):
            super().__init__()
            self.GRU_layer = nn.GRU(input_size, hidden_size, layers_size)
            self.linear = nn.Linear(hidden_size, output_size)
    
        def forward(self, x):
            x, _ = self.GRU_layer(x)
            x = self.linear(x)
            return x
    
    # ...
    
    # æ•°æ®å¤„ç†éƒ¨åˆ†
    ji = np.array(Data.iloc[0:input_size, 3:n+3].dropna())  # ä¿®æ”¹ä¸ºåŒ…å«nç»´å‘é‡çš„å¤„ç†æ–¹å¼
    input_size = ji.shape[0] - 2
    
    # æ„é€ è®­ç»ƒæ•°æ®
    trainB_x = torch.from_numpy(ji[input_size-30:input_size+1].reshape(-1, batch_size, 30, n)).to(torch.float32).to(device)
    trainB_y = torch.from_numpy(ji[input_size+1].reshape(-1, batch_size, output_size)).to(torch.float32).to(device)
    
    # ...
    
    # é¢„æµ‹éƒ¨åˆ†
    pred_x_train = torch.from_numpy(ji[input_size-29:input_size+1].reshape(-1, 1, 30, n)).to(torch.float32).to(device)
    ```
    
    åœ¨è¿™é‡Œï¼Œç¡®ä¿`ji`å’Œ`Data`ä¸­åŒ…å«æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ï¼Œå¹¶ä¸”ç›¸åº”åœ°è°ƒæ•´æ¨¡å‹çš„è¾“å…¥å¤§å°å’Œåˆå§‹åŒ–ã€‚ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨GRUæ¨¡å‹æ¥è®­ç»ƒå’Œé¢„æµ‹$M$çš„å€¼ã€‚
    
    &nbsp;
    
    **User**:
    
    ```python
        trainB_x=torch.from_numpy(ji[input_size-30:input_size].reshape(-1,batch_size,30, 4)).to(torch.float32).to(device)
    TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
    ```
    
    ---
    
    è¿™æ˜¯ä¸ºä»€ä¹ˆï¼Œæˆ‘åº”è¯¥æ€ä¹ˆä¿®æ”¹ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    è¯¥é”™è¯¯é€šå¸¸è¡¨ç¤ºåœ¨è½¬æ¢`np.ndarray`ä¸ºPyTorch tensoræ—¶ï¼Œ`np.ndarray`ä¸­åŒ…å«äº†ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯èƒ½æ˜¯ç”±äº`ji`æ•°ç»„ä¸­çš„æŸäº›å…ƒç´ å…·æœ‰ä¸åŒçš„æ•°æ®ç±»å‹ã€‚
    
    ç¡®ä¿`ji` ä¸­æ‰€æœ‰å…ƒç´ éƒ½å…·æœ‰ç›¸åŒçš„æ•°æ®ç±»å‹ï¼Œæœ€å¥½ä½¿ç”¨ `astype` å°†å…¶è½¬æ¢ä¸ºä¸€è‡´çš„ç±»å‹ã€‚åœ¨ä½ çš„æƒ…å†µä¸‹ï¼Œå°è¯•å°† `ji` è½¬æ¢ä¸º `float32` æ•°æ®ç±»å‹ï¼Œä»¥ç¡®ä¿å®ƒåŒ…å«æ”¯æŒçš„ç±»å‹ä¹‹ä¸€ã€‚
    
    ```python
    ji = np.array(Data.iloc[0:input_size, 3:n+3].dropna(), dtype=np.float32)
    ```
    
    ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ›´æ–°åçš„ `ji` æ„é€  `trainB_x`ï¼š
    
    ```python
    trainB_x = torch.from_numpy(ji[input_size-30:input_size].reshape(-1, batch_size, 30, n)).to(torch.float32).to(device)
    ```
    
    è¿™æ ·åº”è¯¥èƒ½å¤Ÿè§£å†³ä¸Šè¿°é”™è¯¯ã€‚ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§ï¼Œä»¥ä¾¿æˆåŠŸè½¬æ¢ä¸º PyTorch tensorã€‚
    
    &nbsp;
    
    **User**:
    
    ```python
    ji: [1324.6  1323.65 1321.75 ... 1786.   1788.25 1794.6 ]
    ji: [[1.000000e+00 6.096700e+02 0.000000e+00 1.324600e+03]
     [2.000000e+00 6.109200e+02 0.000000e+00 1.323650e+03]
     [3.000000e+00 6.088200e+02 0.000000e+00 1.321750e+03]
     ...
     [1.249000e+03 1.027538e+04 0.000000e+00 1.563700e+03]
     [1.250000e+03 1.035430e+04 0.000000e+00 1.575050e+03]
     [1.251000e+03 1.024243e+04 0.000000e+00 1.581400e+03]]
    Traceback (most recent call last):
      ..."E:\Programs\Anaconda3\envs\genrec\lib\site-packages\torch\nn\modules\rnn.py", line 206, in check_input
        raise RuntimeError(
    RuntimeError: input must have 3 dimensions, got 5
    ```
    
    ---
    
    ä¸ºä»€ä¹ˆä¼šå‡ºç°5ç»´ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    é—®é¢˜å‡ºåœ¨å¯¹ `GRU` æ¨¡å‹çš„è¾“å…¥æ•°æ®æ ¼å¼ä¸Šã€‚åœ¨è¿™é‡Œï¼Œ`trainB_x` çš„å½¢çŠ¶è¢«è®¾ç½®ä¸º `(batch_size, sequence_length, input_size) = (1, 30, 4)`ï¼Œç„¶åé€šè¿‡ `torch.from_numpy(ji[input_size-30:input_size].reshape(-1, batch_size, 30, n))` è¿™è¡Œä»£ç ï¼Œç»´åº¦è¢«è½¬æ¢ä¸º `(batch_size, 1, sequence_length, input_size, n) = (1, 1, 30, 4, 3)`ã€‚
    
    è¿™å¯èƒ½æ˜¯ç”±äºåœ¨æ•°æ®å¤„ç†ä¸­çš„æŸä¸€æ­¥é€ æˆçš„ï¼Œå› æ­¤éœ€è¦ä¿®æ”¹æ•°æ®å¤„ç†çš„æ–¹å¼ï¼Œç¡®ä¿æ•°æ®å½¢çŠ¶ä¼ é€’ç»™æ¨¡å‹æ˜¯æ­£ç¡®çš„ã€‚
    
    åœ¨å¤„ç†æ—¶ï¼Œä½ åªéœ€è¦ç¡®ä¿`trainB_x`çš„å½¢çŠ¶æ˜¯ `(sequence_length, batch_size, input_size)` å°±å¯ä»¥äº†ã€‚å¦‚æœ`ji` ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º `n` çš„å‘é‡ï¼Œè€Œä½ æƒ³è¦ä½¿ç”¨ `30` ä¸ªè¿™æ ·çš„å‘é‡ä½œä¸ºè¾“å…¥ï¼Œä½ å¯ä»¥è¿™æ ·å¤„ç†ï¼š
    
    ```python
    # æ„é€ è®­ç»ƒæ•°æ®
    trainB_x = torch.from_numpy(ji[input_size-30:input_size].reshape(30, batch_size, n)).to(torch.float32).to(device)
    trainB_y = torch.from_numpy(ji[input_size+1].reshape(-1, batch_size, output_size)).to(torch.float32).to(device)
    ```
    
    è¿™åº”è¯¥èƒ½å¤Ÿè§£å†³ç»´åº¦é—®é¢˜ã€‚è¯·ç¡®ä¿è¾“å…¥çš„å½¢çŠ¶æ˜¯ `(sequence_length, batch_size, input_size)`ã€‚
    
    ......

### å¼€å§‹å°è¯•ä½¿ç”¨TensorFlowæ¥å®ç°GRU

æ–°å»ºäº†ä¸€ä¸ªä¸chatgptçš„å¯¹è¯ï¼Œè¯¢é—®å¦‚æœè¾“å…¥æ˜¯ä¸ªå¤šç»´å‘é‡çš„åºåˆ—ï¼Œåº”è¯¥å¦‚ä½•ç¼–å†™ä»£ç ï¼Œè€Œè¿™æ—¶chatgptè®©æˆ‘ä½¿ç”¨TensorFlowæ¥å®ç°ï¼Œç„¶åçœ‹äº†ä¸€ä¸‹å®ƒç»™çš„ä»£ç ï¼Œæ„Ÿè§‰æ¯”ç”¨pytorchå®ç°GRUè¦ç®€å•è®¸å¤šï¼Œäºæ˜¯äº§ç”Ÿäº†å°è¯•ä½¿ç”¨tensorflowå®ç°çš„æƒ³æ³•ã€‚

???+ example "chatlog"

    **User**:
    
    æˆ‘å‡è®¾äº†è¿™æ ·ä¸€ä¸ªå…¬å¼ï¼Œ
    
    $$
    M(x, y, z) = ...
    $$
    
    å¹¶ä¸”æœ‰ä¸€ä¸ªå…³äº x, y, z çš„ä¸‰ç»´å‘é‡çš„ä¸€ä¸ªåºåˆ—ï¼Œæˆ‘æƒ³é€šè¿‡ GRU æ¥å¯¹è¿™ä¸ªåºåˆ—è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œæ¥è·å¾—ptæƒé‡æ–‡ä»¶ï¼Œæˆ‘åº”è¯¥å¦‚ä½•ç¼–å†™ä»£ç ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    ä½ çš„é—®é¢˜æ¶‰åŠåˆ°ä½¿ç”¨ GRUï¼ˆGated Recurrent Unitï¼‰è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œä»¥è·å¾—æƒé‡æ–‡ä»¶ï¼Œä½†åœ¨ä½ çš„é—®é¢˜ä¸­ï¼Œç¼ºå°‘ä¸€äº›ç»†èŠ‚ï¼Œæ¯”å¦‚ç›®æ ‡æ˜¯ä»€ä¹ˆï¼ŒæŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆç­‰ã€‚æˆ‘å°†æä¾›ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ä»£ç ï¼Œå‡è®¾ä½ è¦è§£å†³çš„æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œå³ç»™å®šè¾“å…¥åºåˆ—ï¼Œé¢„æµ‹ä¸€ä¸ªè¿ç»­å€¼ã€‚
    
    åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ Python å’Œæ·±åº¦å­¦ä¹ æ¡†æ¶ TensorFlow æ¥å®ç°ã€‚é¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº† TensorFlowï¼š
    
    ```bash
    pip install tensorflow
    ```
    
    ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ä½œä¸ºèµ·ç‚¹ï¼š
    
    ```python
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import GRU, Dense
    import numpy as np
    
    # ç”Ÿæˆè™šæ‹Ÿæ•°æ®
    def generate_data():
        # ç”Ÿæˆè¾“å…¥åºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼
        seq_length = 10
        num_samples = 1000
        x = np.random.rand(num_samples, seq_length, 3)  # 3ç»´å‘é‡çš„è¾“å…¥åºåˆ—
        y = np.sum(x, axis=2)  # ç›®æ ‡å€¼æ˜¯è¾“å…¥åºåˆ—å„å…ƒç´ ä¹‹å’Œ
    
        return x, y
    
    # æ„å»ºæ¨¡å‹
    def build_model():
        model = Sequential()
        model.add(GRU(50, input_shape=(None, 3)))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
        return model
    
    # ç¼–è¯‘æ¨¡å‹
    model = build_model()
    model.compile(optimizer='adam', loss='mean_squared_error')
    
    # ç”Ÿæˆæ•°æ®
    x_train, y_train = generate_data()
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(x_train, y_train, epochs=10, batch_size=32)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶
    model.save_weights('gru_weights.h5')
    ```
    
    åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`generate_data` å‡½æ•°ç”Ÿæˆäº†ä¸€ä¸ªåŒ…å« 1000 ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªé•¿åº¦ä¸º 10 çš„åºåˆ—ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª 3 ç»´å‘é‡ã€‚æ¨¡å‹ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å« 50 ä¸ªå•å…ƒçš„ GRU å±‚ï¼Œæœ€åè¾“å‡ºä¸€ä¸ªå•ä¸€å€¼ã€‚æˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå¹¶ç”¨ Adam ä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒã€‚
    
    æœ€åï¼Œé€šè¿‡ `model.save_weights` æ–¹æ³•ä¿å­˜æ¨¡å‹çš„æƒé‡åˆ°åä¸º `gru_weights.h5` çš„æ–‡ä»¶ä¸­ã€‚ä½ å¯ä»¥æ ¹æ®ä½ çš„é—®é¢˜å’Œæ•°æ®é›†æ¥ä¿®æ”¹æ¨¡å‹ç»“æ„ã€æŸå¤±å‡½æ•°ç­‰ã€‚

ç„¶åå°è¯•è¿è¡Œäº†ä¸€ä¸‹chatgptç»™æˆ‘çš„ä»£ç ï¼Œå‘ç°èƒ½è¿è¡Œï¼Œäºæ˜¯æ‰“ç®—åœ¨è¿™ä¸ªä»£ç çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹å°è¯•

### debugå¹¶æœ€ç»ˆæˆåŠŸå®ç°æ•°æ®çš„è®­ç»ƒå’Œé¢„æµ‹

>   æ­¤æ—¶å¤§çº¦ä¸‹åˆ4:30

ä¸€å¼€å§‹æ˜¯æƒ³çœ‹çœ‹èƒ½ä¸èƒ½ä½¿ç”¨gpuæ¥è¿›è¡Œè®­ç»ƒï¼Œäºæ˜¯è¯¢é—®ç»§ç»­chatgpt

???+ example "chatlog"

    (å¯¹è¯è¿ç€å‰é¢)
    
    &nbsp;
    
    **User**:
    
    æˆ‘æƒ³ä½¿ç”¨ cuda å’Œ cudnn åŠ é€Ÿï¼Œ æˆ‘åº”è¯¥å¦‚ä½•ä¿®æ”¹ä»£ç ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    å¦‚æœä½ æƒ³ä½¿ç”¨CUDAå’ŒcuDNNæ¥åŠ é€Ÿä½ çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½ éœ€è¦ç¡®ä¿ä½ çš„TensorFlowç‰ˆæœ¬å·²ç»é…ç½®å¥½æ”¯æŒGPUï¼Œå¹¶ä¸”ä½ çš„æœºå™¨ä¸Šå·²ç»å®‰è£…äº†NVIDIAçš„CUDAå’ŒcuDNNåº“ã€‚
    
    é¦–å…ˆï¼Œç¡®ä¿ä½ çš„TensorFlowç‰ˆæœ¬æ”¯æŒGPUã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…TensorFlow GPUç‰ˆæœ¬ï¼š
    
    ```bash
    pip install tensorflow-gpu
    ```
    
    ç„¶åï¼Œç¡®ä¿ä½ çš„æœºå™¨ä¸Šå·²ç»æ­£ç¡®å®‰è£…äº†CUDAå’ŒcuDNNã€‚å¯ä»¥å‚è€ƒNVIDIAçš„å®˜æ–¹æ–‡æ¡£è¿›è¡Œå®‰è£…é…ç½®ã€‚
    
    ä¿®æ”¹ä»£ç ä»¥ä½¿ç”¨GPUåŠ é€Ÿæ—¶ï¼Œåªéœ€ç¡®ä¿TensorFlowèƒ½å¤Ÿè‡ªåŠ¨é€‰æ‹©GPUè®¾å¤‡ã€‚è¿™é€šå¸¸æ˜¯é»˜è®¤è¡Œä¸ºï¼Œä½†ä½ å¯ä»¥æ·»åŠ ä»¥ä¸‹ä»£ç æ¥ç¡®ä¿ï¼š
    
    ```python
    # å¯¼å…¥ TensorFlow å¹¶æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
    import tensorflow as tf
    print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
    
    # å…¶ä½™çš„ä»£ç ä¿æŒä¸å˜
    # ...
    
    # ç¼–è¯‘æ¨¡å‹ï¼Œç¡®ä¿ Tensorflow ä½¿ç”¨ GPU åŠ é€Ÿ
    model.compile(optimizer='adam', loss='mean_squared_error')
    
    # è®­ç»ƒæ¨¡å‹ï¼ŒTensorFlow ä¼šè‡ªåŠ¨ä½¿ç”¨ GPU
    model.fit(x_train, y_train, epochs=10, batch_size=32)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶
    model.save_weights('gru_weights.h5')
    ```
    
    ä»¥ä¸Šä»£ç ä¼šè¾“å‡ºå¯ç”¨çš„GPUæ•°é‡ã€‚å¦‚æœTensorFlowèƒ½å¤Ÿæ­£ç¡®è¯†åˆ«GPUå¹¶ä½¿ç”¨å®ƒï¼Œä½ çš„æ¨¡å‹å°†åœ¨GPUä¸Šè¿è¡Œã€‚
    
    ç¡®ä¿ä½ çš„TensorFlowå’ŒCUDA/cuDNNç‰ˆæœ¬æ˜¯å…¼å®¹çš„ã€‚ä½ å¯ä»¥æŸ¥çœ‹TensorFlowçš„æ–‡æ¡£ä»¥è·å–å…³äºå…¼å®¹æ€§çš„è¯¦ç»†ä¿¡æ¯ã€‚
    
    æ³¨æ„ï¼šåœ¨ä½¿ç”¨GPUæ—¶ï¼Œç¡®ä¿ä½ çš„æ¨¡å‹å’Œæ•°æ®éƒ½é€‚åˆGPUçš„å†…å­˜ã€‚æœ‰æ—¶å€™éœ€è¦è°ƒæ•´æ‰¹å¤„ç†å¤§å°ç­‰è¶…å‚æ•°ï¼Œä»¥ä¾¿é€‚åº”GPUå†…å­˜ã€‚

äºæ˜¯æˆ‘å°è¯•è¿è¡Œå‘½ä»¤

```bash
pip install tensorflow-gpu
```

ä½†æ˜¯å‘ç”Ÿäº†æŠ¥é”™

```bash
...
        File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\setuptools\_vendor\packaging\requirements.py", line 37, in __init__
          raise InvalidRequirement(str(e)) from e
      setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)
          python_version>"3.7"
                        ^
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

Ã— Encountered error while generating package metadata.
â•°â”€> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

ç„¶åæœç´¢ç›¸å…³ä¿¡æ¯ï¼Œæ‰¾åˆ°è¿™ä¸ªissue

[`pip install tensorflow-gpu` gives `setuptools.extern.packaging.requirements.InvalidRequirement` Â· Issue #60074 Â· tensorflow/tensorflow (github.com)](https://github.com/tensorflow/tensorflow/issues/60074)

æ ¹æ® [tilakrayalçš„å›ç­”](https://github.com/tensorflow/tensorflow/issues/60074#issuecomment-1480717509)ï¼Œåœ¨æ–°çš„ç‰ˆæœ¬ä¸­ï¼Œtensorflowå·²ç»ä¸åŒºåˆ†gpuå’Œégpuç‰ˆæœ¬äº†ï¼Œåˆå¹¶æˆä¸€ä¸ªç‰ˆæœ¬

>   From tensorflow v2.12, removed redundant packages **tensorflow-gpu and tf-nightly-gpu**. These packages were removed and replaced with packages that direct users to switch to **tensorflow or tf-nightly respectively**.

äºæ˜¯æˆ‘å°±å°è¯•chatgptå›ç­”ä¸­çš„è¿™ä¸ªä»£ç 

```python
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
```

ä½†æ˜¯æ˜¾ç¤ºäº† 0ã€‚

æœ€åæˆ‘æ„Ÿè§‰è¦å¼„å¥½è¿™ä¸ªgpuå¯èƒ½æ¯”è¾ƒè´¹æ—¶é—´ï¼Œå¹¶ä¸”cpuä¹Ÿè¶³å¤Ÿç”¨ï¼Œäºæ˜¯å°±ä¸æ‰“ç®—å†è¿½ç©¶è¿™ä¸ªã€‚

---

å°†åŸæœ¬chatgptç»™æˆ‘çš„ä»£ç ç¨åŠ ä¿®æ”¹äº†ä¸€ä¸‹(ä¸ºäº†æŸ¥çœ‹è¾“å…¥æ•°æ®/çŸ©é˜µçš„å½¢çŠ¶ï¼Œä»¥åŠè¿›è¡Œé¢„æµ‹)(ç”±äºç”Ÿæˆçš„æ•°æ®å¤ªå¤šï¼Œä¸ºäº†æ–¹ä¾¿æŸ¥çœ‹æ‰€ä»¥å°† `num_samples` è®¾ç½®æˆäº† `10` )ï¼Œ

???+ note "`my_test2.py`"

    ```python linenums="1"
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import GRU, Dense
    import numpy as np
    
    # ç”Ÿæˆè™šæ‹Ÿæ•°æ®
    def generate_data():
        # ç”Ÿæˆè¾“å…¥åºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼
        seq_length = 10
        num_samples = 10
        x = np.random.rand(num_samples, seq_length, 3)  # 3ç»´å‘é‡çš„è¾“å…¥åºåˆ—
        y = np.sum(x, axis=2)  # ç›®æ ‡å€¼æ˜¯è¾“å…¥åºåˆ—å„å…ƒç´ ä¹‹å’Œ
    
        return x, y
    
    # æ„å»ºæ¨¡å‹
    def build_model():
        model = Sequential()
        model.add(GRU(50, input_shape=(None, 3)))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
        return model
    
    # ç¼–è¯‘æ¨¡å‹
    model = build_model()
    model.compile(optimizer='adam', loss='mean_squared_error')
    
    # ç”Ÿæˆæ•°æ®
    x_train, y_train = generate_data()
    
    print(x_train)
    print(y_train)
    
    for a, b in zip(x_train[0], y_train[0]):
        print(f"sum {a} = {b}")
    
    # exit(0)
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(x_train, y_train, epochs=10, batch_size=32)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶
    model.save_weights('gru_weights.h5')
    
    # é¢„æµ‹å¹¶è®¡ç®—æ¯ä¸ªåºåˆ—è¾“å‡ºçš„å’Œ
    predictions = model.predict(x_train)
    sum_predictions = np.sum(predictions, axis=1)
    
    # æ‰“å°ç»“æœ
    print("Predictions:")
    print(predictions)
    ```

ç»ˆç«¯è¾“å‡ºå‡ºæ¥çš„ `x_train` `y_train` (31å’Œ32è¡Œçš„ä¸¤ä¸ª `print` )æ˜¯è¿™æ ·çš„

=== "`x_train`"

    ```python
    [[[0.7944328  0.19991478 0.1717165 ]
      [0.52863161 0.18222096 0.70395594]
      [0.57382239 0.64215397 0.63834019]
      [0.1210313  0.22205257 0.51856381]
      [0.64116937 0.40346543 0.73267251]
      [0.68666603 0.96090184 0.69283727]
      [0.72992198 0.02034594 0.84201691]
      [0.81079883 0.20875773 0.03558987]
      [0.81432445 0.47643867 0.87499175]
      [0.28032208 0.5911067  0.99797703]]
    
     ...
    
     [[0.41748414 0.33052038 0.83109248]
      [0.70159718 0.8524119  0.02944102]
      [0.22653771 0.11587124 0.50194598]
      [0.75814563 0.03297447 0.14874201]
      [0.30183465 0.36516737 0.5084972 ]
      [0.46178636 0.34466552 0.93100872]
      [0.85690693 0.26805801 0.61805625]
      [0.08040773 0.75785972 0.98884817]
      [0.11996411 0.0269658  0.932087  ]
      [0.72871391 0.80410475 0.30108709]]]
    ```

=== "`y_train`"

    ```python
    [[1.16606407 1.4148085  1.85431655 0.86164768 1.77730731 2.34040514
      1.59228483 1.05514643 2.16575486 1.86940581]
     [1.579097   1.5834501  0.84435492 0.93986211 1.17549922 1.73746061
      1.74302119 1.82711561 1.0790169  1.83390576]
     [1.05626799 1.40353675 1.45449643 1.47791795 1.33798364 1.23835848
      1.22593666 0.61404874 1.56806409 2.00279187]
     [1.52692854 1.02003715 1.43574853 2.16162096 1.87126819 2.39616241
      1.58802291 0.59642459 1.03014043 2.33408625]
     [0.6888727  1.89631828 2.22542497 2.02600974 1.80885565 2.04338134
      1.39698629 1.74913134 2.41820951 2.1095142 ]
     [1.45975555 1.66387383 1.31121027 2.17661788 1.53556581 1.65127684
      1.4565594  0.7422163  1.27289512 1.5068791 ]
     [0.54572599 0.89128647 1.99626636 1.02370802 2.11791123 0.68699581
      1.49900562 1.77851229 2.19032219 0.97705314]
     [1.59726203 1.81477729 1.27713759 1.28049124 0.39606842 2.15493916
      1.24729366 0.96491842 1.44108162 1.70740263]
     [2.08867844 0.4113912  1.61522844 1.86740305 2.00394564 2.32879784
      1.17513393 1.68913914 2.71357174 0.92150995]
     [1.46399312 2.29460573 1.33109995 1.54187649 1.93523676 1.60621029
      2.06562629 0.79873924 1.14552017 1.76911693]]
    ```

ä½†æ˜¯å‘ç°è¾“å‡ºçš„ `predictions` ç»“æœæ ¼å¼å¥½åƒæœ‰äº›ä¸ä¸€æ ·ï¼Œæˆ–è€…è¯´ä¸æ­£ç¡®

```python
Predictions:
[[0.7516717 ]
 [0.6875819 ]
 [0.6742991 ]
 [0.68913144]
 [0.8163525 ]
 [0.6028297 ]
 [0.625753  ]
 [0.6262842 ]
 [0.70508885]
 [0.65028405]]
```

**æœ¬æ¥åº”è¯¥æ˜¯è¾“å…¥çš„æ¯ä¸ªæ•°æ®éƒ½æ˜¯ä¸€ä¸ª3ç»´çš„å‘é‡ï¼Œå¹¶å¯¹åº”ä¸€ä¸ªæ•°å€¼çš„è¾“å‡ºï¼Œå¦‚ `[0.7944328  0.19991478 0.1717165 ]` å¯¹åº” `1.16606407` ï¼Œæ‰€ä»¥æœ€åçš„é¢„æµ‹åº”è¯¥æ˜¯ä¸€ä¸ª 10\*10 çš„çŸ©é˜µï¼Œè€Œ `predictions` åªæ˜¯ä¸€ä¸ª 10\*1 çš„çŸ©é˜µ**ï¼Œ

äºæ˜¯ç»§ç»­è¯¢é—®chatgpt

???+ example "chatlog"

    (å¯¹è¯è¿ç€å‰é¢)
    
    &nbsp;
    
    **User**:
    
    ??? note "`my_test2.py`"
    
        ```python linenums="1"
        import tensorflow as tf
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import GRU, Dense
        import numpy as np
    
        # ç”Ÿæˆè™šæ‹Ÿæ•°æ®
        def generate_data():
            # ç”Ÿæˆè¾“å…¥åºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼
            seq_length = 10
            num_samples = 1000
            x = np.random.rand(num_samples, seq_length, 3)  # 3ç»´å‘é‡çš„è¾“å…¥åºåˆ—
            y = np.sum(x, axis=2)  # ç›®æ ‡å€¼æ˜¯è¾“å…¥åºåˆ—å„å…ƒç´ ä¹‹å’Œ
    
            return x, y
    
        # æ„å»ºæ¨¡å‹
        def build_model():
            model = Sequential()
            model.add(GRU(50, input_shape=(None, 3)))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
            model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
            return model
    
        # ç¼–è¯‘æ¨¡å‹
        model = build_model()
        model.compile(optimizer='adam', loss='mean_squared_error')
    
        # ç”Ÿæˆæ•°æ®
        x_train, y_train = generate_data()
    
        print(x_train)
        print(y_train)
    
        for a, b in zip(x_train[0], y_train[0]):
            print(f"sum {a} = {b}")
    
        # exit(0)
    
        # è®­ç»ƒæ¨¡å‹
        model.fit(x_train, y_train, epochs=10, batch_size=32)
    
        # ä¿å­˜æƒé‡æ–‡ä»¶
        model.save_weights('gru_weights.h5')
        ```
    
    ä¸ºä»€ä¹ˆ `model.predict(x_train)` çš„ç»“æœæ˜¯
    
    ```python
    [[1.644006 ]
    [1.5395831]
    [1.5784358]
    [1.3815761]
    [1.4156687]
    [1.3671852]
    [1.5763392]
    [1.4610821]
    [1.5581633]
    ...
    ]
    ```
    
    å’Œ `y_train` ä¸ä¸€æ ·ï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    åœ¨ä½ çš„ä»£ç ä¸­ï¼Œä½ ç”Ÿæˆäº†è¾“å…¥æ•°æ® `x_train` å’Œå¯¹åº”çš„ç›®æ ‡æ•°æ® `y_train`ã€‚åœ¨ `generate_data` å‡½æ•°ä¸­ï¼Œä½ è®¡ç®—äº†æ¯ä¸ªåºåˆ—çš„å…ƒç´ ä¹‹å’Œä½œä¸ºç›®æ ‡å€¼ã€‚ä½†åœ¨ä½ çš„ `for` å¾ªç¯ä¸­ï¼Œä½ ä½¿ç”¨äº† `zip(x_train[0], y_train[0])`ï¼Œè¿™ä¼šè¾“å‡ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å…¥åºåˆ—å’Œç›®æ ‡å€¼çš„å¯¹åº”å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªåˆé€‚çš„å¯¹åº”å…³ç³»ã€‚
    
    GRU æ¨¡å‹çš„è¾“å…¥å½¢çŠ¶æ˜¯ `(batch_size, time_steps, input_dim)`ï¼Œåœ¨ä½ çš„æƒ…å†µä¸‹æ˜¯ `(None, 10, 3)`ï¼Œè€Œè¾“å‡ºå½¢çŠ¶æ˜¯ `(batch_size, units)`ï¼Œåœ¨ä½ çš„æƒ…å†µä¸‹æ˜¯ `(None, 50)`ã€‚å› æ­¤ï¼Œä½¿ç”¨ `model.predict(x_train)` ä¼šè¿”å›æ¯ä¸ªè¾“å…¥åºåˆ—å¯¹åº”çš„è¾“å‡ºå€¼ï¼Œä½†è¿™å¹¶ä¸æ˜¯è¾“å…¥åºåˆ—çš„å…ƒç´ ä¹‹å’Œã€‚
    
    å¦‚æœä½ æƒ³è¦å¾—åˆ°æ¨¡å‹é¢„æµ‹çš„å’Œï¼Œä½ å¯ä»¥è®¡ç®—æ¯ä¸ªåºåˆ—è¾“å‡ºçš„å’Œã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¿®æ”¹åçš„ä»£ç ç¤ºä¾‹ï¼š
    
    ```python
    # é¢„æµ‹å¹¶è®¡ç®—æ¯ä¸ªåºåˆ—è¾“å‡ºçš„å’Œ
    predictions = model.predict(x_train)
    sum_predictions = np.sum(predictions, axis=1)
    
    # æ‰“å°ç»“æœ
    print("Predictions:")
    print(predictions)
    print("Sum of predictions:")
    print(sum_predictions)
    ```
    
    è¿™æ ·åšä¼šæ‰“å°æ¨¡å‹é¢„æµ‹çš„æ¯ä¸ªåºåˆ—çš„è¾“å‡ºå€¼ï¼Œä»¥åŠæ¯ä¸ªåºåˆ—è¾“å‡ºå€¼çš„å’Œã€‚è¿™ä¸ªå’Œåº”è¯¥æ¥è¿‘äºè¾“å…¥åºåˆ—å…ƒç´ ä¹‹å’Œçš„å€¼ã€‚

ä»chatgptçš„å›ç­”ä¸­å¾—çŸ¥äº†ä¸€ä¸ª ==é‡è¦ä¿¡æ¯==ï¼Œ **GRU æ¨¡å‹çš„è¾“å…¥å½¢çŠ¶æ˜¯ `(batch_size, time_steps, input_dim)` (å³è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶)**ï¼Œåœ¨ä¹‹å‰4å·å­¦ä¹ è¯¾ç¨‹æ—¶æœ‰äº†è§£åˆ° `batch_size` æ˜¯æ¯ä¸€æ¬¡è®­ç»ƒæ—¶ ä¸¢è¿›å»çš„æ•°æ®çš„ä¸ªæ•°ï¼Œå†ç»“åˆ `time_steps` (æ ¹æ®åå­—çš„æ„æ€çŒœæµ‹)åº”è¯¥æŒ‡çš„æ˜¯**(å•ä¸ª)åºåˆ—çš„é•¿åº¦**ï¼Œ(æœ€åçš„ `input_dim` æ˜¯ä»£è¡¨è¾“å…¥çš„å•ä¸ªæ•°æ®çš„ç»´åº¦)æ‰€ä»¥ç†è§£äº†è¿™ä¸ªä¹‹åï¼Œå†ç»“åˆé¢˜ç›®çš„æ•°æ®ï¼Œæˆ‘è®¤ä¸ºåº”è¯¥æ˜¯ ==ä¸€åœºæ¯”èµ›çš„æ•°æ®ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„åºåˆ—ï¼Œè€Œè¿™åœºæ¯”èµ›ä¸­æ€»å…±æ‰“çš„==*==åˆ†==*==å°±æ˜¯åºåˆ—çš„é•¿åº¦==ã€‚

ä½†æ˜¯chatgptçš„å›ç­”å¹¶æ²¡æœ‰ç»™æˆ‘è§£å†³è¾“å‡ºä¸æ­£ç¡®çš„æ–¹æ³•ï¼Œäºæ˜¯æˆ‘å»æŸ¥çœ‹ `.fit()` æ–¹æ³•çš„è¯´æ˜

```python
@keras_export("keras.Model", "keras.models.Model")
class Model(base_layer.Layer, version_utils.ModelVersionSelector):
    ...
    @traceback_utils.filter_traceback
    def fit(
        self,
        x=None,
        y=None,
        batch_size=None,
        epochs=1,
        ...
    ):
        """Trains the model for a fixed number of epochs (dataset iterations).

        Args:
            x: Input data. It could be:
              - A Numpy array (or array-like), or a list of arrays
                (in case the model has multiple inputs).
              - A TensorFlow tensor, or a list of tensors
                (in case the model has multiple inputs).
              - A dict mapping input names to the corresponding array/tensors,
                if the model has named inputs.
              - A `tf.data` dataset. Should return a tuple
                of either `(inputs, targets)` or
                `(inputs, targets, sample_weights)`.
              - A generator or `keras.utils.Sequence` returning `(inputs,
                targets)` or `(inputs, targets, sample_weights)`.
              - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a
                callable that takes a single argument of type
                `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.
                `DatasetCreator` should be used when users prefer to specify the
                per-replica batching and sharding logic for the `Dataset`.
                See `tf.keras.utils.experimental.DatasetCreator` doc for more
                information.
              A more detailed description of unpacking behavior for iterator
              types (Dataset, generator, Sequence) is given below. If these
              include `sample_weights` as a third component, note that sample
              weighting applies to the `weighted_metrics` argument but not the
              `metrics` argument in `compile()`. If using
              `tf.distribute.experimental.ParameterServerStrategy`, only
              `DatasetCreator` type is supported for `x`.
            y: Target data. Like the input data `x`,
              it could be either Numpy array(s) or TensorFlow tensor(s).
              It should be consistent with `x` (you cannot have Numpy inputs and
              tensor targets, or inversely). If `x` is a dataset, generator,
              or `keras.utils.Sequence` instance, `y` should
              not be specified (since targets will be obtained from `x`).
        ...
        """
```

ç”¨chatgptç¿»è¯‘äº†ä¸€äº›(æˆ‘è®¤ä¸º)å…³é”®/æœ‰ç”¨çš„éƒ¨åˆ†

!!! tip

    ```markdown
                x: è¾“å…¥æ•°æ®ã€‚å¯èƒ½æ˜¯ï¼š
                   - ä¸€ä¸ª Numpy æ•°ç»„ï¼ˆæˆ–ç±»ä¼¼æ•°ç»„ï¼‰ï¼Œæˆ–è€…æ•°ç»„åˆ—è¡¨
                     ï¼ˆå¦‚æœæ¨¡å‹æœ‰å¤šä¸ªè¾“å…¥ï¼‰ã€‚
                   - ä¸€ä¸ª TensorFlow å¼ é‡ï¼Œæˆ–è€…å¼ é‡åˆ—è¡¨
                     ï¼ˆå¦‚æœæ¨¡å‹æœ‰å¤šä¸ªè¾“å…¥ï¼‰ã€‚
                   - å¦‚æœæ¨¡å‹æœ‰å‘½åè¾“å…¥ï¼Œè¿˜å¯ä»¥æ˜¯å°†è¾“å…¥åç§°æ˜ å°„åˆ°ç›¸åº”æ•°ç»„/å¼ é‡çš„å­—å…¸ã€‚
                   ...
                y: ç›®æ ‡æ•°æ®ã€‚ä¸è¾“å…¥æ•°æ® `x` ç±»ä¼¼ï¼Œ
                   å®ƒå¯ä»¥æ˜¯ Numpy æ•°ç»„æˆ– TensorFlow å¼ é‡ã€‚
                   å®ƒåº”ä¸ `x` ä¿æŒä¸€è‡´ï¼ˆä¸èƒ½æœ‰ Numpy è¾“å…¥å’Œ
                   å¼ é‡ç›®æ ‡ï¼Œåä¹‹äº¦ç„¶ï¼‰ã€‚å¦‚æœ `x` æ˜¯æ•°æ®é›†ã€ç”Ÿæˆå™¨ï¼Œ
                   æˆ– `keras.utils.Sequence` å®ä¾‹ï¼Œåˆ™ä¸åº”æŒ‡å®š `y`
                   ï¼ˆå› ä¸ºç›®æ ‡å°†ä» `x` ä¸­è·å–ï¼‰ã€‚
    ```

ç”±äºåœ¨å‡½æ•°è¯´æ˜ä¸­æåˆ° ç›®æ ‡æ•°æ® `y` éœ€è¦å’Œè¾“å…¥æ•°æ® `x` çš„å½¢çŠ¶ä¿æŒä¸€è‡´ï¼Œè€Œåœ¨ä¹‹å‰çš„ä»£ç ä¸­æ²¡æœ‰ä¿æŒä¸€è‡´( `x_train` æ˜¯3ç»´çŸ©é˜µè€Œ `y_train` æ˜¯2ç»´çŸ©é˜µ)ï¼Œä¸çŸ¥é“æ˜¯ä¸æ˜¯è¿™ä¸ªåœ°æ–¹ä½¿å¾—é¢„æµ‹æ—¶è¾“å‡ºçš„æ•°æ®å½¢çŠ¶ä¸æ­£ç¡®ï¼Œ

äºæ˜¯æˆ‘å°†å‡½æ•° `generate_data` ç¨å¾®ä¿®æ”¹äº†ä¸€ä¸‹

```python
def generate_data():
    # ç”Ÿæˆè¾“å…¥åºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼
    seq_length = 10
    num_samples = 10
    x = np.random.rand(num_samples, seq_length, 3)  # 3ç»´å‘é‡çš„è¾“å…¥åºåˆ—
    y = np.sum(x, axis=2)  # ç›®æ ‡å€¼æ˜¯è¾“å…¥åºåˆ—å„å…ƒç´ ä¹‹å’Œ

    # å°†yçš„æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºè‡ªèº«çš„åˆ—è¡¨
    y_as_list = np.array([[[element] for element in row] for row in y])

    return x, y_as_list
```

è¿™æ · `y_train` å°±å˜æˆäº†

```python
[[[1.55256756]
  [1.54053964]
  [1.03681008]
  [1.85271063]
  [1.42733873]
  [1.31950369]
  [2.30268749]
  [1.30466184]
  [0.82556753]
  [0.85749012]]

 ...
]
```

è¿™å°±ä¸è¾“å…¥çš„ `x_train` å¯¹åº”ä¸Šäº†(å°±å˜æˆäº†ä¸€ä¸ª3ç»´å‘é‡å¯¹åº”ä¸€ä¸ª1ç»´å‘é‡)ã€‚

ä½†æ˜¯è¿è¡Œè¿›è¡Œæµ‹è¯•ï¼Œé¢„æµ‹è¾“å‡ºçš„çŸ©é˜µè¿˜æ˜¯å’ŒåŸæ¥ä¸€æ ·ğŸ˜«ğŸ˜µ(debugäº†å¼„äº†åŠå¤©ç»“æœè¿˜æ˜¯åŸæ ·ï¼Œè¿™æ—¶å€™æ„Ÿè§‰ç²¾ç¥éå¸¸èé¡ï¼Œæ„Ÿè§‰è„‘å­å¿«æˆä¸€å›¢æµ†ç³Šäº†)ï¼Œ

>   æ­¤æ—¶å¤§çº¦åˆšè¿‡æ™šä¸Š8ç‚¹

---

æ­¤æ—¶å·²ç»æœ‰äº›æ²¡æœ‰åŠ¨åŠ›å»debugï¼Œå› æ­¤å°±æƒ³ç€æœç´¢ä¸€äº›ç°æˆçš„ä½¿ç”¨tensorflowçš„ä»£ç ï¼Œ

![gru_browser_history](../images/gru_browser_history.png){ loading=lazy }

ä½†æœç´¢äº†ä¸€æ®µæ—¶é—´æ„Ÿè§‰ä¹Ÿä¸å¤ªèƒ½æ‰¾åˆ°æ¯”è¾ƒåˆé€‚çš„ã€‚

>   æœç´¢è¿‡ç¨‹ä¸­æ‰¾åˆ°äº†ä¸€ä¸ªtensorflowå®˜ç½‘ä¸Šçš„æŒ‡å¯¼
>
>   [`å†…å­˜æ•°æ®` = ç”¨ tf.data åŠ è½½ CSV æ•°æ®  | TensorFlow Core (google.cn)](https://tensorflow.google.cn/tutorials/load_data/csv?hl=zh-cn#å†…å­˜æ•°æ®)
>
>   è¿™é‡Œæœ‰ä»‹ç»ç”¨pandasæ¥è¯»å–csvæ–‡ä»¶ä¸­çš„æ•°æ®å¹¶è¾“å…¥æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒçš„ä¸€äº›ç¤ºä¾‹ä»£ç (æ„Ÿè§‰è¿˜ç®—æ¯”è¾ƒæœ‰ç”¨)

!!! info

    ä¹‹å‰è¿˜æœ‰å°è¯•è¿‡æŠŠè¾“å…¥æ•°æ®æ”¹æˆ2ç»´çš„çŸ©é˜µï¼Œå› ä¸º `.fit()` å‡½æ•°è¯´æ˜ä¸­æåˆ° `x` å¯ä»¥æ˜¯numpyæ•°ç»„çš„åˆ—è¡¨( `a list of arrays` )ï¼Œ
    
    ä½†æ˜¯è¿™æ ·æ”¹åŠ¨åå‘ç”Ÿäº†è¿™æ ·çš„æŠ¥é”™
    
    ```python
    ...
    Epoch 1/10
    Traceback (most recent call last):
      File "E:\Github\Repo-for-2024MCM\2022MCM-C-problem\ä»£ç å’Œä¸­é—´æ•°æ®ç»“æœ\my_test2.py", line 42, in <module>
        model.fit(x_train, y_train, epochs=10, batch_size=32)
      ...
        File "E:\Programs\Anaconda3\envs\genrec\lib\site-packages\keras\src\engine\input_spec.py", line 235, in assert_input_compatibility
            raise ValueError(
    
        ValueError: Exception encountered when calling layer 'sequential' (type Sequential).
    
        Input 0 of layer "gru" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3)
    
        Call arguments received by layer 'sequential' (type Sequential):
          â€¢ inputs=tf.Tensor(shape=(None, 3), dtype=float32)
          â€¢ training=True
          â€¢ mask=None
    
    è¿›ç¨‹å·²ç»“æŸï¼Œé€€å‡ºä»£ç ä¸º 1
    ```
    
    äºæ˜¯åœ¨googleä¸Šæœç´¢è¿™æ ·çš„æŠ¥é”™ï¼Œ
    
    [ValueError: Exception encountered when calling layer 'sequential' (type Sequential). Input 0 of layer "gru" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 3) Call arguments received by layer 'sequential' (type Sequential): â€¢ inputs=tf.Tensor(shape=(None, 3), dtype=int32) â€¢ training=False â€¢ mask=None - Google æœç´¢](https://www.google.com/search?q=ValueError%3A+Exception+encountered+when+calling+layer+%27sequential%27+%28type+Sequential%29.++%0D%0AInput+0+of+layer+%22gru%22+is+incompatible+with+the+layer%3A+expected+ndim%3D3%2C+found+ndim%3D2.+Full+shape+received%3A+%28None%2C+3%29%0D%0A%0D%0ACall+arguments+received+by+layer+%27sequential%27+%28type+Sequential%29%3A%0D%0A++%E2%80%A2+inputs%3Dtf.Tensor%28shape%3D%28None%2C+3%29%2C+dtype%3Dint32%29%0D%0A++%E2%80%A2+training%3DFalse%0D%0A++%E2%80%A2+mask%3DNone&newwindow=1&sca_esv=37f724872de16756&sxsrf=ACQVn0-LuFmna4ijpPFbjNeaYA25l8Od3g%3A1707050979991&source=hp&ei=44e_Zeq8OuihkPIP3tawgA8&iflsig=ANes7DEAAAAAZb-V8wWi2TcRtFaesvqSe8m0GdSVVhzX&ved=0ahUKEwiqupi63JGEAxXoEEQIHV4rDPAQ4dUDCA0&uact=5&oq=ValueError%3A+Exception+encountered+when+calling+layer+%27sequential%27+%28type+Sequential%29.++%0D%0AInput+0+of+layer+%22gru%22+is+incompatible+with+the+layer%3A+expected+ndim%3D3%2C+found+ndim%3D2.+Full+shape+received%3A+%28None%2C+3%29%0D%0A%0D%0ACall+arguments+received+by+layer+%27sequential%27+%28type+Sequential%29%3A%0D%0A++%E2%80%A2+inputs%3Dtf.Tensor%28shape%3D%28None%2C+3%29%2C+dtype%3Dint32%29%0D%0A++%E2%80%A2+training%3DFalse%0D%0A++%E2%80%A2+mask%3DNone&gs_lp=Egdnd3Mtd2l6IucCVmFsdWVFcnJvcjogRXhjZXB0aW9uIGVuY291bnRlcmVkIHdoZW4gY2FsbGluZyBsYXllciAnc2VxdWVudGlhbCcgKHR5cGUgU2VxdWVudGlhbCkuICAKSW5wdXQgMCBvZiBsYXllciAiZ3J1IiBpcyBpbmNvbXBhdGlibGUgd2l0aCB0aGUgbGF5ZXI6IGV4cGVjdGVkIG5kaW09MywgZm91bmQgbmRpbT0yLiBGdWxsIHNoYXBlIHJlY2VpdmVkOiAoTm9uZSwgMykKCkNhbGwgYXJndW1lbnRzIHJlY2VpdmVkIGJ5IGxheWVyICdzZXF1ZW50aWFsJyAodHlwZSBTZXF1ZW50aWFsKToKICDigKIgaW5wdXRzPXRmLlRlbnNvcihzaGFwZT0oTm9uZSwgMyksIGR0eXBlPWludDMyKQogIOKAoiB0cmFpbmluZz1GYWxzZQogIOKAoiBtYXNrPU5vbmVIAFAAWABwAHgAkAEAmAEAoAEAqgEAuAEDyAEA-AEC-AEB&sclient=gws-wiz)
    
    ![google_search_dimensions_error](../images/google_search_dimensions_error.png){ loading=lazy }

åœ¨googleä¸Šæœç´¢æ—¶ï¼Œå‘ç°äº†åœ¨è¿™ä¸¤ä¸ªé—®é¢˜çš„å›ç­”ä¸­

-   [python - ValueError: Input 0 is incompatible with layer lstm_13: expected ndim=3, found ndim=4 - Stack Overflow](https://stackoverflow.com/questions/44583254/valueerror-input-0-is-incompatible-with-layer-lstm-13-expected-ndim-3-found-n)
-   [Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=4 Â· Issue #7403 Â· keras-team/keras (github.com)](https://github.com/keras-team/keras/issues/7403)

**éƒ½æœ‰äººæåˆ°äº†å°† `.fit()` çš„ `return_sequences` è¿™ä¸ªå‚æ•°è®¾ç½®æˆ `True`**

>   -   [stackoverflowä¸Šçš„å›ç­”](https://stackoverflow.com/a/55086664)
>   -   [githubä¸Šçš„å›ç­”](https://github.com/keras-team/keras/issues/7403#issuecomment-317199855)

è™½ç„¶ä¸æ˜ç™½å«ä¹‰ï¼Œä½†æ˜¯ç”±äºè„‘å­æ¯”è¾ƒç´¯äº†ï¼Œäºæ˜¯å°±ä¸æŠ¥å¸Œæœ›åœ°å°è¯•äº†ä¸€ä¸‹ï¼Œ

==ç»“æœå‘ç°å±…ç„¶æˆåŠŸå®ç°äº†== ğŸ˜²ğŸ˜²ğŸ˜²

??? note "`my_test2.py`"

    ```python hl_lines="22" linenums="1"
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import GRU, Dense
    import numpy as np
    
    # ç”Ÿæˆè™šæ‹Ÿæ•°æ®
    def generate_data():
        # ç”Ÿæˆè¾“å…¥åºåˆ—å’Œå¯¹åº”çš„ç›®æ ‡å€¼
        seq_length = 10
        num_samples = 10
        x = np.random.rand(num_samples, seq_length, 3)  # 3ç»´å‘é‡çš„è¾“å…¥åºåˆ—
        y = np.sum(x, axis=2)  # ç›®æ ‡å€¼æ˜¯è¾“å…¥åºåˆ—å„å…ƒç´ ä¹‹å’Œ
    
        # å°†yçš„æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºè‡ªèº«çš„åˆ—è¡¨
        y_as_list = np.array([[[element] for element in row] for row in y])
    
        return x, y_as_list
    
    # æ„å»ºæ¨¡å‹
    def build_model():
        model = Sequential()
        model.add(GRU(50, input_shape=(None, 3), return_sequences=True))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
        return model
    
    # ç¼–è¯‘æ¨¡å‹
    model = build_model()
    model.compile(optimizer='adam', loss='mean_squared_error')
    
    # ç”Ÿæˆæ•°æ®
    x_train, y_train = generate_data()
    
    print(x_train)
    print(y_train)
    
    for a, b in zip(x_train[0], y_train[0]):
        print(f"sum {a} = {b}")
    
    # exit(0)
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(x_train, y_train, epochs=10, batch_size=32)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶
    model.save_weights('gru_weights.h5')
    
    # é¢„æµ‹å¹¶è®¡ç®—æ¯ä¸ªåºåˆ—è¾“å‡ºçš„å’Œ
    predictions = model.predict(x_train)
    sum_predictions = np.sum(predictions, axis=1)
    
    # æ‰“å°ç»“æœ
    print("Predictions:")
    print(predictions)
    ```

ç»ˆç«¯ä¸­æ˜¾ç¤ºçš„é¢„æµ‹çš„è¾“å‡ºå˜æˆäº†

```python
Predictions:
[[[0.3234646 ]
  [0.5083787 ]
  [0.5576476 ]
  [0.54944414]
  [0.47944394]
  [0.5049154 ]
  [0.50329256]
  [0.5258315 ]
  [0.69459194]
  [0.7504357 ]]
 ...
]
```

ç„¶åæˆ‘åœ¨æœ«å°¾æ·»åŠ äº†è¿™æ ·å‡ è¡Œä»£ç æ¥**æ¯”è¾ƒé¢„æµ‹ç»“æœå’Œç›®æ ‡ç»“æœ**ï¼Œå¹¶å°† `epochs` è®¾ç½®æˆäº† `1000`

```python linenums="56"
for i in range(10):
    print(i, " -------")
    for a, b, c in zip(x_train[i], y_train[i], predictions[i]):
        print(f"act_sum {a} = {b}, pre: {c}")
```

ä¸‹é¢æ˜¯è®­ç»ƒåè¿›è¡Œé¢„æµ‹çš„(éƒ¨åˆ†)ç»“æœ

```python
...
9  -------
act_sum [0.17794148 0.2804384  0.91154308] = [1.36992296], pre: [1.3727927]
act_sum [0.9505687  0.57981087 0.11528877] = [1.64566833], pre: [1.6547668]
act_sum [0.10600898 0.16341549 0.22128649] = [0.49071096], pre: [0.49099457]
act_sum [0.32428769 0.09856033 0.46073104] = [0.88357906], pre: [0.8894826]
...
```

>   ğŸ˜²æ²¡æƒ³åˆ°è¢«å¡äº†è¿™ä¹ˆä¹…çš„åœ°æ–¹å°±è¿™æ ·è¢«è§£å†³äº†ğŸ˜…ğŸ˜­

---

åœ¨ä¹‹åæ€è€ƒäº†ä¸€ä¸‹ `return_sequences` åˆ†åˆ«ä¸º `True` å’Œ `False` çš„åŒºåˆ«ï¼Œ

æˆ‘çš„ç†è§£ä¸­ï¼Œå¦‚æœä¸º `True` åº”è¯¥å°±æ˜¯åƒè¿™é¢˜é‡Œæˆ‘ä»¬æƒ³è¦å¤„ç†çš„æ•°æ®ä¸€æ ·ï¼Œ**==æ¯ä¸€ä¸ªè¾“å…¥çš„æ•°æ®éƒ½å¯¹åº”ä¸€ä¸ªè¾“å‡º==**ï¼Œ

è€Œå¦‚æœä¸º `False` åº”è¯¥å°±æ˜¯åƒ22å¹´Cé¢˜ä¸€æ · **æ¯”å¦‚éœ€è¦ç”¨ä¹‹å‰ä¸€æ®µåºåˆ—çš„æ•°æ®æ¥è·å¾—å•ä¸ªçš„æ•°æ®(ç”¨å‰30å¤©çš„ä»·æ ¼æ¥é¢„æµ‹ä¸‹ä¸€å¤©çš„ä»·æ ¼)**

### ç”¨å†³èµ›æ•°æ®è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹

é˜Ÿå‹ä¹‹å‰å·²ç»å°†å† å†›å†³èµ›çš„æ•°æ®å¤„ç†å¥½å‘ç»™æˆ‘äº†ï¼Œäºæ˜¯æˆ‘å°±é‡æ–°å†™äº†ä¸€ä¸ªpythonæ–‡ä»¶ï¼Œ

???+ note "`my_test5.py`"

    ```python linenums="1"
    import pandas as pd
    import numpy as np
    
    import tensorflow as tf
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import GRU, Dense
    
    data = pd.read_csv("CA(test)_2.csv")
    data = np.array([data])
    print(data)
    
    data = data[:, :, 2:].astype(np.float32)
    print(data)
    labels, data, val = data[:, :-15, :1], data[:, :-15, 1:], data[:, :, :]
    Shi, val = val[:, :, :1], val[:, :, 1:]
    print(data)
    print(labels)
    
    def build_model():
        model = Sequential()
        model.add(GRU(50, input_shape=(None, 14), return_sequences=True))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
        return model
    
    # ç¼–è¯‘æ¨¡å‹
    model = build_model()
    model.compile(optimizer='adam', loss="mean_absolute_error")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(data, labels, epochs=1000, batch_size=1)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶
    model.save_weights('Shi.h5')
    
    pre = model.predict(val)
    for _ in zip(pre[0], Shi[0]):
        print("pre: ", _[0], "act: ", _[1])
    ```

```python linenums="8"
data = pd.read_csv("CA(test)_2.csv")
data = np.array([data])
```

é¦–å…ˆï¼Œç”¨pandasè¯»å–csvä¸­çš„æ•°æ®ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯(å¦‚æœå‚æ•°éƒ½ä½¿ç”¨é»˜è®¤è®¾ç½®) `read_csv` å‡½æ•°è¯»å–æ•°æ®æ—¶ä¼šå°†ç¬¬ä¸€è¡Œè®¤ä¸ºæ˜¯æ ‡é¢˜è€Œä¸ä¸å…¶ä»–æ•°æ®ä¸€èµ·å­˜å‚¨ã€‚

ç„¶åï¼Œå°†è¯»å–å‡ºæ¥çš„æ•°æ® `data` è½¬æ¢æˆnumpyæ•°ç»„ï¼Œè¿™é‡Œç”±äºcsvè¡¨æ ¼è¯»å–å‡ºæ¥æ˜¯ç±»ä¼¼ä¸€ä¸ª2ç»´çš„çŸ©é˜µï¼Œåˆå› ä¸ºåªæœ‰ä¸€ä¸ªåºåˆ—ï¼Œæ‰€ä»¥åœ¨ `data` å¤–å†å¥—ä¸€å±‚åˆ—è¡¨å†è½¬æ¢æˆnumpyæ•°ç»„å³å¯ã€‚

```python linenums="12"
data = data[:, :, 2:].astype(np.float32)
```

å†å°†å¤´ä¸¤åˆ— é€‰æ‰‹å å’Œ *åˆ†*æ•° å»é™¤ï¼Œå¹¶å°†æ•°æ®å…¨éƒ¨è½¬æ¢æˆnumpyçš„ `float32` ç±»å‹(å¦‚æœæ•°æ®æ ¼å¼ä¸åŒæ„ï¼Œåœ¨è®­ç»ƒæ—¶ä¼šæŠ¥é”™)ã€‚

```python linenums="14"
labels, data, val = data[:, :-15, :1], data[:, :-15, 1:], data[:, :, :]
Shi, val = val[:, :, :1], val[:, :, 1:]
```

å°†(å»é™¤åçš„)ç¬¬ä¸€åˆ—ä½œä¸ºç›®æ ‡æ•°æ®ï¼Œå…¶ä½™åˆ—ä½œä¸ºè¾“å…¥æ•°æ®ï¼Œå¹¶éƒ½é™¤å»æœ€å15ä¸ªæ•°æ®ç”¨äºä¹‹åçš„é¢„æµ‹æ¯”è¾ƒï¼Œ

å¤åˆ¶ä¸€ä»½å®Œæ•´æ•°æ®ç”¨äºé¢„æµ‹ã€‚

```python linenums="19"
def build_model():
    model = Sequential()
    model.add(GRU(50, input_shape=(None, 14), return_sequences=True))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
    model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜

    return model
```

å› ä¸ºè¾“å…¥æ•°æ®æœ‰14çš„æŒ‡æ ‡ï¼Œæ‰€ä»¥è®¾ä¸º14ç»´ï¼Œè€Œ**æ¿€æ´»å‡½æ•° `activation` å·²ç»æœ‰é»˜è®¤çš„è®¾ç½®**æ‰€ä»¥å°±ä¸ç”¨æ·»åŠ äº†ã€‚

!!! info

    ```python
    @keras_export("keras.layers.GRU", v1=[])
    class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):
        ...
        def __init__(
            self,
            units,
            activation="tanh",
            recurrent_activation="sigmoid",
            ...
    ```

```python linenums="31"
model.fit(data, labels, epochs=1000, batch_size=1)
```

ç”±äºåªæœ‰ä¸€ä¸ªåºåˆ—çš„æ•°æ®ï¼Œæ‰€ä»¥ `batch_size` è®¾ç½®ä¸º `1` ã€‚

æœ€åè¾“å‡ºçš„ç»“æœ(å±•ç¤ºäº†20ä¸ª)

```python
...
pre:  [0.47406548] act:  [0.55]
pre:  [-0.01765729] act:  [0.]
pre:  [0.18175407] act:  [0.2]
pre:  [0.49228787] act:  [0.45]
pre:  [0.97142494] act:  [1.]
pre:  [0.8183602] act:  [1.]
pre:  [0.731129] act:  [0.8]
pre:  [0.7134777] act:  [0.55]
pre:  [0.4599433] act:  [0.]
pre:  [0.8594324] act:  [0.2]
pre:  [1.2307503] act:  [0.25]
pre:  [1.5505791] act:  [0.55]
pre:  [1.2079588] act:  [0.2]
pre:  [1.3865234] act:  [0.45]
pre:  [1.6969306] act:  [0.8]
pre:  [1.2981229] act:  [0.75]
pre:  [0.9129096] act:  [0.45]
pre:  [1.3361117] act:  [0.]
pre:  [1.0873715] act:  [0.]
pre:  [0.786334] act:  [0.]
```

>   æ•°æ®ä¿å­˜æˆäº† `Shi.csv`

å°†ç”¨å…¬å¼è®¡ç®—çš„*åŠ¿å¤´*å’Œé¢„æµ‹çš„*åŠ¿å¤´*ç”¨excelç”»å‡ºæŠ˜çº¿å›¾æ¯”è¾ƒ

=== "version 1"

    ![mcm_first_predict](../images/mcm_first_predict.svg){ loading=lazy }

=== "version 2"

    ![mcm_first_predict_2](../images/mcm_first_predict_2.svg){ loading=lazy }

æ²¡æƒ³åˆ°åœ¨å®Œå…¨é¢„æµ‹éƒ¨åˆ†çš„ç»“æœä¸­ï¼Œå˜æ¢çš„è¶‹åŠ¿éƒ½æ˜¯èƒ½å¯¹åº”ä¸Šçš„ğŸ˜²ï¼Œæ‰€ä»¥æ„Ÿè§‰è¿˜ä¸é”™ğŸ˜„ã€‚

ç”±äºç´¯äº†ä¸€å¤©ï¼Œå¹¶ä¸”ä¸‹ä¸€å¤©å¯èƒ½è¿˜éœ€è¦ç†¬å¤œé€šå®µï¼Œæ‰€ä»¥æŠŠæ•°æ®ä¼ ç»™é˜Ÿå‹å°±ç»“æŸä»Šå¤©çš„ä»»åŠ¡äº†ã€‚

## Day 4 & Day 5

### æ‰©å¤§å†³èµ›æ•°æ®çš„é¢„æµ‹éƒ¨åˆ† (5å·æ—©)

>   å¤§çº¦æ—©ä¸Š8ç‚¹åŠè‡³9ç‚¹

æ‰“ç®—å‡å°‘ç”¨äºè®­ç»ƒçš„æ•°æ®å æ¯”ï¼Œå°†å®Œå…¨è¿›è¡Œé¢„æµ‹çš„æ•°æ®å¢å¤§åˆ°1/3ï¼Œçœ‹çœ‹é¢„æµ‹çš„æ•ˆæœ

```python title="my_test5.py" linenums="14"
nums = len(data[0])
labels, data, val = data[:, :-nums//3, :1], data[:, :-nums//3, 1:], data[:, :, :]
```

>   æ•°æ®ä¿å­˜æˆäº† `Shi_2.csv`

=== "version 1"

    ![mcm_second_predict](../images/mcm_second_predict.svg){ loading=lazy }

=== "version 2"

    ![mcm_second_predict_2](../images/mcm_second_predict_2.svg){ loading=lazy }

### å°è¯•æŸ¥çœ‹ `.h5` æƒé‡æ–‡ä»¶

è¿™æ—¶æƒ³é€šè¿‡æŸ¥çœ‹æƒé‡æ–‡ä»¶ä¸­çš„å‚æ•°æ¥åˆ†ææ¯ä¸ªæŒ‡æ ‡/ç»´åº¦ä¸*åŠ¿å¤´*çš„ç›¸å…³æ€§ï¼Œ

äºæ˜¯æœç´¢ï¼Œç„¶åå‘ç°è¿™ä¸¤ç¯‡æ–‡ç« 

-   [å¯è§†åŒ–å·¥å…·æ‰“å¼€hdf5æ–‡ä»¶_hdf5æ–‡ä»¶ç”¨ä»€ä¹ˆæ‰“å¼€-CSDNåšå®¢](https://blog.csdn.net/qq_42508076/article/details/116600303)
-   [æŸ¥çœ‹h5æ–‡ä»¶æ–¹æ³•â€”â€”vitables-CSDNåšå®¢](https://blog.csdn.net/weixin_45674669/article/details/130029509)

éƒ½æåˆ°ä½¿ç”¨vitablesæ¥æŸ¥çœ‹ `.h5` æ–‡ä»¶ï¼Œäºæ˜¯æŒ‰ç…§æ–‡ç« æ‰€è¯´ï¼Œ `pip install vitables` ä¹‹åï¼Œ

```bash
vitables Shi_2.h5
```

![vitables](../images/vitables.png){ loading=lazy }

ä½†å¼„äº†ä¸€ç•ªä¹‹åæ„Ÿè§‰ä¼¼ä¹å¹¶ä¸æ˜¯å¤ªæœ‰ç”¨ï¼Œäºæ˜¯å°±ä¸æ‰“ç®—å†ç ”ç©¶ `.h5` æ–‡ä»¶äº†

>   æœŸé—´è¿˜æœ‰å°è¯•ç”Ÿæˆä¸åŒçš„å•ä½å‘é‡(æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ª)ï¼Œç„¶åè¾“å…¥è¿›è¡Œé¢„æµ‹ï¼Œè¿›è€ŒæŸ¥çœ‹ç»“æœï¼Œç„¶åé€šè¿‡ç»“æœæ¥åˆ†æä¸åŒç»´åº¦å¯¹*åŠ¿å¤´*çš„ç›¸å…³æ€§ï¼Œ
>
>   ä½†æ˜¯åæ¥æƒ³äº†æƒ³ï¼Œè§‰å¾—æœ‰äº›ä¸åˆç†ï¼Œå¹¶ä¸”å…³ç³»éƒ½æ˜¯éçº¿æ€§çš„ï¼Œå¯èƒ½é¢„æµ‹çš„ç»“æœä½“ç°ä¸å‡ºç›¸å…³æ€§ï¼Œæœ€åå°±æ”¾å¼ƒäº†è¿™æ ·çš„æ€è·¯

### å¢æ·»å†³èµ›çš„æ•°æ®å¹¶å†æ¬¡è®­ç»ƒ (5å·åˆ)

å¿«ä¸­åˆæ—¶ï¼ŒæŸ¥çœ‹å†³èµ›çš„æ•°æ®çš„æ—¶å€™ï¼Œæ„Ÿè§‰ **æ¯ä¸€å±€ä¸­è¿˜å‰©å‡ åˆ†èƒ½è·èƒœ** åº”è¯¥æ˜¯ä¸€ä¸ªå¯¹åŠ¿å¤´å½±å“æ¯”è¾ƒå¤§çš„å› ç´ ï¼Œè€Œé˜Ÿå‹å´æ²¡æœ‰æ”¾åœ¨csvæ–‡ä»¶ä¸­ï¼Œäºæ˜¯è®©é˜Ÿå‹é‡æ–°å¤„ç†æ·»åŠ äº†æ•°æ®ï¼Œ

ç„¶ååˆ°äº†ä¸‹åˆæˆ‘é‡æ–°è¿›è¡Œäº†è®­ç»ƒå’Œé¢„æµ‹ã€‚

>   æ•°æ®ä¿å­˜æˆäº† `Shi_3.csv`

---

è¿™ä¸ªæ—¶å€™è¿˜é¡ºå¸¦å‘ç°äº†ï¼ŒåŸæ¥tensorflowå¯ä»¥ç›´æ¥ä¿å­˜æ¨¡å‹æ–‡ä»¶ğŸ˜…(è€Œä¸æ˜¯æƒé‡æ–‡ä»¶)

???+ tip

    ä¹‹å‰chatgptæä¾›çš„ä»£ç (æœ€å¼€å§‹æä¾›çš„tensorflow GRUæ¨¡å‹çš„ä»£ç )ä¸­åŒ…å«äº†ä¿å­˜æƒé‡æ–‡ä»¶çš„éƒ¨åˆ†ï¼Œ
    
    ```python
    # ä¿å­˜æƒé‡æ–‡ä»¶
    model.save_weights('gru_weights.h5')
    ```
    
    äºæ˜¯åœ¨é‚£ä¹‹åä¸ä¹…åˆå»è¯¢é—®äº†å¦‚ä½•åŠ è½½æƒé‡æ–‡ä»¶ï¼Œ
    
    ```python
    def build_model():
        ...
    
    model = build_model()
    model.load_weights('gru_weights.h5')
    ```
    
    å¹¶ä¸”éœ€è¦æ³¨æ„ï¼Œ**==æ„å»ºçš„æ¨¡å‹éœ€è¦ä¸ä¹‹å‰çš„æ¨¡å‹ç»“æ„ä¸€æ ·==**ï¼Œå¦åˆ™å¯èƒ½ä¼šæŠ¥é”™ã€‚

å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹é€šè¿‡ `save` æ–¹æ³•å³å¯ä¿å­˜ï¼Œå¦‚

```python
model.save('my_gru_model.keras')
```

>   kerasæ–‡ä»¶åº”è¯¥å°±æ˜¯tensorflowè‡ªå·±çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼ï¼Œå› ä¸ºkeraså¥½åƒå°±æ˜¯tensorflowä¸‹çš„ä¸€ä¸ªåŒ…

ç„¶åå°±å¯ä»¥(==éå¸¸è¿…é€Ÿåœ°==ğŸ˜)**ç›´æ¥ä»æ–‡ä»¶ä¸­åŠ è½½æ¨¡å‹**(ä¸ä¹‹ç›¸å¯¹çš„å°±æ˜¯ï¼Œå…ˆæ­å»ºå¥½æ¨¡å‹å†ç”¨æ¨¡å‹åŠ è½½æƒé‡æ–‡ä»¶)

```python
from tensorflow.keras.models import load_model

model = load_model("my_gru_model.keras")
```

ç„¶åå°±å¯ä»¥ç›´æ¥è¿›è¡Œé¢„æµ‹ç­‰æ“ä½œã€‚

### ç€æ‰‹å¤„ç†å…¨éƒ¨çš„æ•°æ®

é˜Ÿå‹ä¹‹å‰å·²ç»å°†è¿™åˆ—æˆ‘ä»¬è®¤ä¸ºæ²¡æœ‰å¤ªå¤§ä½œç”¨çš„æ•°æ®å»é™¤äº†ï¼Œ

```css
winner_shot_type
p1_distance_run
p2_distance_run
rally_count
speed_mph
serve_widt
serve_depth
return_depth
```

ç„¶åæˆ‘åˆè®©ä»–å†çœ‹çœ‹è¿˜æœ‰ä»€ä¹ˆåœ¨å¤„ç†å†³èµ›æ•°æ®æ—¶èƒ½ç”¨ä¸Šçš„æ“ä½œï¼Œå‘ç°åªèƒ½æ›¿æ¢åˆ†æ•°ï¼Œ

>   åœ¨ `p1_score` å’Œ `p2_score` é‚£ä¸¤åˆ—ï¼Œ
>
>   -   æŠŠ `15` æ›¿æ¢æˆ `1`
>   -   æŠŠ `30` æ›¿æ¢æˆ `2`
>   -   æŠŠ `40` æ›¿æ¢æˆ `3`
>   -   æŠŠ `AD` æ›¿æ¢æˆ `4`

äºæ˜¯ä»–æ›¿æ¢å¥½åˆ†æ•°ä¹‹åæŠŠæ€»çš„csvå‘ç»™äº†æˆ‘ï¼Œç„¶åæˆ‘å°±å¼€å§‹å†™å¤„ç†å…¨éƒ¨æ•°æ®çš„pythonä»£ç ã€‚

>   æ­¤æ—¶å¤§çº¦ä¸‹åˆ4ç‚¹

---

è¿™æ—¶æˆ‘æœ‰ä¸¤ç§æ€è·¯ï¼Œ

1.   é€‰å‡ºç¬¬ä¸€è½®32ä¸ªé€‰æ‰‹çš„æ•°æ®ï¼Œç„¶åç”¨32ä¸ªæ•°æ®åºåˆ—æ¥è¿›è¡Œè®­ç»ƒ
2.   é€‰å‡ºå† å†›5åœºæ¯”èµ›çš„æ‰€æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒ

æˆ‘å…ˆå¤„ç†ç¬¬ä¸€ç§æ€è·¯ï¼Œ

???+ note "`my_test7.py` `my_test8.py` `my_test9.py`"

    === "`my_test7.py`"
    
        ```python title="my_test7.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        all_data = pd.read_csv("Wimbledon_featured_matches.csv")
    
        print(all_data)
        print(all_data.columns)
    
        all_data = np.array(all_data)
    
        print(all_data)
    
        # exit(0)
    
        # å°† match_id ç»“å°¾ä¸º 13xx çš„æ•°æ®ç­›é€‰å‡ºæ¥ï¼Œå¹¶ä¸”å°†æ¯ä¸€ä¸ª id çš„æ•°æ®å­˜åœ¨å•ç‹¬çš„ csv æ–‡ä»¶ä¸­
        data_of_13 = np.array(list(filter(lambda x: int(x[0][-4:]) // 100 == 13, all_data)))
    
        print(data_of_13)
    
        result = {}
        for data in data_of_13:
            if data[0] not in result:
                result[data[0]] = []
            result[data[0]].append(data)
        for i, data in zip(range(len(result)), result.values()):
            df = pd.DataFrame(data)
            # name like: 00 01 02 ...
            df.to_csv(f"everyone/{(2*i+1):02d}.csv", index=False, header=True)
            # copy the file to another name
            df.to_csv(f"everyone/{(2*i+2):02d}.csv", index=False, header=True)
        ```
    
    === "`my_test8.py`"
    
        ```python title="my_test8.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        all_data = pd.read_csv("Wimbledon_featured_matches.csv")
    
        print(all_data)
        print(all_data.columns)
    
        col_list = list(all_data.columns)
        thing_to_find = ("match_id", "elapsed_time", "set_no", "game_no", "point_no",
                         "game_victor", "set_victor", "p1_net_pt", "p2_net_pt", "p1_net_pt_won",
                         "p2_net_pt_won", 'p1_points_won', 'p2_points_won')
        # get the index of the column
        index_to_find = []
        for i in range(len(col_list)):
            if col_list[i] in thing_to_find:
                index_to_find.append(i)
    
        # exit(0)
    
        for i in range(1, 33):
            data = pd.read_csv(f"everyone/{i:02d}.csv")
            data = np.array(data)
            # delete the column whose index is in index_to_find
            data = np.delete(data, index_to_find, axis=1)
            # print(data)
    
            # save the data to the file
            df = pd.DataFrame(data)
            df.to_csv(f"everyone/{i:02d}.csv", index=False, header=True)
        ```
    
    === "`my_test9.py`"
    
        ```python title="my_test9.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        """
        ['player1', 'player2',
               'p1_sets', 'p2_sets', 
               'p1_games', 'p2_games', 
               'p1_score', 'p2_score', 
               'server', 'serve_no', 
               'point_victor',
               'p1_ace', 'p2_ace',
               'p1_winner', 'p2_winner', 
               'p1_double_fault', 'p2_double_fault',
               'p1_unf_err', 'p2_unf_err',
               'p1_break_pt', 'p2_break_pt', 
               'p1_break_pt_won', 'p2_break_pt_won', 
               'p1_break_pt_missed', 'p2_break_pt_missed']
        """
    
        for i in range(1, 33):
            data = pd.read_csv(f"everyone/{i:02d}.csv")
            data = np.array(data)
    
            # ç®— é¢†å…ˆç›˜æ•° é¢†å…ˆå±€æ•° é¢†å…ˆçƒæ•° æ˜¯ä¸æ˜¯ä¸€å±€æœ€åä¸‰çƒ å‡ çƒèµ¢è¿™å±€ åŠ¿å¤´
            # add a new column to the end of data
            new_column = np.zeros((len(data), 6))
            data = np.hstack((data, new_column))
    
            for j in range(len(data)):
                data[j][-6] = data[j][2] - data[j][3]
                data[j][-5] = data[j][4] - data[j][5]
                data[j][-4] = data[j][6] - data[j][7]
                if i % 2 == 0:
                    data[j][-6] *= -1
                    data[j][-5] *= -1
                    data[j][-4] *= -1
    
                # æ˜¯å¦å½±å“ä¸‹ä¸€å±€
                if j < len(data) - 3:
                    data[j][-3] = 0 if data[j][8] == data[j+3][8] else 1
    
                # åˆ¤æ–­è¿˜å‰©å‡ çƒèµ¢è¿™å±€
                me, they = (6, 7) if i % 2 == 1 else (7, 6)
                if data[j][4] == 6 and data[j][5] == 6:  # åˆ¤æ–­æ˜¯å¦æŠ¢ä¸ƒ
                    target = 10 if data[j][2] == 2 and data[j][3] == 2 else 7
                    # å¯¹æ‰‹è¶…è¿‡6å°±åŠ åˆ†
                    data[j][-2] = data[j][they] + 2 - data[j][me] if data[j][they] >= target - 1 else target - data[j][me]
                else:
                    if data[j][they] < 3:
                        data[j][-2] = 4 - data[j][me]
                    elif data[j][they] == 3:
                        data[j][-2] = 5 - data[j][me]
                    else:
                        data[j][-2] = 3
    
            index_to_delete = list(range(i%2, 8, 2)) + list(range(i%2+11, 25, 2))
    
            data = np.delete(data, index_to_delete, axis=1)
    
            """
            ['player1',
                   'p1_sets',
                   'p1_games',
                   'p1_score',
                   'server', 'serve_no', 
                   'point_victor',
                   'p1_ace',
                   'p1_winner', 
                   'p1_double_fault',
                   'p1_unf_err',
                   'p1_break_pt', 
                   'p1_break_pt_won', 
                   'p1_break_pt_missed']
            """
    
            for x in data:
                if x[4]%2 == i % 2:
                    x[4] = 1
                else:
                    x[4] = 0
    
                if x[6]%2 == i % 2:
                    x[6] = 1
                else:
                    x[6] = 0
    
            # è®¡ç®—åŠ¿å¤´
            a, b, c = 0.55, 0.25, 0.2
            for j in range(len(data) - 3):
                data[j][-1] = a * data[j+1][6] + b * data[j+2][6] + c * data[j+3][6]
    
            # save the data to the file
            df = pd.DataFrame(data)
            df.to_csv(f"everyone/{i:02d}.csv", index=False, header=True)
        ```

<h4><big>ç¬¬ä¸€æ­¥ï¼Œåœ¨ my_test7.py ä¸­</big></h4>

å…ˆè¯»å–äº†csvä¸­çš„æ•°æ®å¹¶è½¬æ¢æˆnumpyæ•°ç»„ï¼Œ

```python
all_data = pd.read_csv("Wimbledon_featured_matches.csv")
...
all_data = np.array(all_data)
```

å†å°† `match_id` ç»“å°¾ä¸º `13xx` çš„æ•°æ®ç­›é€‰å‡ºæ¥ï¼Œ

!!! tip

    åœ¨ `Wimbledon_featured_matches.csv` ä¸­ï¼Œä¸€å…±è¿›è¡Œäº†5è½®æ¯”èµ›ï¼Œ
    
    -   ç¬¬1è½®ï¼Œ`match_id` ä»¥ `13xx` ç»“å°¾ï¼Œå¯¹åº”**32è¿›16**
    -   ç¬¬2è½®ï¼Œ`match_id` ä»¥ `14xx` ç»“å°¾ï¼Œå¯¹åº”**16è¿›8**
    -   ç¬¬3è½®ï¼Œ`match_id` ä»¥ `15xx` ç»“å°¾ï¼Œå¯¹åº”**8è¿›4**
    -   ç¬¬4è½®ï¼Œ`match_id` ä»¥ `16xx` ç»“å°¾ï¼Œå¯¹åº”**4è¿›2**
    -   ç¬¬5è½®ï¼Œ`match_id` ä»¥ `17xx` ç»“å°¾ï¼Œå¯¹åº”**å†³èµ›**

å¹¶ä¸”å°†æ¯ä¸€ä¸ª id çš„æ•°æ®å­˜åœ¨å•ç‹¬çš„csvæ–‡ä»¶ä¸­ï¼Œ

ä¸”ç”±äºæ¯åœºæ¯”èµ›æœ‰ä¸¤ä¸ªé€‰æ‰‹çš„æ•°æ®ï¼Œæ‰€ä»¥æ¯åœºæ¯”èµ›éƒ½å„å¤åˆ¶äº†ä¸€ä»½ï¼Œ

```python linenums="15"
# å°† match_id ç»“å°¾ä¸º 13xx çš„æ•°æ®ç­›é€‰å‡ºæ¥ï¼Œå¹¶ä¸”å°†æ¯ä¸€ä¸ª id çš„æ•°æ®å­˜åœ¨å•ç‹¬çš„ csv æ–‡ä»¶ä¸­
data_of_13 = np.array(list(filter(lambda x: int(x[0][-4:]) // 100 == 13, all_data)))

print(data_of_13)

result = {}
for data in data_of_13:
    if data[0] not in result:
        result[data[0]] = []
    result[data[0]].append(data)
for i, data in zip(range(len(result)), result.values()):
    df = pd.DataFrame(data)
    # name like: 00 01 02 ...
    df.to_csv(f"everyone/{(2*i+1):02d}.csv", index=False, header=True)
    # copy the file to another name
    df.to_csv(f"everyone/{(2*i+2):02d}.csv", index=False, header=True)
```

è¿™é‡Œåœ¨è°ƒç”¨ `to_csv` æ—¶ï¼Œè®¾ç½® `header=True` æ˜¯å› ä¸ºpandasåœ¨ `read_csv` æ—¶ä¼šé»˜è®¤æŠŠç¬¬ä¸€è¡Œå½“ä½œåˆ—æ ‡é¢˜(ä¹‹å‰ä¹Ÿæœ‰æåˆ°è¿‡)ï¼Œæ‰€ä»¥åœ¨æ‘æ–‡ä»¶çš„æ—¶å€™è¦ç»™æ–°çš„csvæ·»åŠ åˆ—æ ‡é¢˜ã€‚

<h4><big>ç¬¬äºŒæ­¥ï¼Œåœ¨ my_test8.py ä¸­</big></h4>

ç”±äºéœ€è¦åˆ é™¤å‡ åˆ—æ²¡æœ‰ç”¨çš„æ•°æ®ï¼Œæ‰€ä»¥é‡‡å–äº†è¿™æ ·çš„æ–¹æ³•ï¼Œ

å› ä¸ºåŸæ¥çš„ `Wimbledon_featured_matches.csv` åˆ—åæ˜¯æ•°æ®åï¼Œè€Œè¦ä½¿ç”¨numpyä¸­çš„ `detele` å‡½æ•°éœ€è¦ä¼ å…¥è¦åˆ é™¤åˆ—çš„ä¸‹æ ‡åºå·ï¼Œ

æ‰€ä»¥æˆ‘å…ˆåœ¨ `Wimbledon_featured_matches.csv` ä¸­è·å–äº†åˆ—åçš„åˆ—è¡¨ï¼Œ

```python
all_data = pd.read_csv("Wimbledon_featured_matches.csv")
...
col_list = list(all_data.columns)
```

å†è·å–è¦åˆ å»çš„åˆ—çš„ä¸‹æ ‡åºå·ï¼Œ

```python linenums="10"
thing_to_find = ("match_id", "elapsed_time", "set_no", "game_no", "point_no",
                 "game_victor", "set_victor", "p1_net_pt", "p2_net_pt", "p1_net_pt_won",
                 "p2_net_pt_won", 'p1_points_won', 'p2_points_won')
# get the index of the column
index_to_find = []
for i in range(len(col_list)):
    if col_list[i] in thing_to_find:
        index_to_find.append(i)
```

æœ€ååœ¨ç¬¬ä¸€æ­¥å¾—åˆ°çš„csvä¸­åˆ å»å¯¹åº”çš„åˆ—ï¼Œ

```python linenums="21"
for i in range(1, 33):
    data = pd.read_csv(f"everyone/{i:02d}.csv")
    data = np.array(data)
    # delete the column whose index is in index_to_find
    data = np.delete(data, index_to_find, axis=1)
    # print(data)

    # save the data to the file
    df = pd.DataFrame(data)
    df.to_csv(f"everyone/{i:02d}.csv", index=False, header=True)
```

<h4><big>ç¬¬ä¸‰æ­¥ï¼Œåœ¨ my_test9.py ä¸­</big></h4>

éœ€è¦å°†æ¯ä¸ªé€‰æ‰‹çš„æ•°æ®ä¿ç•™è€Œåˆ å»å¯¹æ‰‹çš„æ•°æ®ï¼Œå¹¶æ·»åŠ /è®¡ç®—ä¸€äº›æ‰€éœ€çš„æ•°æ®ã€‚

ç”±äºæœ‰6ä¸ªæ•°æ®éœ€è¦æ–°æ·»ï¼Œæ‰€ä»¥åœ¨çŸ©é˜µæœ«å°¾æ·»åŠ 6ä¸ªæ–°åˆ—ï¼Œå¹¶åˆå§‹åŒ–ä¸º0ï¼Œ

```python linenums="24"
# ç®— é¢†å…ˆç›˜æ•° é¢†å…ˆå±€æ•° é¢†å…ˆçƒæ•° æ˜¯ä¸æ˜¯ä¸€å±€æœ€åä¸‰çƒ å‡ çƒèµ¢è¿™å±€ åŠ¿å¤´
# add a new column to the end of data
new_column = np.zeros((len(data), 6))
data = np.hstack((data, new_column))
```

???+ tip

    `np.hstack` çš„ç”¨æ³•ä¼¼ä¹æ˜¯åœ¨numpyçŸ©é˜µçš„**æœ€åä¸€ä¸ªå±‚(æˆ–è€…è¯´æ•°ç»„çš„ç»´åº¦)**æ·»åŠ æ•°æ®ã€‚
    
    è€Œnumpyè¿˜æœ‰ä¸€ä¸ª `np.stack` å‡½æ•°ï¼Œä¸ `np.hstack` ç”¨æ³•ç±»ä¼¼ï¼Œä½†è¿˜éœ€è¦ä¼ å…¥ä¸€ä¸ªå‚æ•° `axis=` æ¥æŒ‡å®š**åœ¨å“ªä¸€å±‚æ’å…¥æ–°çš„å…ƒç´ **
    
    ???+ example "chatlog"
    
        **User**:
    
        å¦‚ä½•å°†å¤šä¸ª äºŒç»´ numpy array åˆå¹¶æˆ ä¸€ä¸ªä¸‰ç»´ numpy array?
    
        &nbsp;
    
        **ChatGPT**:
    
        è¦å°†å¤šä¸ªäºŒç»´NumPyæ•°ç»„åˆå¹¶æˆä¸€ä¸ªä¸‰ç»´NumPyæ•°ç»„ï¼Œå¯ä»¥ä½¿ç”¨ `np.stack` å‡½æ•°æˆ– `np.concatenate` å‡½æ•°ã€‚è¿™ä¸¤ä¸ªå‡½æ•°å…è®¸ä½ åœ¨æŒ‡å®šçš„è½´ä¸Šåˆå¹¶æ•°ç»„ã€‚
    
        ä»¥ä¸‹æ˜¯ä½¿ç”¨ `np.stack` çš„ç¤ºä¾‹ï¼š
    
        ```python
        import numpy as np
    
        # å‡è®¾æœ‰ä¸¤ä¸ªäºŒç»´æ•°ç»„ arr1 å’Œ arr2
        arr1 = np.array([[1, 2, 3], [4, 5, 6]])
        arr2 = np.array([[7, 8, 9], [10, 11, 12]])
    
        # ä½¿ç”¨ np.stack æ²¿ç€æ–°çš„è½´ï¼ˆç¬¬ä¸‰ç»´åº¦ï¼‰åˆå¹¶æ•°ç»„
        result = np.stack((arr1, arr2), axis=2)
    
        # æ‰“å°ç»“æœ
        print(result)
        ```
    
        æˆ–è€…ï¼Œä½¿ç”¨ `np.concatenate` ï¼š
    
        ```python
        import numpy as np
    
        # ä½¿ç”¨ np.concatenate æ²¿ç€æ–°çš„è½´ï¼ˆç¬¬ä¸‰ç»´åº¦ï¼‰åˆå¹¶æ•°ç»„
        result = np.concatenate((arr1[:, :, np.newaxis], arr2[:, :, np.newaxis]), axis=2)
    
        # æ‰“å°ç»“æœ
        print(result)
        ```
    
        è¿™ä¸¤ç§æ–¹æ³•éƒ½å°† `arr1` å’Œ `arr2` åˆå¹¶åˆ°ä¸€ä¸ªæ–°çš„ä¸‰ç»´æ•°ç»„ä¸­ï¼Œå…¶ä¸­ç¬¬ä¸‰ç»´åº¦æ˜¯åˆå¹¶çš„ç»´åº¦ã€‚ç¡®ä¿è°ƒæ•´è½´å’Œæ•°ç»„çš„å½¢çŠ¶ä»¥é€‚åº”ä½ çš„å®é™…éœ€æ±‚ã€‚

-   è®¡ç®—é¢†å…ˆçš„ç›˜æ•°ï¼Œå¦‚æœæ˜¯p2(player2)ï¼Œåˆ™ä¹˜ä¸ª `-1`

    ```python linenums="30"
    data[j][-6] = data[j][2] - data[j][3]
    data[j][-5] = data[j][4] - data[j][5]
    data[j][-4] = data[j][6] - data[j][7]
    if i % 2 == 0:
        data[j][-6] *= -1
        data[j][-5] *= -1
        data[j][-4] *= -1
    ```

-   åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€å±€çš„æœ€åå…¶ä¸­ä¸‰åˆ†(è®¡ç®—çš„åŠ¿å¤´åŒ…å«ä¸‹ä¸€å±€çš„åˆ†)

    ```python linenums="38"
    # æ˜¯å¦å½±å“ä¸‹ä¸€å±€
    if j < len(data) - 3:
        data[j][-3] = 0 if data[j][8] == data[j+3][8] else 1
    ```

-   è®¡ç®—è¿˜å‰©å‡ åˆ†å¯ä»¥èµ¢å¾—è¿™ä¸€å±€(**==éœ€è¦åˆ¤æ–­æ˜¯å¦æ˜¯==*==æŠ¢ä¸ƒ==*==æˆ–è€…==*==æŠ¢å==*==çš„æƒ…å†µ==**)

    ```python linenums="42"
    # åˆ¤æ–­è¿˜å‰©å‡ çƒèµ¢è¿™å±€
    me, they = (6, 7) if i % 2 == 1 else (7, 6)
    if data[j][4] == 6 and data[j][5] == 6:  # åˆ¤æ–­æ˜¯å¦æŠ¢ä¸ƒ
        target = 10 if data[j][2] == 2 and data[j][3] == 2 else 7
        # å¯¹æ‰‹è¶…è¿‡6å°±åŠ åˆ†
        data[j][-2] = data[j][they] + 2 - data[j][me] if data[j][they] >= target - 1 else target - data[j][me]
    else:
        if data[j][they] < 3:
            data[j][-2] = 4 - data[j][me]
        elif data[j][they] == 3:
            data[j][-2] = 5 - data[j][me]
        else:
            data[j][-2] = 3
    ```

åˆ é™¤å¯¹æ‰‹çš„æ•°æ®ï¼Œ

```python linenums="56"
index_to_delete = list(range(i%2, 8, 2)) + list(range(i%2+11, 25, 2))

data = np.delete(data, index_to_delete, axis=1)
```

å¤„ç†é€‰æ‰‹å½“å‰åˆ†æ˜¯å¦èµ¢çš„æ•°æ®(åŸæœ¬ä¸º `point_victor` ï¼Œ `p1` èµ¢åˆ™ä¸º `1` ï¼Œ `p2` èµ¢åˆ™ä¸º `2` )ï¼Œ

```python linenums="76"
for x in data:
    if x[4]%2 == i % 2:
        x[4] = 1
    else:
        x[4] = 0

    if x[6]%2 == i % 2:
        x[6] = 1
    else:
        x[6] = 0
```

è®¡ç®—*åŠ¿å¤´*ï¼Œ

```python linenums="87"
# è®¡ç®—åŠ¿å¤´
a, b, c = 0.55, 0.25, 0.2
for j in range(len(data) - 3):
    data[j][-1] = a * data[j+1][6] + b * data[j+2][6] + c * data[j+3][6]
```

æœ€åä¿å­˜æ•°æ®ï¼Œ

```python linenums="92"
# save the data to the file
df = pd.DataFrame(data)
df.to_csv(f"everyone/{i:02d}.csv", index=False, header=True)
```

---

ç»§ç»­å¤„ç†ç¬¬äºŒç§æ€è·¯ï¼Œ

??? note "`my_test7_1.py` `my_test8_1.py` `my_test9_1.py`"

    === "`my_test7_1.py`"
    
        ```python title="my_test7_1.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        all_data = pd.read_csv("Wimbledon_featured_matches.csv")
    
        print(all_data)
        print(all_data.columns)
    
        all_data = np.array(all_data)
    
        print(all_data)
    
        # exit(0)
    
        # å°† match_id ç»“å°¾ä¸º 13xx çš„æ•°æ®ç­›é€‰å‡ºæ¥ï¼Œå¹¶ä¸”å°†æ¯ä¸€ä¸ª id çš„æ•°æ®å­˜åœ¨å•ç‹¬çš„ csv æ–‡ä»¶ä¸­
        data_of_one = np.array(list(filter(lambda x: x[1] == "Carlos Alcaraz", all_data)))
    
        print(data_of_one)
    
        result = {}
        for data in data_of_one:
            if data[0] not in result:
                result[data[0]] = []
            result[data[0]].append(data)
    
        for data in result.values():
            df = pd.DataFrame(data)
    
            # name like: 00 01 02 ...
            df.to_csv(f"onlyone/{data[0][0][-4:]}.csv", index=False, header=True)
        ```
    
    === "`my_test8_1.py`"
    
        ```python title="my_test8_1.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        all_data = pd.read_csv("Wimbledon_featured_matches.csv")
    
        print(all_data)
        print(all_data.columns)
    
        col_list = list(all_data.columns)
        thing_to_find = ("match_id", "elapsed_time", "set_no", "game_no", "point_no",
                        "game_victor", "set_victor", "p1_net_pt", "p2_net_pt", "p1_net_pt_won",
                        "p2_net_pt_won", 'p1_points_won', 'p2_points_won')
        # get the index of the column
        index_to_find = []
        for i in range(len(col_list)):
            if col_list[i] in thing_to_find:
                index_to_find.append(i)
    
        # exit(0)
    
        for i in range(13, 18):
            data = pd.read_csv(f"onlyone/{i:02d}01.csv")
            data = np.array(data)
            # delete the column whose index is in index_to_find
            data = np.delete(data, index_to_find, axis=1)
            # print(data)
    
            # save the data to the file
            df = pd.DataFrame(data)
            df.to_csv(f"onlyone/{i:02d}01.csv", index=False, header=True)
        ```
    
    === "`my_test9_1.py`"
    
        ```python title="my_test9_1.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        """
        ['player1', 'player2',
            'p1_sets', 'p2_sets', 
            'p1_games', 'p2_games', 
            'p1_score', 'p2_score', 
            'server', 'serve_no', 
            'point_victor',
            'p1_ace', 'p2_ace',
            'p1_winner', 'p2_winner', 
            'p1_double_fault', 'p2_double_fault',
            'p1_unf_err', 'p2_unf_err',
            'p1_break_pt', 'p2_break_pt', 
            'p1_break_pt_won', 'p2_break_pt_won', 
            'p1_break_pt_missed', 'p2_break_pt_missed']
        """
    
        for i in range(13, 18):
            data = pd.read_csv(f"onlyone/{i:02d}01.csv")
            data = np.array(data)
    
            # ç®— é¢†å…ˆç›˜æ•° é¢†å…ˆå±€æ•° é¢†å…ˆçƒæ•° æ˜¯ä¸æ˜¯ä¸€å±€æœ€åä¸‰çƒ å‡ çƒèµ¢è¿™å±€ åŠ¿å¤´
            # add a new column to the end of data
            new_column = np.zeros((len(data), 6))
            data = np.hstack((data, new_column))
    
            for j in range(len(data)):
                data[j][-6] = data[j][2] - data[j][3]
                data[j][-5] = data[j][4] - data[j][5]
                data[j][-4] = data[j][6] - data[j][7]
                # if i % 2 == 0:
                #     data[j][-6] *= -1
                #     data[j][-5] *= -1
                #     data[j][-4] *= -1
    
                # æ˜¯å¦å½±å“ä¸‹ä¸€å±€
                if j < len(data) - 3:
                    data[j][-3] = 0 if data[j][8] == data[j+3][8] else 1
    
                # åˆ¤æ–­è¿˜å‰©å‡ çƒèµ¢è¿™å±€
                me, they = (6, 7)  # if i % 2 == 1 else (7, 6)
                if data[j][4] == 6 and data[j][5] == 6:  # åˆ¤æ–­æ˜¯å¦æŠ¢ä¸ƒ
                    target = 10 if data[j][2] == 2 and data[j][3] == 2 else 7
                    # å¯¹æ‰‹è¶…è¿‡6å°±åŠ åˆ†
                    data[j][-2] = data[j][they] + 2 - data[j][me] if data[j][they] >= target - 1 else target - data[j][me]
                else:
                    if data[j][they] < 3:
                        data[j][-2] = 4 - data[j][me]
                    elif data[j][they] == 3:
                        data[j][-2] = 5 - data[j][me]
                    else:
                        data[j][-2] = 3
    
            index_to_delete = list(range(1, 8, 2)) + list(range(1+11, 25, 2))
    
            data = np.delete(data, index_to_delete, axis=1)
    
            """
            ['player1',
                'p1_sets',
                'p1_games',
                'p1_score',
                'server', 'serve_no', 
                'point_victor',
                'p1_ace',
                'p1_winner', 
                'p1_double_fault',
                'p1_unf_err',
                'p1_break_pt', 
                'p1_break_pt_won', 
                'p1_break_pt_missed']
            """
    
            for x in data:
                if x[4] == 1:
                    x[4] = 1
                else:
                    x[4] = 0
    
                if x[6] == 1:
                    x[6] = 1
                else:
                    x[6] = 0
    
            # è®¡ç®—åŠ¿å¤´
            a, b, c = 0.55, 0.25, 0.2
            for j in range(len(data) - 3):
                data[j][-1] = a * data[j+1][6] + b * data[j+2][6] + c * data[j+3][6]
    
            # save the data to the file
            df = pd.DataFrame(data)
            df.to_csv(f"onlyone/{i:02d}01.csv", index=False, header=True)
        ```

è¿™æ¬¡æˆ‘åŸºæœ¬ä¸Šæ˜¯åŸºäºå¤„ç†ç¬¬ä¸€ç§æ€è·¯æ—¶çš„ä»£ç è¿›è¡Œä¿®æ”¹çš„ï¼Œæ‰€ä»¥å¤§ä½“ä¸Šå·®ä¸å¤šã€‚

### ç”¨å¤„ç†å¥½çš„æ•°æ®è¿›è¡Œè®­ç»ƒåŠé¢„æµ‹ (5å·æ™š)

??? note "ä¸¤ç§æ€è·¯åˆ†åˆ«å¯¹åº”çš„æ¨¡å‹è®­ç»ƒpythonä»£ç "

    === "`my_test10.py`"
    
        ```python title="my_test10.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        import tensorflow as tf
        from tensorflow.keras import Sequential
        from tensorflow.keras.layers import GRU, Dense
    
        all_datas = []
        for i in range(1, 33):
            if i != 5 and i != 6:
                data = pd.read_csv(f"everyone/{i:02d}.csv")
                all_datas.append(np.array(data)[:164, :])
        # data = pd.read_csv("CA(test)_2.csv")
        data = np.stack(tuple(all_datas))
        print(data)
    
        data = data[:, :, 1:].astype(np.float32)  # delete name
        print(data)
    
        # input()
    
        nums = len(data[0])
        labels, data, val = data[:, :, -1:], data[:, :, :-1], data[:, -nums//3:, :]
        # Shi, val = val[:, :, :1], val[:, :, 1:]
        print(data)
        print(labels)
    
        # input()
    
        def build_model():
            model = Sequential()
            model.add(GRU(50, input_shape=(None, 18), return_sequences=True))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
            model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
            return model
    
        # ç¼–è¯‘æ¨¡å‹
        model = build_model()
        model.compile(optimizer='adam', loss="mean_absolute_error")
    
        # è®­ç»ƒæ¨¡å‹
        model.fit(data, labels, epochs=10000, batch_size=30)
    
        # ä¿å­˜æƒé‡æ–‡ä»¶
        name = "Shi_everyone"
        model.save_weights(name + '.h5')
        model.save(name + ".keras")
        ```
    
    === "`my_test10_1.py`"
    
        ```python title="my_test10_1.py" linenums="1"
        import pandas as pd
        import numpy as np
    
        import tensorflow as tf
        from tensorflow.keras import Sequential
        from tensorflow.keras.layers import GRU, Dense
    
        all_datas = []
        for i in range(13, 18):
            # if i != 5 and i != 6:
            data = pd.read_csv(f"onlyone/{i:02d}01.csv")
            all_datas.append(np.array(data)[:159-3, :])
        # data = pd.read_csv("CA(test)_2.csv")
        data = np.stack(tuple(all_datas))
        print(data)
    
        data = data[:, :, 1:].astype(np.float32)  # delete name
        print(data)
    
        # input()
    
        nums = len(data[0])
        labels, data, val = data[:, :, -1:], data[:, :, :-1], data[:, -nums//3:, :]
        # Shi, val = val[:, :, :1], val[:, :, 1:]
        print(data)
        print(labels)
    
        # input()
    
        def build_model():
            model = Sequential()
            model.add(GRU(50, input_shape=(None, 18), return_sequences=True))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
            model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
            return model
    
        # ç¼–è¯‘æ¨¡å‹
        model = build_model()
        model.compile(optimizer='adam', loss="mean_absolute_error")
    
        # è®­ç»ƒæ¨¡å‹
        model.fit(data, labels, epochs=10000, batch_size=5)
    
        # ä¿å­˜æƒé‡æ–‡ä»¶
        name = "Shi_onlyone"
        model.save_weights(name + '.h5')
        model.save(name + ".keras")
        ```

è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ32ä¸ªé€‰æ‰‹å³å¯¹åº”32ä¸ªæ•°æ®åºåˆ—ï¼Œæ‰€ä»¥ `batch_size` è®¾ä¸º `32` ï¼Œ

ä½†è¾“å…¥æ•°æ®æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå³ **==æ¯ä¸ªåºåˆ—çš„é•¿åº¦å¿…é¡»ä¸€æ ·==**ã€‚

äºæ˜¯è·å–åˆ°äº†16åœºæ¯”èµ›å„è‡ªè¿›è¡Œäº†å¤šå°‘*åˆ†*,

```css
1   300, 201, 134, 337, 
5   246, 332, 232, 190, 
9   213, 318, 170, 275, 
13  290, 185, 198, 167,
```

å†é€šè¿‡ä»£ç è®¡ç®—å‡ºèƒ½ä½¿ç”¨æœ€å¤šæ•°æ®è¿›è¡Œè®­ç»ƒçš„æ–¹æ¡ˆï¼Œ

??? note "`my_test11.py`"

    ```python linenums="1"
    import pandas as pd
    import numpy as np
    
    import tensorflow as tf
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import GRU, Dense
    
    # res = []
    # for i in range(1, 33):
    #     data = pd.read_csv(f"everyone/{i:02d}.csv")
    #     res.append(len(np.array(data)))
    #
    # print(res)
    
    res = [300, 300, 201, 201, 134, 134, 337, 337, 246, 246, 332, 332, 232, 232, 190, 190, 213, 213, 318, 318, 170, 170, 275, 275, 290, 290, 185, 185, 198, 198, 167, 167]
    
    print(min(res))
    print(len(list(filter(lambda x: x < 167, res))))
    
    s = sum(res)
    
    for x in res:
        sublist = list(filter(lambda y: y >= x, res))
        print(f"{s - sum(sublist)}, {len(sublist) * x}")
    ```

æœ€åå¾—åˆ°çš„æ–¹æ¡ˆæ˜¯ï¼Œåªé™¤å»æ¯”åˆ†æœ€å°‘(134)çš„ä¸€åœºï¼Œå…¶ä½™çš„åºåˆ—é•¿åº¦éƒ½ä¿ç•™è‡³164(167-3ï¼Œæœ€åä¸‰åˆ†åŠ¿å¤´æ— æ³•è¿›è¡Œé¢„æµ‹)ï¼Œ

```python title="my_test10.py" linenums="8"
all_datas = []
for i in range(1, 33):
    if i != 5 and i != 6:
        data = pd.read_csv(f"everyone/{i:02d}.csv")
        all_datas.append(np.array(data)[:164, :])
# data = pd.read_csv("CA(test)_2.csv")
data = np.stack(tuple(all_datas))
```

>   è®­ç»ƒç»“æœå‘½åä¸º `Shi_everyone.xxx`

---

ç¬¬äºŒç§æ€è·¯ä¹Ÿæœ‰ç›¸ä¼¼çš„å¤„ç†æ­¥éª¤ï¼Œ

```python title="my_test10_1.py" linenums="8"
all_datas = []
for i in range(13, 18):
    # if i != 5 and i != 6:
    data = pd.read_csv(f"onlyone/{i:02d}01.csv")
    all_datas.append(np.array(data)[:159-3, :])
# data = pd.read_csv("CA(test)_2.csv")
data = np.stack(tuple(all_datas))
```

!!! info

    5ä¸ªåºåˆ—é•¿åº¦åˆ†åˆ«æ˜¯ `300, 225, 189, 159, 334`

>   è®­ç»ƒç»“æœå‘½åä¸º `Shi_onlyone.xxx`

---

ç„¶åç”¨ç¬¬ä¸€ç§æ€è·¯è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å¯¹å† å†›ç¬¬3è½®1501çš„æ¯”èµ›çš„æ•°æ®è¿›è¡Œäº†é¢„æµ‹ï¼Œç»“æœå¦‚ä¸‹å›¾

![mcm_predict_everyone_on_1501_p1](../images/mcm_predict_everyone_on_1501_p1.svg){ loading=lazy }

>   æ•°æ®ä¿å­˜ä¸º `Shi_everyone.csv`

æ„Ÿè§‰å¾ˆokğŸ˜Š

---

ä¸ºäº†æ¯”è¾ƒä¸¤ç§æ¨¡å‹çš„é¢„æµ‹æ•ˆæœï¼Œéœ€è¦æœ‰ä¸€åœºåœ¨ä¸¤è¾¹éƒ½æ²¡æœ‰ä½¿ç”¨è¿‡çš„æ¯”èµ›æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œäºæ˜¯åˆç¼–å†™äº† `my_test7_1_2.py` `my_test8_1_2.py` `my_test9_1_2.py` æ¥å¤„ç†5åœºäºšå†›çš„æ•°æ®ï¼Œ

>   ä¿å­˜åœ¨äº† `onlyone2/` ä¸‹

ç„¶åç”¨äºšå†›ç¬¬4è½®åŠå†³èµ›1502çš„æ¯”èµ›æ•°æ®ï¼Œä¸€èµ·è¾“å…¥ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œç»“æœå¦‚ä¸‹å›¾

![mcm_predict_everyone_onlyone_on_1602_p2](../images/mcm_predict_everyone_onlyone_on_1602_p2.svg){ loading=lazy }

äºæ˜¯å‘ç°ï¼Œå±…ç„¶ **==æ‹¿å† å†›ä¸ªäººçš„æ•°æ®è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„é¢„æµ‹æ•ˆæœè¦æ¯”æ‹¿32åé€‰æ‰‹çš„æ•°æ®è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹è¦å¥½==** ğŸ˜²ï¼Œè¿™æ˜¯æˆ‘æ²¡æƒ³åˆ°çš„æƒ…å†µï¼Œ

äºæ˜¯é‰´äºè¿™æ ·çš„æƒ…å†µï¼Œæˆ‘å†³å®šå†ç”¨äºšå†›çš„æ•°æ®è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå†ç”¨ä¸‰ä¸ªæ¨¡å‹ä¸€èµ·é¢„æµ‹åŒä¸€åœºæ¯”èµ›ã€‚

>   æ­¤æ—¶å¤§çº¦6å·0ç‚¹40åˆ†

---

æ”¹å‡º `my_test10_2.py` æ¥è®­ç»ƒäºšå†›çš„æ•°æ®ï¼Œ

??? note "`my_test10_2.py`"

    ```python linenums="1"
    import pandas as pd
    import numpy as np
    
    import tensorflow as tf
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import GRU, Dense
    
    all_datas = []
    for i in ("1316", "1408", "1504", "1602", "1701"):
        # if i != 5 and i != 6:
        data = pd.read_csv(f"onlyone_2/{i}.csv")
        all_datas.append(np.array(data)[:167-3, :])
    # data = pd.read_csv("CA(test)_2.csv")
    data = np.stack(tuple(all_datas))
    print(data)
    
    data = data[:, :, 1:].astype(np.float32)  # delete name
    print(data)
    
    # input()
    
    nums = len(data[0])
    labels, data, val = data[:, :, -1:], data[:, :, :-1], data[:, -nums//3:, :]
    # Shi, val = val[:, :, :1], val[:, :, 1:]
    print(data)
    print(labels)
    
    # input()
    
    def build_model():
        model = Sequential()
        model.add(GRU(50, input_shape=(None, 18), return_sequences=True))  # ä½¿ç”¨50ä¸ªGRUå•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºå±‚ï¼Œç”¨äºå›å½’é—®é¢˜
    
        return model
    
    # ç¼–è¯‘æ¨¡å‹
    model = build_model()
    model.compile(optimizer='adam', loss="mean_absolute_error")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(data, labels, epochs=10000, batch_size=5)
    
    # ä¿å­˜æƒé‡æ–‡ä»¶
    name = "Shi_onlyone_2"
    model.save_weights(name + '.h5')
    model.save(name + ".keras")
    ```

>    è®­ç»ƒç»“æœå‘½åä¸º `Shi_onlyone_2.xxx`

ç„¶åå¤„ç†å‡º **è¾“ç»™äºšå†›çš„å››å¼º** çš„4åœºæ¯”èµ›æ•°æ®( `my_test7_1_3.py` `my_test8_1_3.py` `my_test9_1_3.py` )ï¼Œç„¶åç”¨ä¸‰ä¸ªä¸åŒçš„æ¨¡å‹å¯¹**ä»–**ç¬¬3è½®1503çš„æ¯”èµ›æ•°æ®è¿›è¡Œé¢„æµ‹(==ç”¨è¿™ä¸€åœºå› ä¸ºä¸‰ä¸ªæ¨¡å‹éƒ½ä¸è¿™ä¸€åœºæ¯”èµ›æ— å…³==)ï¼Œç»“æœå¦‚ä¸‹å›¾

=== "3ä¸ªæ¨¡å‹ä¸€èµ·é¢„æµ‹"

    ![mcm_predict_three_models_on_1503_p1](../images/mcm_predict_three_models_on_1503_p1.svg){ loading=lazy }

=== "å¤§ä¼—æ¨¡å‹å’Œäºšå†›æ¨¡å‹"

    ![mcm_predict_everyone_onlyone_2_on_1503_p1](../images/mcm_predict_everyone_onlyone_2_on_1503_p1.svg){ loading=lazy }

çœ‹èµ·æ¥ä¼¼ä¹ç¡®å®ç”¨å•äººæ•°æ®è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹è¦æ¯”å¤§ä¼—è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹é¢„æµ‹æ•ˆæœè¦å¥½ğŸ¤”ï¼Œæˆ–è®¸å¯èƒ½æ˜¯å› ä¸ºåŒä¸€ä¸ªé€‰æ‰‹çš„æ¯”èµ›æ•°æ®ç›¸å…³æ€§æ›´é«˜ï¼Œæ‰€ä»¥è®­ç»ƒå‡ºçš„æ¨¡å‹æ•ˆæœæ›´å¥½ã€‚

>   æ­¤æ—¶å¤§çº¦å‡Œæ™¨1ç‚¹åŠ

---

ç”±äºå·²ç»æ¯”è¾ƒæ™šï¼Œæ‰€ä»¥ä¹Ÿä¸çŸ¥é“è¿˜èƒ½æ€æ ·å–æ•°æ®è¿›è¡Œé¢„æµ‹å’Œè®­ç»ƒï¼Œå°±æ²¡æœ‰å†å»å¤„ç†æ–°çš„æ•°æ®äº†ã€‚

### åˆ†æé¢„æµ‹å‡†ç¡®çš„æ•°æ® (6å·å‡Œæ™¨)

(ç”±äºå·²ç»æ²¡ä»€ä¹ˆäº‹äº†ï¼Œäºæ˜¯)æˆ‘åœ¨pycharmçš„copilotä¸Šè¯¢é—®å®ƒè®­ç»ƒå‡ºæ¥æ¨¡å‹åï¼Œå¦‚ä½•æå–è¾“å…¥çš„ç‰¹å¾

>   ç¬¬äºŒä¸ªé—®é¢˜æ˜¯copilotæç¤º/çŒœæµ‹æˆ‘è¦è¯¢é—®å®ƒçš„é—®é¢˜ï¼Œæˆ‘è§‰å¾—æ˜¯ä¸ªå¥½é—®é¢˜äºæ˜¯ç»§ç»­æŒ‰ç…§å®ƒçš„æç¤ºè¯¢é—®å®ƒ

???+ example "chatlog"

    **RonaldLN**:
    
    I have learned some data through TensorFlow's GRU to obtain a model that can predict given inputs. I would like to know how to extract the characteristics of the input when the predicted value matches the true value?
    
    &nbsp;
    
    **Github Copilot**:
    
    You can extract the characteristics of the input when the predicted value matches the true value by comparing the predicted outputs with the true outputs. If they match within a certain tolerance, you can store the corresponding input. Here's how you can do it:
    
    1.   Load your model.
    2.   Use the model to predict outputs based on your inputs.
    3.   Initialize an empty list to store inputs where predicted output matches true output.
    4.   Iterate over each predicted output:
         -   If the predicted output matches the true output within a certain tolerance, append the corresponding input to your list.
    
    Here's the Python code that implements this:
    
    ```python
    import numpy as np
    
    # Assuming you have your inputs and true outputs in variables `inputs` and `true_outputs`
    # And your model in variable `model`
    
    predicted_outputs = model.predict(inputs)
    
    # Initialize an empty list to store inputs where predicted output matches true output
    matching_inputs = []
    
    for i in range(len(predicted_outputs)):
        # If the predicted output matches the true output
        if np.allclose(predicted_outputs[i], true_outputs[i], atol=0.1):
            # Store the corresponding input
            matching_inputs.append(inputs[i])
    
    # Now, `matching_inputs` contains the inputs where the predicted output matches the true output
    ```
    
    In this code, `np.allclose` is used to check if the predicted output matches the true output within a tolerance of 0.1. You can adjust this tolerance as needed. The `matching_inputs` list will contain the inputs for which the predicted and true outputs match.
    
    ??? quote "Using 3 Reference"
    
        -   my_test10_predict.py
        -   my_test10_2.py
        -   my_test3.py
    
    &nbsp;
    
    **RonaldLN**:
    
    How can I visualize the characteristics of the input data that matches the true value?
    
    &nbsp;
    
    **Github Copilot**:
    
    You can visualize the characteristics of the input data that matches the true value by plotting histograms for each dimension of the input data. Here's how you can do it:
    
    1.   Convert your list of matching inputs to a numpy array for easier manipulation.
    2.   For each dimension in the input data, create a new figure and plot a histogram of the data in the current dimension.
    3.   Set the title of the plot to the current dimension and show the plot.
    
    Here's the Python code that implements this:
    
    ```python
    import matplotlib.pyplot as plt
    
    # Convert matching_inputs to a numpy array for easier manipulation
    matching_inputs_np = np.array(matching_inputs)
    
    # For each dimension in the input data
    for i in range(matching_inputs_np.shape[1]):
        # Create a new figure
        plt.figure()
    
        # Plot a histogram of the data in the current dimension
        plt.hist(matching_inputs_np[:, i], bins=20)
    
        # Set the title of the plot
        plt.title(f'Input Dimension {i + 1}')
    
        # Show the plot
        plt.show()
    ```
    
    In this code, `plt.hist` is used to plot a histogram of the data in the current dimension. The `bins` parameter is set to 20, but you can adjust this as needed. The title of the plot is set to the current dimension using `plt.title`. Finally, the plot is displayed using `plt.show`.
    
    ??? quote "Using 3 Reference"
    
        -   my_test10_predict.py
        -   my_test10_2.py
        -   my_test3.py
        
    ---
    
    ??? quote "ChatGPTç¿»è¯‘"
    
        **RonaldLN**ï¼š
    
        æˆ‘é€šè¿‡TensorFlowçš„GRUå­¦åˆ°äº†ä¸€äº›æ•°æ®ï¼Œå¾—åˆ°äº†ä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®ç»™å®šè¾“å…¥è¿›è¡Œé¢„æµ‹çš„æ¨¡å‹ã€‚æˆ‘æƒ³çŸ¥é“åœ¨é¢„æµ‹å€¼ä¸çœŸå®å€¼åŒ¹é…æ—¶å¦‚ä½•æå–è¾“å…¥çš„ç‰¹å¾ï¼Ÿ
    
        &nbsp;
    
        **Github Copilot**ï¼š
    
        å½“é¢„æµ‹å€¼ä¸çœŸå®å€¼åŒ¹é…æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡æ¯”è¾ƒé¢„æµ‹è¾“å‡ºå’ŒçœŸå®è¾“å‡ºæ¥æå–è¾“å…¥çš„ç‰¹å¾ã€‚å¦‚æœå®ƒä»¬åœ¨æŸä¸ªå®¹å·®èŒƒå›´å†…åŒ¹é…ï¼Œä½ å¯ä»¥å­˜å‚¨ç›¸åº”çš„è¾“å…¥ã€‚ä»¥ä¸‹æ˜¯å…·ä½“æ­¥éª¤ï¼š
    
        1.   åŠ è½½ä½ çš„æ¨¡å‹ã€‚
        2.   ä½¿ç”¨æ¨¡å‹æ ¹æ®è¾“å…¥é¢„æµ‹è¾“å‡ºã€‚
        3.   åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨é¢„æµ‹è¾“å‡ºä¸çœŸå®è¾“å‡ºåŒ¹é…çš„è¾“å…¥ã€‚
        4.   éå†æ¯ä¸ªé¢„æµ‹è¾“å‡ºï¼š
             -   å¦‚æœé¢„æµ‹è¾“å‡ºåœ¨æŸä¸ªå®¹å·®èŒƒå›´å†…ä¸çœŸå®è¾“å‡ºåŒ¹é…ï¼Œåˆ™å°†ç›¸åº”çš„è¾“å…¥æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
    
        ä»¥ä¸‹æ˜¯å®ç°è¿™ä¸€è¿‡ç¨‹çš„Pythonä»£ç ï¼š
    
        ```python
        import numpy as np
    
        # å‡è®¾ä½ çš„è¾“å…¥å’ŒçœŸå®è¾“å‡ºåˆ†åˆ«å­˜å‚¨åœ¨å˜é‡`inputs`å’Œ`true_outputs`ä¸­
        # ä½ çš„æ¨¡å‹å­˜å‚¨åœ¨å˜é‡`model`ä¸­
    
        predicted_outputs = model.predict(inputs)
    
        # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨é¢„æµ‹è¾“å‡ºä¸çœŸå®è¾“å‡ºåŒ¹é…çš„è¾“å…¥
        matching_inputs = []
    
        for i in range(len(predicted_outputs)):
            # å¦‚æœé¢„æµ‹è¾“å‡ºä¸çœŸå®è¾“å‡ºåŒ¹é…
            if np.allclose(predicted_outputs[i], true_outputs[i], atol=0.1):
                # å­˜å‚¨ç›¸åº”çš„è¾“å…¥
                matching_inputs.append(inputs[i])
    
        # ç°åœ¨ï¼Œ`matching_inputs`åŒ…å«é¢„æµ‹è¾“å‡ºä¸çœŸå®è¾“å‡ºåŒ¹é…çš„è¾“å…¥
        ```
    
        åœ¨è¿™æ®µä»£ç ä¸­ï¼Œ`np.allclose`ç”¨äºæ£€æŸ¥é¢„æµ‹è¾“å‡ºæ˜¯å¦åœ¨å®¹å·®ä¸º0.1çš„èŒƒå›´å†…ä¸çœŸå®è¾“å‡ºåŒ¹é…ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªå®¹å·®ã€‚`matching_inputs`åˆ—è¡¨å°†åŒ…å«é¢„æµ‹å’ŒçœŸå®è¾“å‡ºåŒ¹é…çš„è¾“å…¥ã€‚
    
        &nbsp;
    
        **RonaldLN**ï¼š
    
        æˆ‘å¦‚ä½•å¯è§†åŒ–ä¸çœŸå®å€¼åŒ¹é…çš„è¾“å…¥æ•°æ®çš„ç‰¹å¾ï¼Ÿ
    
        &nbsp;
    
        **Github Copilot**ï¼š
    
        ä½ å¯ä»¥é€šè¿‡ç»˜åˆ¶æ¯ä¸ªè¾“å…¥æ•°æ®ç»´åº¦çš„ç›´æ–¹å›¾æ¥å¯è§†åŒ–ä¸çœŸå®å€¼åŒ¹é…çš„è¾“å…¥æ•°æ®çš„ç‰¹å¾ã€‚ä»¥ä¸‹æ˜¯å…·ä½“æ­¥éª¤ï¼š
    
        1.   å°†åŒ¹é…è¾“å…¥çš„åˆ—è¡¨è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä»¥ä¾¿æ›´å®¹æ˜“å¤„ç†ã€‚
        2.   å¯¹äºè¾“å…¥æ•°æ®çš„æ¯ä¸ªç»´åº¦ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢ï¼Œå¹¶ç»˜åˆ¶å½“å‰ç»´åº¦æ•°æ®çš„ç›´æ–¹å›¾ã€‚
        3.   å°†å›¾å½¢çš„æ ‡é¢˜è®¾ç½®ä¸ºå½“å‰ç»´åº¦ï¼Œå¹¶æ˜¾ç¤ºå›¾å½¢ã€‚
    
        ä»¥ä¸‹æ˜¯å®ç°è¿™ä¸€è¿‡ç¨‹çš„Pythonä»£ç ï¼š
    
        ```python
        import matplotlib.pyplot as plt
    
        # å°†åŒ¹é…è¾“å…¥è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œä»¥ä¾¿æ›´å®¹æ˜“å¤„ç†
        matching_inputs_np = np.array(matching_inputs)
    
        # å¯¹äºè¾“å…¥æ•°æ®çš„æ¯ä¸ªç»´åº¦
        for i in range(matching_inputs_np.shape[1]):
            # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢
            plt.figure()
    
            # ç»˜åˆ¶å½“å‰ç»´åº¦æ•°æ®çš„ç›´æ–¹å›¾
            plt.hist(matching_inputs_np[:, i], bins=20)
    
            # è®¾ç½®å›¾å½¢çš„æ ‡é¢˜
            plt.title(f'è¾“å…¥ç»´åº¦ {i + 1}')
    
            # æ˜¾ç¤ºå›¾å½¢
            plt.show()
        ```
    
        åœ¨è¿™æ®µä»£ç ä¸­ï¼Œ`plt.hist`ç”¨äºç»˜åˆ¶å½“å‰ç»´åº¦æ•°æ®çš„ç›´æ–¹å›¾ã€‚`bins`å‚æ•°è®¾ç½®ä¸º20ï¼Œä½†ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ã€‚ä½¿ç”¨`plt.title`è®¾ç½®å›¾å½¢çš„æ ‡é¢˜ä¸ºå½“å‰ç»´åº¦ã€‚æœ€åï¼Œä½¿ç”¨`plt.show`æ˜¾ç¤ºå›¾å½¢ã€‚

äºæ˜¯æˆ‘æŒ‰ç…§copilotæä¾›çš„ä»£ç ï¼Œç¼–å†™äº† `my_test12.py` (ä¸ºäº†é€‚é…æˆ‘çš„è¾“å‡ºçš„æ•°æ®åšäº†éƒ¨åˆ†ä¿®æ”¹)ï¼Œä½¿ç”¨å† å†›æ¨¡å‹æ¥é¢„æµ‹å†³èµ›äºšå†›çš„æ•°æ®ï¼Œå¹¶å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåˆ†æï¼Œ

??? note "`my_test12.py`"

    ```python linenums="1"
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    
    import tensorflow as tf
    from tensorflow.keras import Sequential
    from tensorflow.keras.layers import GRU, Dense
    from tensorflow.keras.models import load_model, Model
    
    data = pd.read_csv("onlyone_2/1602.csv")
    data = np.array([data])
    inputs = data[:, :, 1:][:, :, :-1].astype(np.float32)
    true_outputs = data[:, :, -1:].astype(np.float32)
    
    model = load_model("Shi_onlyone.keras")
    
    # Assuming you have your inputs and true outputs in variables `inputs` and `true_outputs`
    # And your model in variable `model`
    
    predicted_outputs = model.predict(inputs)
    
    print(predicted_outputs)
    
    # Initialize an empty list to store inputs where predicted output matches true output
    matching_inputs = []
    
    for i in range(len(predicted_outputs[0])):
        # If the predicted output matches the true output
        if np.allclose(predicted_outputs[0][i][0], true_outputs[0][i][0], atol=0.1):
            # Store the corresponding input
            matching_inputs.append(inputs[0][i])
    
    # Now, `matching_inputs` contains the inputs where the predicted output matches the true output
    print(matching_inputs)
    print(len(matching_inputs))
    print(len(inputs[0]))
    
    # Convert matching_inputs to a numpy array for easier manipulation
    matching_inputs_np = np.array(matching_inputs)
    
    # For each dimension in the input data
    for i in range(matching_inputs_np.shape[1]):
        # Create a new figure
        plt.figure()
    
        # Plot a histogram of the data in the current dimension
        plt.hist(matching_inputs_np[:, i], bins=20)
    
        # Set the title of the plot
        plt.title(f'Input Dimension {i + 1}')
    
        # Show the plot
        # plt.show()
        plt.savefig(f"pic/matching Input Dimension {i + 1}.png")
    
        # close the plot
        plt.close()
    
    # Initialize an empty list to store inputs where predicted output does not match true output
    non_matching_inputs = []
    
    for i in range(len(predicted_outputs[0])):
        # If the predicted output does not match the true output
        if not np.allclose(predicted_outputs[0][i][0], true_outputs[0][i][0], atol=0.1):
            # Store the corresponding input
            non_matching_inputs.append(inputs[0][i])
    
    # Now, `non_matching_inputs` contains the inputs where the predicted output does not match the true output
    
    # Convert non_matching_inputs to a numpy array for easier manipulation
    non_matching_inputs_np = np.array(non_matching_inputs)
    
    # For each dimension in the input data
    for i in range(non_matching_inputs_np.shape[1]):
        # Create a new figure
        plt.figure()
    
        # Plot a histogram of the data in the current dimension
        plt.hist(non_matching_inputs_np[:, i], bins=20)
    
        # Set the title of the plot
        plt.title(f'Input Dimension {i+1}')
    
        # Show the plot
        # plt.show()
        plt.savefig(f"pic/non_matching Input Dimension {i + 1}.png")
    
        # close the plot
        plt.close()
    ```

**éœ€è¦æ³¨æ„çš„æ˜¯**ï¼Œåœ¨ä½¿ç”¨ `np.allclose` å‡½æ•°æ—¶ï¼Œé™¤äº†ä¼ å…¥é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼Œè¿˜**è®¾ç½® `rtol` å’Œ `atol` å‚æ•°**ï¼Œå¤§æ¦‚æ˜¯å…è®¸è¯¯å·®çš„èŒƒå›´/è¯¯å·®å®¹å¿( `tol` å¯¹åº” tolerance)ï¼Œ

ä¸€å¼€å§‹æˆ‘æ²¡æœ‰è®¾ç½®(å°±é‡‡ç”¨çš„æ˜¯å®ƒé»˜è®¤çš„å€¼ï¼Œé»˜è®¤çš„å€¼å¾ˆå°)ï¼Œæ‰€ä»¥è¿è¡Œå‡ºæ¥æ˜¾ç¤ºé¢„æµ‹ç»“æœä¸­æ²¡æœ‰ä¸çœŸå®æ•°æ®ç›¸è¿‘çš„ğŸ˜…ã€‚

 `np.allclose` çš„å‡½æ•°è¯´æ˜å¦‚ä¸‹

```python
@array_function_dispatch(_allclose_dispatcher)
def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
    """
    Returns True if two arrays are element-wise equal within a tolerance.

    The tolerance values are positive, typically very small numbers.  The
    relative difference (`rtol` * abs(`b`)) and the absolute difference
    `atol` are added together to compare against the absolute difference
    between `a` and `b`.
    
    ...
    
    Parameters
    ----------
    a, b : array_like
        Input arrays to compare.
    rtol : float
        The relative tolerance parameter (see Notes).
    atol : float
        The absolute tolerance parameter (see Notes).
    ...
    
    Notes
    -----
    If the following equation is element-wise True, then allclose returns
    True.

     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
     
    The above equation is not symmetric in `a` and `b`, so that
    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
    some rare cases.
    
    ...
    """
```

ç„¶ååˆé—®äº†ä¸€ä¸‹chatgpt

???+ example "chatlog"

    **User**:
    
    ```markdown
        Notes
        -----
        If the following equation is element-wise True, then allclose returns
        True.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        The above equation is not symmetric in `a` and `b`, so that
        ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
        some rare cases.
    ```
    
    translate to chinese
    
    &nbsp;
    
    **ChatGPT**:
    
    ```markdown
        æ³¨é‡Š
        -----
        å¦‚æœä»¥ä¸‹æ–¹ç¨‹åœ¨å…ƒç´ çº§åˆ«ä¸Šä¸ºTrueï¼Œåˆ™allcloseè¿”å›Trueã€‚
    
         ç»å¯¹å€¼ï¼ˆ`a` - `b`ï¼‰<=ï¼ˆ`atol` + `rtol` * ç»å¯¹å€¼ï¼ˆ`b`ï¼‰ï¼‰
    
        ä¸Šè¿°æ–¹ç¨‹åœ¨`a`å’Œ`b`ä¸­ä¸å¯¹ç§°ï¼Œå› æ­¤åœ¨ä¸€äº›ç½•è§æƒ…å†µä¸‹ï¼Œ``allclose(a, b)``å¯èƒ½ä¸``allclose(b, a)``ä¸åŒã€‚
    ```
    
    &nbsp;
    
    **User**:
    
    `atol` å’Œ `rtol` åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
    
    &nbsp;
    
    **ChatGPT**:
    
    åœ¨ä¸Šè¿°æ–‡æ¡£ä¸­ï¼Œ`atol` å’Œ `rtol` æ˜¯ç”¨äºæ§åˆ¶`allclose`å‡½æ•°çš„ä¸¤ä¸ªå‚æ•°ã€‚
    
    -   `atol` æ˜¯"absolute tolerance"ï¼ˆç»å¯¹å®¹å·®ï¼‰çš„ç¼©å†™ã€‚å®ƒæ˜¯ä¸€ä¸ªç»å¯¹å€¼ï¼Œè¡¨ç¤ºä¸¤ä¸ªæ•°å€¼åœ¨è¿›è¡Œæ¯”è¾ƒæ—¶çš„ç»å¯¹å®¹å¿åº¦ã€‚å¦‚æœä¸¤ä¸ªæ•°å€¼ä¹‹é—´çš„å·®å¼‚å°äºæˆ–ç­‰äº`atol`ï¼Œåˆ™è¢«è®¤ä¸ºæ˜¯ç›¸ç­‰çš„ã€‚åœ¨ä¸Šè¿°çš„ç­‰å¼ä¸­ï¼Œ`atol` æ§åˆ¶äº†ä¸¤ä¸ªæ•°å€¼ä¹‹é—´çš„å·®å¼‚æ˜¯å¦è¢«è®¤ä¸ºè¶³å¤Ÿå°ï¼Œä»¥ä½¿å®ƒä»¬è¢«è§†ä¸ºç›¸ç­‰ã€‚
    -   `rtol` æ˜¯"relative tolerance"ï¼ˆç›¸å¯¹å®¹å·®ï¼‰çš„ç¼©å†™ã€‚å®ƒæ˜¯ä¸€ä¸ªæ¯”ä¾‹å€¼ï¼Œè¡¨ç¤ºä¸¤ä¸ªæ•°å€¼åœ¨è¿›è¡Œæ¯”è¾ƒæ—¶çš„ç›¸å¯¹å®¹å¿åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœä¸¤ä¸ªæ•°å€¼ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚å°äºæˆ–ç­‰äº `rtol` ä¹˜ä»¥ç¬¬äºŒä¸ªæ•°å€¼çš„ç»å¯¹å€¼ï¼Œé‚£ä¹ˆå®ƒä»¬è¢«è®¤ä¸ºæ˜¯ç›¸ç­‰çš„ã€‚åœ¨ä¸Šè¿°çš„ç­‰å¼ä¸­ï¼Œ`rtol` æ§åˆ¶äº†ç›¸å¯¹å®¹å¿åº¦çš„ç¨‹åº¦ã€‚
    
    è¿™ä¸¤ä¸ªå‚æ•°ä¸€èµ·ç¡®å®šäº†åœ¨ä½¿ç”¨`allclose`å‡½æ•°è¿›è¡Œæ•°å€¼æ¯”è¾ƒæ—¶çš„å®¹å¿åº¦èŒƒå›´ã€‚

äºæ˜¯å¼„æ˜ç™½å‚æ•° `rtol` å’Œ `atol` çš„å«ä¹‰ä¹‹åï¼Œæˆ‘æ‰“ç®—åªè®¾ç½® `atol` çš„å€¼å°±è¡Œäº†ã€‚

ç»è¿‡å‡ æ¬¡å°è¯•ï¼Œæœ€åæˆ‘å°† `atol` è®¾ç½®æˆäº† `0.1` ï¼Œå› ä¸ºæ­¤æ—¶è¢«è®¤ä¸ºæ˜¯é¢„æµ‹å‡†ç¡®çš„æ•°æ®æœ‰34ä¸ªï¼Œæ¯”ä¾‹å·®ä¸å¤šå 15%è¿™æ ·ã€‚

---

æœ€åå°†é¢„æµ‹è¾ƒå‡†ç¡®çš„æ•°æ®å’Œä¸å‡†ç¡®æ•°æ®çš„ä¸ªæ•°åˆ†åˆ«ç»˜åˆ¶æˆæŸ±å½¢å›¾ï¼Œéƒ¨åˆ†ç»“æœå¦‚ä¸‹

=== "ç»´åº¦2: å½“å‰ç›˜æ‰€èµ¢å±€æ•°"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_2](../images/matching_input_dimension_2.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_2](../images/non_matching_input_dimension_2.png){ loading=lazy }

=== "ç»´åº¦3: å½“å‰å±€æ‰€èµ¢åˆ†æ•°"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_3](../images/matching_input_dimension_3.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_3](../images/non_matching_input_dimension_3.png){ loading=lazy }

=== "ç»´åº¦7: æ˜¯å¦ace"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_7](../images/matching_input_dimension_7.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_7](../images/non_matching_input_dimension_7.png){ loading=lazy }

=== "ç»´åº¦14: é¢†å…ˆç›˜æ•°"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_14](../images/matching_input_dimension_14.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_14](../images/non_matching_input_dimension_14.png){ loading=lazy }

=== "ç»´åº¦15: é¢†å…ˆå±€æ•°"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_15](../images/matching_input_dimension_15.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_15](../images/non_matching_input_dimension_15.png){ loading=lazy }

=== "ç»´åº¦16: é¢†å…ˆåˆ†æ•°"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_16](../images/matching_input_dimension_16.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_16](../images/non_matching_input_dimension_16.png){ loading=lazy }

=== "ç»´åº¦14: æ˜¯å¦ä¸€å±€æœ€å3çƒ"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_17](../images/matching_input_dimension_17.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_17](../images/non_matching_input_dimension_17.png){ loading=lazy }

=== "ç»´åº¦18: å½“å‰å±€è·èƒœæ‰€éœ€åˆ†æ•°"

    é¢„æµ‹è¾ƒå‡†æ•°æ®ï¼š
    
    ![matching_input_dimension_18](../images/matching_input_dimension_18.png){ loading=lazy }
    
    å…¶ä½™æ•°æ®ï¼š
    
    ![non_matching_input_dimension_18](../images/non_matching_input_dimension_18.png){ loading=lazy }

æˆ‘åˆ†ææ„Ÿè§‰ï¼Œ**é‚£äº›å‡†ç¡®å’Œä¸å‡†ç¡®ä¸­æ¯”ä¾‹éƒ½å·®ä¸å¤šçš„ç»´åº¦/æŒ‡æ ‡ï¼Œè¯´æ˜å°±å’ŒåŠ¿å¤´ç›¸å…³æ€§ä¸æ˜¯é‚£ä¹ˆå¤§ï¼Œè€Œ ==æ¯”ä¾‹æœ‰æ˜æ˜¾ä¸åŒ== çš„ç»´åº¦/æŒ‡æ ‡ï¼Œå°±æ„å‘³ç€å¯¹å‡†ç¡®åº¦æ˜¯å…³é”®å› ç´ **ã€‚

äºæ˜¯æŠŠå›¾ç‰‡ä¼ ç»™é˜Ÿå‹ï¼Œä¸çŸ¥é“ä»–æ˜¯å¦ç”¨ä¸Šã€‚

ç„¶åå‘ç°å…¶å®è¿˜å¯ä»¥æ›´æ”¹ **å‡†ç¡®** çš„åˆ¤æ–­ä¾æ®ï¼Œæ¯”å¦‚æ£€æŸ¥ ***åŠ¿å¤´*ä¸Šå‡ä¸‹é™æˆ–è½¬æŠ˜çš„è¶‹åŠ¿** æ˜¯å¦å’ŒçœŸå®æ•°æ®ä¸€è‡´ï¼Œä½†æ˜¯ç”±äºè·ç¦»æ¯”èµ›ç»“æŸæ—¶é—´ä¸æ˜¯å¾ˆå¤šäº†ï¼Œæœ€åå°±æ²¡æœ‰å°è¯•è¿™æ ·çš„æ€è·¯ã€‚

>   æ­¤æ—¶çº¦å‡Œæ™¨3ç‚¹40

---

æœ€åå°±æ˜¯é˜Ÿå‹åœ¨ç¼–å†™è®ºæ–‡ï¼Œæˆ‘å°±æ²¡æœ‰ä»€ä¹ˆä»»åŠ¡äº†ã€‚

åˆ°äº†(6å·)æ—©ä¸Š8ç‚¹é˜Ÿå‹æäº¤äº†è®ºæ–‡ã€‚

## ä¸€ç‚¹æ„Ÿå—

è¿™å‡ å¤©ä¸‹æ¥æ˜¯çœŸçš„ç´¯ğŸ˜‡ï¼Œæ„Ÿè§‰æ¯å¤©éƒ½åœ¨å¤´è„‘é£æš´ï¼Œæœ€åä¸€å¤©ç”šè‡³éœ€è¦é€šå®µï¼Œ

ä½†æ˜¯ç¡®å®åˆå­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼Œä¸ªäººè§‰å¾—è¿˜æ˜¯å¾ˆæœ‰æ„ä¹‰çš„ğŸ§(åªå¸Œæœ›æœ€åä¸è¦ç»™æˆ‘ä¸€ä¸ªSå¥–ï¼Œé‚£æˆ‘æ˜¯çœŸçš„ä¼šè°¢ğŸ˜…)ã€‚

<h2>ä»£ç ä»“åº“</h2>

-   [RonaldLN/Code_for_2024_MCM_C (github.com)](https://github.com/RonaldLN/Code_for_2024_MCM_C)
-   [Ronald_Luo/Code_for_2024_MCM_C (gitee.com)](https://gitee.com/ronald-luo/Code_for_2024_MCM_C)

æœ¬æ–‡å¯¹åº”ä»£ç ç‰ˆæœ¬ä½äºoriginalåˆ†æ”¯ä¸­
